Nachfolgend eine **modulare Skript-Gliederung** (Dateien `.R`), wobei innerhalb jeder Datei nur **algorithmischer Pseudocode** steht.
Jeder Block beschreibt

* Zweck der Datei
* Wichtigste Funktionen (Prozeduren)
* Mathematisch prÃ¤zise Schritte, gegliedert in Unterpunkte

Dadurch kann man die `config`-Liste oder die Zahl der Dimensionen $K$ Ã¤ndern, ohne irgendeine andere Datei anfassen zu mÃ¼ssen, und jederzeit neue Modelle nachrÃ¼sten.

---

### **Script 1: 00\_globals.R**

```
FUNCTION setup_global():
    INPUT  : â€”
    OUTPUT : list G = (N, config, seed, split_ratio, H_grid, model_ids, P_max)

    1  N           â† 500                                 # Stichprobenumfang
    2  config      â† USER-EINGABE                        # Liste mit K Elementen
    3  seed        â† 42                                  # Reproduzierbarkeit
    4  split_ratio â† 0.70                                # Train-Anteil
    5  P_max       â† 6                                   # hÃ¶chster Polynomgrad
    6  H_grid      â† {h âˆˆ â„• | 1 â‰¤ h â‰¤ P_max}             # Hyperparameter fÃ¼r TTM
    7  model_ids   â† {"TTM", "TRUE"}                     # erweiterbar
    8  RETURN G
```

---






### **Script 2: 01\_data\_generation.R**

```
FUNCTION gen_samples(G):
    INPUT : G = (N, config, seed, â€¦)
    OUTPUT: Matrix X âˆˆ â„^{NÃ—K}         # gleiche Reihenfolge wie config

    1  SET rng â† seed
    2  FOR i = 1,â€¦,N:
    3      FOR k = 1,â€¦,K:                        # sequentiell â‡’ bedingt
    4          c_k      â† config[k]
    5          params_k â†
                 IF is.null(c_k.parm)
                 THEN {}
                 ELSE c_k.parm( X[i, 1:(kâˆ’1)] )  # greift auf frÃ¼here Spalten
    6          X[i,k]  â† draw_from( c_k.distr , params_k )
    7  RETURN X
```

*`draw_from`* ist eine generische Hilfsroutine (z. B. Norm, Gamma usw.).

---

### **Script 3: 02\_split.R**

```
FUNCTION train_test_split(X, split_ratio, seed):
    INPUT : Datenmatrix X, split-Ratio r, seed
    OUTPUT: list S = (X_tr, X_te)

    1  SET rng â† seed + 1              # anderer Stream
    2  idx   â† random_permutation(1,â€¦,N)
    3  N_tr  â† âŒŠrÂ·NâŒ‹
    4  X_tr  â† X[ idx[1:N_tr],  ]
    5  X_te  â† X[ idx[N_tr+1:N], ]
    6  RETURN S
```

---

### **Script 4: models/ttm\_model.R**  (Referenz-Modell aus dem Paper)


#  Hilfsroutinen 

FUNCTION log_phi_K(z):
    # Log-Dichte der K-dimensionalen Standardnormalverteilung
    RETURN âˆ’0.5 * ( K * log(2Ï€) + ||z||Â² )

FUNCTION logsumexp(v):
    # stabil: log( Î£_j exp(v_j) )
    m â† max(v)
    RETURN m + log( Î£ exp(v âˆ’ m) )

FUNCTION log_integrate_exp(f, a, b, n = 32):
    # log âˆ«_a^b exp(f(t)) dt  mittels Gauss-Legendre-Quadratur (n Knoten)
    (w, s) â† gauss_legendre_nodes_weights(n)          # Gewichte w_j, StÃ¼tzstellen s_jâˆˆ(âˆ’1,1)
    t      â† 0.5*(bâˆ’a) * s + 0.5*(b+a)               # RÃ¼cktransformation
    v      â† log(w) + log(0.5*(bâˆ’a)) + f(t)          # alles additiv
    RETURN logsumexp(v)

#  Transport-Map und Logdet 

FUNCTION S(x ; Î¸, h):
    INPUT  : x = (xâ‚,â€¦,x_K),  Î¸ = {Î²_k, Î±_k}_k,  Grad h
    OUTPUT : z = (zâ‚,â€¦,z_K)

    FOR k = 1,â€¦,K:
        1  g_k â† Polynomial_deg(h)( xâ‚,â€¦,x_{kâˆ’1} ; Î²_k )
        2  P_k(t, xâ‚:_{kâˆ’1}) â† Polynomial_deg(h)( t, xâ‚,â€¦,x_{kâˆ’1} ; Î±_k )
        3  logI_k â† log_integrate_exp( Î» t: P_k(t, xâ‚:_{kâˆ’1}) , 0 , x_k )
        4  z_k â† g_k + exp( logI_k )                  # einziger exp-Schritt
    RETURN z

FUNCTION logJ(x ; Î¸, h):
    # Log-Jacobi-Determinante
    RETURN Î£_{k=1}^{K} P_k( x_k , xâ‚:_{kâˆ’1} )

#  (Negativ-)Log-Likelihood 

FUNCTION â„“(Î¸ | x, h):
    z     â† S(x ; Î¸, h)
    logJx â† logJ(x ; Î¸, h)
    RETURN log_phi_K(z) + logJx                      # log Ï€Ì‚(x)

FUNCTION ğ”_train(Î¸ | h):
    RETURN âˆ’ |X_tr|^{-1} Î£_{xâˆˆX_tr} â„“(Î¸ | x, h)      # zu minimieren

#  Training / Hyperparameter-Sweep 

FUNCTION fit_TTM(X_tr, X_te, H_grid):
    INPUT  : Trainings-/TestÂ­daten, H_grid
    OUTPUT : M_TTM = (Î¸*, h*, logL_te)

    FOR each h âˆˆ H_grid:
        1  Î¸â° â† 0                                   # alle Koeffizienten = 0
        2  Î¸Ì‚(h) â† argmin_Î¸  ğ”_train(Î¸ | h)          # L-BFGS-B
               stopping: â€–âˆ‡ğ”_trainâ€–_âˆ < 1eâˆ’6
        3  logL_te(h) â† âˆ’|X_te|^{-1} Î£_{xâˆˆX_te} â„“(Î¸Ì‚(h) | x, h)
        4  MESSAGE "h={h}, logL_te={logL_te(h)}"

    5  h* â† argmin_h logL_te(h)
    6  Î¸* â† Î¸Ì‚(h*)
    7  RETURN (Î¸*, h*, logL_te(h*))

#  Auswertung & optionales Sampling 

FUNCTION logL_TTM(M_TTM, X):
    INPUT  : (Î¸*, h*),  X
    OUTPUT : âˆ’|X|^{-1} Î£_{xâˆˆX} â„“(Î¸*, x, h*)

FUNCTION sample_TTM(M_TTM, Z):
    # optional fÃ¼r Diagnosen
    INPUT  : Modell M_TTM, Z ~ N(0, I_K)
    OUTPUT : X ~ Ï€Ì‚  via sequentielle Inversion der S_k
```


---

### **Script 5: models/true\_model.R**  (â€wahrerâ€œ Baseline-MLE)

```
FUNCTION fit_TRUE(X_tr, X_te, config):
    INPUT : X_tr, X_te, config
    OUTPUT: M_TRUE = (Î˜Ì‚, logL_te)

    FOR k = 1,â€¦,K:
        distr_k â† config[k].distr
        Î˜Ì‚_k    â† argmax_Î¸_k Î£_{xâˆˆX_tr[,k]} log f_{distr_k}(x | Î¸_k)   # wie bisher
                 # in der Implementierung ist log f() ohnehin im Log-Raum
    logL_te â† âˆ’ |X_te|^{-1} Î£_i Î£_k log f_{distr_k}( X_te[i,k] | Î˜Ì‚_k )
    RETURN (Î˜Ì‚, logL_te)

FUNCTION logL_TRUE(M_TRUE, X):
    RETURN âˆ’ |X|^{-1} Î£_i Î£_k log f_{distr_k}( X[i,k] | Î˜Ì‚_k )




```

*Anmerkung:* Jede Dimension wird unabhÃ¤ngig behandelt, denn â€wahrerâ€œ Mechanismus kennt die bedingten Dichten.

---

### **Script 6: 04\_evaluation.R**

```
FUNCTION evaluate_all(X_te, model_list):
    INPUT : Testdaten X_te, Liste model_list = {M_TTM, M_TRUE, â€¦}
    OUTPUT: Tabelle P = (model_id, âˆ’logL_te)

    1  INIT empty table P
    2  FOR i = 1,â€¦,|model_list|:
    3      id   â† names(model_list)[i]
    4      M    â† model_list[[i]]
    5      loss â† logL_<id>(M, X_te)            # dynamischer Funktionsaufruf
    6      append_row(P, (id, loss))
    7  sort_by loss ascending
    8  RETURN P
```

`model_specific_logL` ruft intern `logL_TTM`, `logL_TRUE`, oder eine Analogie fÃ¼r kÃ¼nftige Modelle.

---

### **Script 7: 05\_main.R**  (Orchestrator)

```
FUNCTION main():
    1  G        â† setup_global()                    # Script 1
    2  X        â† gen_samples(G)                    # Script 2
    3  S        â† train_test_split(X, G.split_ratio, G.seed)   # Script 3
    4  M_TTM    â† fit_TTM(S.X_tr, S.X_te, G.H_grid) # Script 4
    5  M_TRUE   â† fit_TRUE(S.X_tr, S.X_te, G.config) # Script 5
    6  results  â† evaluate_all(S.X_te, {M_TTM, M_TRUE})   # Script 6
    7  print(results)
    8  # einfache MÃ¶glichkeit, weitere Modelle:
       # Quellcode in models/<neues>.R mit
       #   fit_<NAME>()  und  logL_<NAME>()
       # anschlieÃŸend hier einfach laden & anhÃ¤ngen.
```

---

## **Wie man die FlexibilitÃ¤t nutzt**

1. **DimensionalitÃ¤t Ã¤ndern**
   *Passe nur* `config` in `00_globals.R` an.
   Das Sampling in `01_data_generation.R` sowie alle Schleifen Ã¼ber $k=1,\dots,K$ adaptieren automatisch.

2. **Andere Bedingungsstrukturen**
   Ersetze in `config[[k]]$parm` die Funktion, die aus den bereits generierten Spalten Parameter ableitet.
   Kein weiterer Code muss geÃ¤ndert werden.

3. **Neues Modell hinzufÃ¼gen**

   * Datei `models/<neues_modell>.R` anlegen
   * Zwei Funktionen implementieren: `fit_<NAME>()`, `logL_<NAME>()`
   * In `00_globals.R` den Namen ins Set `model_ids` und in `05_main.R` nach dem Fitting in `evaluate_all` Ã¼bergeben.

4. **Hyperparameter-Sweep verÃ¤ndern**
   AusschlieÃŸlich `H_grid` in `00_globals.R` anpassen.

Damit bleibt das Gesamtsystem **skript-modular** und **konfigurationsgetrieben**: sÃ¤mtliche Pipeline-Ã„nderungen erfolgen, ohne andere Dateien â€anzufassenâ€œ oder internen Code umschreiben zu mÃ¼ssen.

