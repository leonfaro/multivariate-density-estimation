---
title: "Report"
author: "LÃ©on Kia Faro"
date: "2025-08-20"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Methodology and Algorithms

## 0. Data-generating process (DGP)

We generate triangular (autoregressive) data via the factorization
\[
p(x) \;=\; p_1(x_1)\,\prod_{k=2}^{K} p_k\big(x_k \mid x_{1:k-1}\big).
\]

In the experiments we use the following configuration (softplus(u) = log(1 + exp(u))):
\[
\begin{aligned}
&X_1 \sim \mathcal{N}(0,1),\\
&X_2 \mid X_1 \sim \mathrm{Exp}\big(\text{rate} = \mathrm{softplus}(X_1)\big),\\
&X_3 \mid X_{1:2} \sim \mathrm{Beta}\big(\alpha = \mathrm{softplus}(X_2),\ \beta = \mathrm{softplus}(X_1)\big),\\
&X_4 \mid X_{1:3} \sim \mathrm{Gamma}\big(\text{shape} = \mathrm{softplus}(X_3),\ \text{scale} = \mathrm{softplus}(X_2)\big).
\end{aligned}
\]

R configuration used to generate samples:

```r
config <- list(
  list(distr = "norm", parm = NULL),
  list(distr = "exp",  parm = function(d) list(rate = softplus(d$X1))),
  list(distr = "beta",
       parm = function(d) list(shape1 = softplus(d$X2),
                               shape2 = softplus(d$X1))),
  list(distr = "gamma",
       parm = function(d) list(shape = softplus(d$X3),
                               scale = softplus(d$X2)))
)
```

All positive parameters are clamped to at least 1e-3 in the code to ensure well-defined likelihoods. The factorization is fully triangular: for each k, \(p_k(\cdot \mid x_{1:k-1})\) may depend on all predecessors.

## 1. Notation and standardization

- Data: \(X \in \mathbb{R}^{N \times K}\), rows \(X_i \in \mathbb{R}^K\).
- Train-only standardization (stored and reused on validation/test):
  \[
  x^{\text{std}} = \frac{x - \mu}{\sigma} \quad \text{(component-wise)}, \qquad
  \log J_{\text{std}}(x) = \sum_{k=1}^K (-\log \sigma_k).
  \]
- Triangular map: \(S(x) = (S_1(x_1), S_2(x_{1:2}), \ldots, S_K(x_{1:K}))\) with \(\partial_{x_k} S_k(x) > 0\).
- Reference density: \(\eta = \mathcal{N}(0, I)\).

Common per-dimension log-density (all TTM variants):
\[
\ell_k(x) \;=\; -\tfrac12\,z_k(x)^2 \;-\; \tfrac12 \log(2\pi) \;+\; \log \partial_{x_k} S_k(x), 
\quad z_k(x) := S_k(x_{1:k}).
\]
The train-only standardization contributes \(-\log \sigma_k\) to \(\log \partial_{x_k} S_k\). The joint log-density is \(\sum_{k=1}^K \ell_k(x)\).

## 2. Objective for learned TTM variants

We minimize the forward-KL functional using its Monte Carlo form:
\[
J(S) \;=\; \frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K}
\Big( \tfrac12\,S_k(X_i)^2 - \log \partial_{x_k} S_k(X_i) \Big)
\;=\; \sum_{k=1}^K J_k(S_k).
\]
This is the training criterion in `trainSeparableMap()` and `trainCrossTermMap()` (with simple \(\ell_2\) ridge). The marginal TTM uses a fast rank-to-Normal calibration instead of explicit KL optimization.

## 3. Reference models (TRUE) - detailed

### 3.1 TRUE (marginal)

- Model:
  \[
  p(x) \;=\; \prod_{k=1}^K p_k(x_k \mid \theta_k),
  \quad p_k \in \{\text{Normal}, \text{Exponential}, \text{Beta}, \text{Gamma}\}.
  \]
- Training (separate MLE per dimension):
  \[
  \hat{\theta}_k \;=\; \arg\max_{\theta} \sum_{i=1}^{N} \log p_k(X_{i,k} \mid \theta),
  \]
  using method-of-moments initializations and positivity bounds.
- Prediction: \(\ell_k(x) = \log p_k(x_k \mid \hat{\theta}_k)\).
- Scope: ignores all cross-dimensional dependence.

### 3.2 TRUE (joint/oracle)

- Factorization with full conditioning:
  \[
  p(x) \;=\; \prod_{k=1}^K \pi_k(x_k \mid x_{1:k-1}), \qquad
  \vartheta_k(x_{<k}) \;=\; \texttt{config[[k]]\$parm}(x_{1:k-1}).
  \]
  For \(k=3\) this conditions on \(x_1\) and \(x_2\) (not just one step back).
- Training: none (parameters are deterministic functions of \(x_{<k}\)).
- Prediction: \(\ell_k(x) = \log \pi_k(x_k \mid x_{1:k-1})\).
- Scope: oracle reference that exploits the full triangular structure of the DGP.

## 4. Transformation Random Forests (TRTF) - detailed

Goal: learn a conditional monotone transform \(T_k(\cdot \mid x_{<k})\) such that, approximately,
\[
Z_k \;:=\; T_k(X_k \mid X_{<k}) \sim \mathcal{N}(0,1).
\]

Per-dimension log-density (change-of-variables):
\[
\ell_k(x) \;=\; \log \phi\big(T_k(x_k \mid x_{<k})\big) \;+\; \log \partial_{x_k} T_k(x_k \mid x_{<k}),
\]
with \(\phi\) the standard normal density.

Training sketch (as implemented):
- \(k=1\): univariate transformation model (e.g., Box-Cox in `tram`).
- \(k \ge 2\): transformation forest (`traforest` in `trtf/tram`) with predictors \(x_{<k}\) and response \(x_k\). Forest weights yield a local transformation \(T_k\) and its derivative for the log-Jacobian term.

Comparison to TTM: TRTF constructs conditional transformations implicitly via forest weighting, while TTM parametrizes \(S_k\) explicitly with guaranteed monotonicity and optimizes the forward-KL objective.

## 5. Triangular Transport Maps (TTM)

### 5.1 Common evaluation pipeline

1. Standardize \(x \mapsto x^{\text{std}}\) using stored \((\mu, \sigma)\) from training data.
2. For \(k = 1{:}K\): compute \(z_k = S_k(x_{1:k})\) and \(\log \partial_{x_k} S_k(x) - \log \sigma_k\).
3. Compute \(\ell_k(x) = -\tfrac12 z_k^2 - \tfrac12 \log(2\pi) + \log \partial_{x_k} S_k(x)\).
4. Sum over \(k\) for the joint log-density.

### 5.2 TTM - Marginal (diagonal)

- Parametrization:
  \[
  S_k(x) \;=\; a_k + b_k\,x_k^{\text{std}}, \qquad b_k > 0 \ (\text{implemented as } b_k = \exp(\alpha_k)).
  \]
- Log-Jacobian: \(\log \partial_{x_k} S_k = \log b_k - \log \sigma_k\) (constant in \(x\)).
- Training (fast rank-to-Normal calibration):
  - \(u_i = \mathrm{rank}(x_{i,k}^{\text{std}})/(N+1)\), clamp to \([1/(N+1),\, N/(N+1)]\).
  - \(z_i^\star = \Phi^{-1}(u_i)\).
  - \(\hat{b}_k = \max\{0, \mathrm{Cov}(x^{\text{std}}, z^\star) / \mathrm{Var}(x^{\text{std}})\}\), 
    \(\hat{a}_k = \overline{z^\star} - \hat{b}_k\,\overline{x^{\text{std}}}\).
- Scope: captures univariate marginal shapes only.

### 5.3 TTM - Separable \((S_k = g_k + f_k)\)

- Parametrization:
  \[
  S_k(x_{1:k}) \;=\; g_k(x_{<k}) + f_k(x_k), \qquad f_k'(x_k) > 0.
  \]
  In code:
  - \(g_k(x_{<k}) = \Psi^{\text{non}}_k(x_{<k})\,c^{\text{non}}_k\) (polynomial basis),
  - \(f_k(x_k) = \Psi^{\text{mon}}_k(x_k)\,c^{\text{mon}}_k\) with \(\Psi^{\text{mon}} = [x, \mathrm{erf}(x)]\),
    \(f'_k(x) = B(x)\,c^{\text{mon}}\), \(B(x) = [1,\ 2/\sqrt{\pi}\,e^{-x^2}]\).
- Log-Jacobian: \(\log \partial_{x_k} S_k = \log(B(x_k)\,c^{\text{mon}}) - \log \sigma_k\).
- Training (forward-KL with ridge, eliminating \(g_k\)):
  - Let \(M = (\Psi^{\text{non}\top}\Psi^{\text{non}} + \lambda I)^{-1}\Psi^{\text{non}\top}\),
    \(A = (I - \Psi^{\text{non}}M)\Psi^{\text{mon}}\), \(D = M\Psi^{\text{mon}}\).
  - Optimize \(c := c^{\text{mon}} > 0\) by L-BFGS-B:
    \[
    \min_{c>0} \ \tfrac12 \|A c\|_2^2 - \mathbf{1}^\top \log(Bc) + \tfrac{\lambda}{2}(\|D c\|_2^2 + \|c\|_2^2).
    \]
- Scope: context \(x_{<k}\) shifts the level via \(g_k\); the slope in \(x_k\) is context-free.

### 5.4 TTM - Cross-term (integrated rectifier)

- Parametrization (monotonicity guaranteed):
  \[
  S_k(x_{1:k}) \;=\; g_k(x_{<k}) + \int_{0}^{x_k} \exp\big(h_k(t, x_{<k})\big)\,dt,
  \]
  with \(g_k(x_{<k}) = \Phi(x_{<k})^\top \alpha_k\) and \(h_k(t, x_{<k}) = \psi(t, x_{<k})^\top \beta_k\).
- Log-Jacobian: \(\log \partial_{x_k} S_k = h_k(x_k, x_{<k}) - \log \sigma_k\).
- Numerical quadrature (Gauss-Legendre on \([0,1]\), with exponent clipping):
  \[
  \int_{0}^{x_k} \exp h_k \ \approx\ x_k \sum_{q=1}^{Q} w_q \exp\big(h_k(s_q x_k, x_{<k})\big).
  \]
- Training (forward-KL with ridge):
  \[
  J_k(\alpha, \beta) \;=\; \tfrac1N \sum_i \Big( \tfrac12 S_k(X_i)^2 - h_k(X_{i,k}, X_{i,<k}) \Big) + \tfrac{\lambda}{2}(\|\alpha\|_2^2 + \|\beta\|_2^2).
  \]

## 6. Differential structure for optimization

Two notions of Jacobian are useful:

1. Data-Jacobian \(\partial_{x_k} S_k\) enters the log-density through \(\log \partial_{x_k} S_k\).
2. Parameter-Jacobian \(\partial_{\theta} S_k\) enters the gradient and Hessian of the training objective.

For a parameter vector \(\theta\) (per \(k\)) and ridge \(\lambda\):
\[
J_k(\theta) \;=\; \tfrac1N \sum_i \Big( \tfrac12 S_k(X_i;\theta)^2 - \log \partial_{x_k} S_k(X_i;\theta) \Big) + \tfrac{\lambda}{2}\|\theta\|_2^2.
\]

Gradient:
\[
\nabla_{\theta} J_k \;=\; \tfrac1N \sum_i \Big[ S_k(X_i;\theta)\,\partial_{\theta} S_k(X_i;\theta) \;-\; \partial_{\theta} \log \partial_{x_k} S_k(X_i;\theta) \Big] + \lambda\,\theta.
\]

Hessian (exact):
\[
\nabla_{\theta}^2 J_k \;=\; \tfrac1N \sum_i \Big[ (\partial_{\theta} S_k)(\partial_{\theta} S_k)^\top \;+\; S_k\,\partial_{\theta\theta}^2 S_k \;-\; \partial_{\theta\theta}^2 \log \partial_{x_k} S_k \Big] \;+\; \lambda I.
\]

Local quadratic approximation near \(\theta\):
\[
J_k(\theta + h) \;\approx\; J_k(\theta) \;+\; \nabla J_k(\theta)^\top h \;+\; \tfrac12\, h^\top \nabla^2 J_k(\theta)\, h.
\]

In practice, L-BFGS-B builds an inverse-Hessian approximation iteratively. A Gauss-Newton surrogate keeps only \(\sum_i (\partial_{\theta} S_k)(\partial_{\theta} S_k)^\top\), which is positive semidefinite and often numerically stable.

## 7. Reproducibility invariants

- Constants: \(-\tfrac12 \log(2\pi)\) per dimension; \(-\log \sigma_k\) from train-only standardization per dimension.
- Shapes: `predict(..., "logdensity_by_dim")` returns an \(N \times K\) matrix; the joint log-density is the row sum.
- Seeding: training routines set `set.seed(42)`; TRTF fit uses the same seed.
- Ranks and clamping (marginal TTM): `ties.method = "average"`, \(u \in [1/(N+1),\, N/(N+1)]\).
- Numerical safeguards: positivity constraints, exponent clipping in cross-term, no NA/Inf.

## 8. Computational cost (rules of thumb)

- TTM-Marginal: \(O(NK)\).
- TTM-Separable: \(O(NK \cdot m)\) with small number of monotone basis functions plus one-time projection cost.
- TTM-Cross-term: \(O(NKQ \cdot m_h)\) where \(Q\) is the number of quadrature nodes and \(m_h\) the number of basis functions for \(h_k\).
- TRTF: between \(O(NK \log N)\) and \(O(NK \cdot \text{\#trees})\) depending on settings.

## 9. Outcome table (NLL in nats; lower is better)

Values are mean per-dimension NLL \( \pm \) 2 standard errors; the last row shows column sums and the standard error of the total.

| dim | distribution | True (marginal) | True (Joint) | Random Forest | Marginal Map | Separable Map | Cross-term Map |
|:---:|:------------:|:---------------:|:------------:|:-------------:|:------------:|:-------------:|:--------------:|
| 1 | norm  | 1.55 \( \pm \) 0.47 | 1.54 \( \pm \) 0.48 | 1.62 \( \pm \) 0.42 | 1.57 \( \pm \) 0.38 | 1.55 \( \pm \) 0.47 | 1.50 \( \pm \) 0.46 |
| 2 | exp   | 1.65 \( \pm \) 0.63 | 1.65 \( \pm \) 0.87 | 2.73 \( \pm \) 1.00 | 3.30 \( \pm \) 0.01 | 1.91 \( \pm \) 0.65 | 0.54 \( \pm \) 1.72 |
| 3 | beta  | -0.75 \( \pm \) 1.23 | -1.23 \( \pm \) 1.69 | -0.12 \( \pm \) 0.52 | 0.41 \( \pm \) 0.22 | -0.00 \( \pm \) 0.63 | 0.77 \( \pm \) 7.75 |
| 4 | gamma | 2.93 \( \pm \) 1.97 | 2.25 \( \pm \) 1.47 | 2.72 \( \pm \) 1.98 | 3.32 \( \pm \) 1.45 | 3.36 \( \pm \) 2.68 | 1.58 \( \pm \) 0.34 |
| k | SUM   | 5.38 \( \pm \) 2.03 | 4.21 \( \pm \) 2.17 | 6.95 \( \pm \) 3.02 | 8.59 \( \pm \) 1.77 | 6.81 \( \pm \) 3.36 | 4.39 \( \pm \) 8.36 |

