# crossterm_basis.R
## Cross-term t-basis helpers with symmetric knots and tail features

# Open B-spline basis from explicit interior knots and boundary [a,b]
.bspline_basis_open_knots <- function(t, interior = numeric(0), degree = 3L, boundary = NULL) {
  stopifnot(degree == as.integer(degree), degree >= 0)
  t <- as.numeric(t)
  if (is.null(boundary)) {
    if (length(interior) == 0L) {
      boundary <- range(t)
    } else {
      boundary <- range(c(interior, t))
    }
  }
  a <- boundary[1]; b <- boundary[2]
  # Ensure strict ordering and within bounds
  interior <- sort(as.numeric(interior))
  interior <- interior[interior > a & interior < b]
  p <- as.integer(degree)
  n_basis <- length(interior) + p + 1L
  # Open uniform knot vector induced by interior+boundary
  knots <- c(rep(a, p + 1L), interior, rep(b, p + 1L))
  # degree-0 basis
  M <- length(t)
  N0 <- function(x, i) as.numeric(knots[i] <= x & x < knots[i + 1L])
  N0b <- function(x, i) {
    y <- N0(x, i)
    if (i == (length(knots) - 1L)) y[x >= knots[length(knots)]] <- 1
    y
  }
  B <- matrix(0, nrow = M, ncol = n_basis)
  for (i in seq_len(n_basis)) B[, i] <- N0b(t, i)
  if (p >= 1L) {
    for (q in 1:p) {
      Bq <- matrix(0, nrow = M, ncol = n_basis)
      for (i in seq_len(n_basis)) {
        denom1 <- knots[i + q] - knots[i]
        denom2 <- knots[i + q + 1L] - knots[i + 1L]
        a1 <- if (denom1 > 0) (t - knots[i]) / denom1 else 0
        a2 <- if (denom2 > 0) (knots[i + q + 1L] - t) / denom2 else 0
        Bq[, i] <- a1 * B[, i] + a2 * if (i + 1L <= n_basis) B[, i + 1L] else 0
      }
      B <- Bq
    }
  }
  attr(B, "knots") <- knots
  attr(B, "boundary") <- c(a, b)
  B
}

# Construct symmetric interior knots about 0 from empirical t-samples
build_symmetric_knots <- function(t_sample, df = 8L, degree = 3L, prob_hi = 0.995) {
  t_sample <- as.numeric(t_sample)
  t_sample <- t_sample[is.finite(t_sample)]
  if (length(t_sample) == 0L) return(list(interior = numeric(0), boundary = c(-1, 1)))
  p <- as.integer(degree)
  n_basis <- as.integer(df)
  n_int <- max(0L, n_basis - p - 1L)
  T <- as.numeric(stats::quantile(abs(t_sample), probs = prob_hi, names = FALSE))
  T <- max(T, 1.0)
  if (n_int == 0L) return(list(interior = numeric(0), boundary = c(-T, T)))
  m_half <- n_int %/% 2L
  probs <- if (m_half > 0L) seq(1, m_half) / (m_half + 1) else numeric(0)
  pos <- if (length(probs) > 0) as.numeric(stats::quantile(abs(t_sample), probs = probs, names = FALSE)) else numeric(0)
  pos <- pmin(pmax(pos, 1e-8), T * 0.999)
  if ((n_int %% 2L) == 1L) {
    interior <- c(-rev(pos), 0, pos)
  } else {
    interior <- c(-rev(pos), pos)
  }
  list(interior = as.numeric(interior), boundary = c(-T, T))
}

# Tail features that produce constant h in tails (per side)
tail_plateau_features <- function(t, boundary) {
  a <- boundary[1]; b <- boundary[2]
  left  <- as.numeric(t <= a)
  right <- as.numeric(t >= b)
  cbind(left, right)
}



# fit_ttm_crossterm.R
## Cross-term Triangular Transport Map (TTM)
## Concrete fitter and evaluator; no proxies, R-only, deterministic.

if (!exists("%||%")) `%||%` <- function(a, b) if (is.null(a)) b else a

# Robust loader for repo-local components (works from tests/scripts)
if (!exists(".source_repo_file")) {
  .source_repo_file <- function(rel_path) {
    cand <- character(0)
    cand <- c(cand, rel_path)
    this_file <- tryCatch(normalizePath(sys.frames()[[1]]$ofile, winslash = "/", mustWork = FALSE), error = function(e) NA_character_)
    if (is.character(this_file) && nzchar(this_file) && !is.na(this_file)) cand <- c(cand, file.path(dirname(this_file), basename(rel_path)))
    if (exists("root_path")) cand <- c(cand, file.path(root_path, rel_path))
    p <- normalizePath(getwd(), winslash = "/", mustWork = FALSE)
    for (i in 1:8) {
      if (file.exists(file.path(p, "00_globals.R"))) { cand <- c(cand, file.path(p, rel_path)); break }
      np <- dirname(p); if (identical(np, p)) break; p <- np
    }
    cand <- unique(cand)
    for (pth in cand) { if (file.exists(pth)) { source(pth); return(invisible(TRUE)) } }
    invisible(FALSE)
  }
}

# Source required TTM components
.source_repo_file(file.path("models", "ttm", "ttm_bases.R"))
.source_repo_file(file.path("models", "ttm", "crossterm_basis.R"))
.source_repo_file(file.path("models", "ttm", "ttm_core.R"))
.source_repo_file(file.path("models", "ttm", "ttm_optimize.R"))

.std_stats <- function(X) {
  mu <- colMeans(X)
  sigma <- apply(X, 2, sd) + .Machine$double.eps
  list(mu = mu, sigma = sigma)
}

.standardize <- function(X, mu, sigma) {
  sweep(sweep(X, 2, mu, "-"), 2, sigma, "/")
}

#' Fit cross-term TTM via maps-from-samples with GL quadrature
#'
#' @param data matrix X or list with X_tr, X_te
#' @param seed integer RNG seed
#' @param deg_g degree for predecessor polynomial features g(x_<k>)
#' @param df_t B-spline df for h-basis in t=x_k
#' @param Q Gaussâ€“Legendre nodes on [0,1]
#' @param lambda ridge weight (split internally into non/mon parts)
#' @param Hmax clamp for h before exponentiation
#' @param maxit optimizer max iterations (sets option cross.maxit)
#' @return list(S, NLL_train, NLL_test, time_train, time_pred)
fit_ttm_crossterm <- function(data, seed = 42L, deg_g = 2L, df_t = 6L, Q = 16L, lambda = 1e-3, Hmax = 20, maxit = NULL,
                              order_search = NULL) {
  set.seed(as.integer(seed))
  if (!is.null(maxit)) options(cross.maxit = as.integer(maxit))
  # Accept split or matrix
  S_in <- if (is.list(data) && !is.null(data$X_tr) && !is.null(data$X_te)) data else {
    stopifnot(is.matrix(data))
    if (!exists("split_data")) source("02_split.R")
    list(X_tr = split_data(data, seed)$X_tr, X_te = split_data(data, seed)$X_te)
  }
  X_tr <- as.matrix(S_in$X_tr); X_te <- as.matrix(S_in$X_te)
  K <- ncol(X_tr); N <- nrow(X_tr)

  # Helper: oriented integral and bound stats for given k (smooth bound semantics)
  .oriented_integral <- function(u, Uprev, beta, spec_h, nodes, weights, H, stats_env = NULL) {
    N <- length(u)
    I <- rep(0, N)
    max_abs_h <- -Inf
    max_exp_ht <- -Inf
    max_abs_h_t <- -Inf
    n_bound_pos <- 0L
    n_bound_neg <- 0L
    for (q in seq_along(nodes)) {
      tq <- u * nodes[q]
      Hq <- build_h(tq, Uprev, spec_h)
      hq <- as.numeric(Hq %*% beta)
      # Count bound applications before exponentiation
      n_bound_pos <- n_bound_pos + sum(hq > H, na.rm = TRUE)
      n_bound_neg <- n_bound_neg + sum(hq < -H, na.rm = TRUE)
      max_abs_h <- max(max_abs_h, max(abs(hq), na.rm = TRUE))
      htil <- pmin(pmax(hq, -H), H)
      eh <- exp(htil)
      max_abs_h_t <- max(max_abs_h_t, max(abs(htil), na.rm = TRUE))
      max_exp_ht <- max(max_exp_ht, max(eh, na.rm = TRUE))
      I <- I + weights[q] * eh
    }
    if (!is.null(stats_env)) {
      stats_env$n_bound_pos <- stats_env$n_bound_pos + as.integer(n_bound_pos)
      stats_env$n_bound_neg <- stats_env$n_bound_neg + as.integer(n_bound_neg)
      stats_env$max_abs_h_raw <- max(stats_env$max_abs_h_raw, max_abs_h)
      stats_env$max_exp_h_bound <- max(stats_env$max_exp_h_bound, max_exp_ht)
      stats_env$max_abs_h_bound <- max(stats_env$max_abs_h_bound, max_abs_h_t)
    }
    u * I
  }

  # Prepare symmetric t-basis spec from empirical t-samples across train
  Q_eff <- .get_cross_quad_nodes(Q)
  gl <- gauss_legendre_nodes(as.integer(Q_eff))
  nodes <- gl$nodes; weights <- gl$weights
  # Standardize using training to get U
  st0 <- .std_stats(as.matrix(if (is.list(data) && !is.null(data$X_tr)) data$X_tr else data))
  mu0 <- st0$mu; sigma0 <- st0$sigma
  X0 <- if (is.list(data) && !is.null(data$X_tr)) as.matrix(data$X_tr) else as.matrix(data)
  U0 <- .standardize(X0, mu0, sigma0)
  # Sample t = u * xi across dims and nodes (subsample rows for efficiency)
  set.seed(as.integer(seed))
  N0 <- nrow(U0); K0 <- ncol(U0)
  idx <- if (N0 > 200) sample.int(N0, 200) else seq_len(N0)
  U_s <- as.matrix(U0[idx, , drop = FALSE])
  t_list <- lapply(seq_len(K0), function(k) as.numeric(U_s[, k]) %o% nodes)
  t_sample <- as.numeric(do.call(c, lapply(t_list, as.vector)))
  # Build symmetric knots and boundary; choose effective df as option-capped
  df_opt <- .get_cross_df_t(df_t)
  df_eff <- as.integer(min(df_opt, max(4L, floor(nrow(U_s) / 10))))
  deg_bs <- 3L
  sk <- build_symmetric_knots(t_sample, df = df_eff, degree = deg_bs, prob_hi = 0.999)
  # Effective g-degree (respect user-specified value)
  deg_g_eff <- as.integer(deg_g)
  spec_h_global <- list(df = df_eff, degree = deg_bs, deg_g = as.integer(deg_g_eff),
                        knots_t = sk$interior, boundary_t = sk$boundary, tail_const = TRUE)

  # Optional: internal ordering search (greedy adjacent swaps) on a fit/val split
  if (!is.null(order_search)) {
    set.seed(as.integer(seed))
    val_frac <- tryCatch(as.numeric(order_search$val_frac), error = function(e) NA_real_)
    if (!is.finite(val_frac) || val_frac <= 0 || val_frac >= 0.9) val_frac <- 0.2
    idx <- sample.int(N)
    n_fit <- as.integer(max(2L, floor((1 - val_frac) * N)))
    id_fit <- idx[seq_len(n_fit)]; id_val <- idx[(n_fit + 1L):N]
    X_fit <- X_tr[id_fit, , drop = FALSE]
    X_val <- X_tr[id_val, , drop = FALSE]
    # Helper: score a permutation via short cross-term fit and NLL on val
    score_perm <- function(perm) {
      ds <- list(X_tr = X_fit[, perm, drop = FALSE], X_te = X_val[, perm, drop = FALSE])
      t0 <- proc.time()[3]
      fast <- fit_ttm_crossterm(ds, seed = seed, deg_g = deg_g_eff, df_t = df_t, Q = Q,
                                lambda = NA_real_, Hmax = Hmax,
                                maxit = as.integer(order_search$maxit_fast %||% 50L),
                                order_search = NULL)
      tr_sec <- proc.time()[3] - t0
      nll_val <- tryCatch(mean(-predict_ttm_crossterm(fast$S, X_val[, perm, drop = FALSE], type = "logdensity")),
                          error = function(e) Inf)
      list(nll = nll_val, train_sec = tr_sec)
    }
    # Initial candidates: identity and chol_pivot if available
    id <- seq_len(K)
    best <- list(perm = id, nll = Inf, train_sec = NA_real_)
    rows <- list(); step <- 0L
    eval_and_log <- function(perm, accepted = FALSE) {
      sc <- score_perm(perm)
      step <<- step + 1L
      rows[[length(rows) + 1L]] <<- data.frame(step = step,
                                               perm_csv = paste(perm, collapse = ","),
                                               nll_val = sc$nll,
                                               accepted = as.logical(accepted),
                                               train_sec = sc$train_sec,
                                               stringsAsFactors = FALSE)
      sc
    }
    sc_id <- eval_and_log(id, accepted = FALSE)
    best <- list(perm = id, nll = sc_id$nll, train_sec = sc_id$train_sec)
    # chol_pivot start
    cp <- NULL
    if (exists("learn_ordering")) {
      cp <- tryCatch(learn_ordering(list(X_tr = X_fit), seed = seed, method = "chol_pivot", gaussianize = TRUE)$perm,
                     error = function(e) NULL)
    }
    if (!is.null(cp) && length(cp) == K) {
      sc_cp <- eval_and_log(cp, accepted = FALSE)
      if (sc_cp$nll + 1e-12 < best$nll) best <- list(perm = cp, nll = sc_cp$nll, train_sec = sc_cp$train_sec)
    }
    # Greedy adjacent swaps
    max_orders <- as.integer(order_search$max_orders %||% 12L)
    eps_impr <- as.numeric(order_search$eps %||% 1e-3)
    steps <- 0L
    repeat {
      if (steps >= max_orders) break
      perms <- lapply(seq_len(K - 1L), function(i) { p <- best$perm; tmp <- p[i]; p[i] <- p[i + 1L]; p[i + 1L] <- tmp; p })
      scores <- lapply(perms, function(p) eval_and_log(p, accepted = FALSE))
      nlls <- vapply(scores, `[[`, numeric(1), "nll")
      idx_best <- which.min(nlls)
      if (length(idx_best) == 0L) break
      if (nlls[idx_best] + eps_impr < best$nll) {
        best$perm <- perms[[idx_best]]; best$nll <- nlls[idx_best]
        # mark acceptance
        rows[[length(rows) + 1L]] <- data.frame(step = step + 1L,
                                               perm_csv = paste(best$perm, collapse = ","),
                                               nll_val = best$nll,
                                               accepted = TRUE,
                                               train_sec = scores[[idx_best]]$train_sec,
                                               stringsAsFactors = FALSE)
        steps <- steps + 1L
      } else break
    }
    # Persist search log
    dir.create("artifacts", showWarnings = FALSE)
    try(utils::write.csv(do.call(rbind, rows), file = file.path("artifacts", "order_search.csv"), row.names = FALSE), silent = TRUE)
    # Reorder training/test for final fit
    X_tr <- X_tr[, best$perm, drop = FALSE]
    X_te <- X_te[, best$perm, drop = FALSE]
    final_perm <- best$perm
  } else {
    final_perm <- seq_len(K)
  }

  # Optional sparsity masks for predecessor features based on correlation threshold
  sparsity_tau <- tryCatch(getOption("cross.sparsity_tau", 0.05), error = function(e) 0.05)
  mask_list <- vector("list", K)
  if (is.finite(sparsity_tau) && sparsity_tau > 0) {
    st_all <- .std_stats(X_tr)
    Z_all <- .standardize(X_tr, st_all$mu, st_all$sigma)
    R <- tryCatch(stats::cor(Z_all), error = function(e) NULL)
    if (is.null(R) || any(!is.finite(R))) R <- matrix(0, nrow = K, ncol = K)
    for (k in seq_len(K)) {
      p <- k - 1L
      if (p <= 0L) { mask_list[[k]] <- matrix(FALSE, nrow = 0, ncol = deg_g_eff); next }
      mk <- matrix(TRUE, nrow = p, ncol = deg_g_eff)
      for (j in seq_len(p)) {
        if (abs(R[j, k]) < sparsity_tau) mk[j, ] <- FALSE
      }
      mask_list[[k]] <- mk
    }
  } else {
    for (k in seq_len(K)) mask_list[[k]] <- matrix(TRUE, nrow = max(0, k - 1L), ncol = deg_g_eff)
  }

  # Train
  time_train <- system.time({
    st <- .std_stats(X_tr); mu <- st$mu; sigma <- st$sigma
    Xs <- .standardize(X_tr, mu, sigma)
    coeffs <- vector("list", K)
    spec_h <- spec_h_global; gl_nodes <- nodes; gl_weights <- weights
    # Bound event log accumulator
    bound_log <- list()
    for (k in seq_len(K)) {
      x_prev <- if (k > 1) Xs[, 1:(k - 1), drop = FALSE] else matrix(0, N, 0)
      xk <- Xs[, k]
      spec_k <- spec_h_global; spec_k$mask_g <- mask_list[[k]]
      optk <- .opt_crossterm_k(x_prev, xk, deg_g = deg_g_eff, df_t = df_t, lambda = lambda, Q = Q, Hmax = Hmax, k = k,
                               spec_h = spec_k, nodes_override = nodes, weights_override = weights)
      coeffs[[k]] <- list(alpha = optk$alpha, beta = optk$beta)
      # spec_h already set globally
      # Post-fit bound statistics on training set for this k
      stats_env <- new.env(parent = emptyenv())
      stats_env$n_bound_pos <- 0L
      stats_env$n_bound_neg <- 0L
      stats_env$max_abs_h_raw  <- -Inf
      stats_env$max_exp_h_bound <- -Inf
      stats_env$max_abs_h_bound <- -Inf
      invisible(.oriented_integral(xk, x_prev, coeffs[[k]]$beta, spec_k, gl_nodes, gl_weights, Hmax, stats_env))
      bound_log[[k]] <- data.frame(k = k,
                                   iter = NA_integer_,
                                   n_bound_pos = as.integer(stats_env$n_bound_pos),
                                   n_bound_neg = as.integer(stats_env$n_bound_neg),
                                   max_abs_h_raw = as.numeric(stats_env$max_abs_h_raw),
                                   max_abs_h_bound = as.numeric(stats_env$max_abs_h_bound),
                                   max_exp_h_bound = as.numeric(stats_env$max_exp_h_bound))
      msg <- sprintf("[CTM][k=%d] n_bound_pos=%d n_bound_neg=%d max|h|_bound=%.6g max|h|_raw=%.6g max exp(h_bound)=%.6g",
                     k, bound_log[[k]]$n_bound_pos, bound_log[[k]]$n_bound_neg,
                     bound_log[[k]]$max_abs_h_bound, bound_log[[k]]$max_abs_h_raw, bound_log[[k]]$max_exp_h_bound)
      try(message(msg), silent = TRUE)
    }
    S <- list(
      algo = "crossterm",
      mu = mu, sigma = sigma,
      coeffs = coeffs,
      spec_h = spec_h,
      gl_nodes = gl_nodes,
      gl_weights = gl_weights,
      Hmax = Hmax,
      deg_g = as.integer(deg_g_eff),
      order = as.integer(final_perm),
      feat_names = colnames(X_tr)
    )
    class(S) <- "ttm_cross_term"
    assign(".last_fit_ttm_crossterm", S, envir = .GlobalEnv)
    # Persist clip log CSV
    dir.create("artifacts", showWarnings = FALSE)
    cldf <- do.call(rbind, bound_log)
    utils::write.csv(cldf, file = file.path("artifacts", "cross_clip_events.csv"), row.names = FALSE)
    # Basis summary logging
    # Summarize t-basis on sample t_sample
    Bt_s <- .bspline_basis_open_knots(t_sample, interior = spec_h$knots_t %||% numeric(0), degree = spec_h$degree %||% 3L, boundary = spec_h$boundary_t %||% range(t_sample))
    if (isTRUE(spec_h$tail_const %||% TRUE)) Bt_s <- cbind(Bt_s, tail_plateau_features(t_sample, spec_h$boundary_t))
    Gx_s <- if (ncol(Xs) > 1) build_g(matrix(0, nrow = length(t_sample), ncol = ncol(Xs) - 1), deg = spec_h$deg_g) else matrix(1, nrow = length(t_sample), ncol = 1)
    H_dim <- ncol(Bt_s) * ncol(Gx_s)
    basis_df <- data.frame(
      t_basis_dim = ncol(Bt_s), g_dim = ncol(Gx_s), h_dim = H_dim,
      degree = spec_h$degree, df = spec_h$df, boundary_left = spec_h$boundary_t[1], boundary_right = spec_h$boundary_t[2],
      stringsAsFactors = FALSE
    )
    # Add simple means/sds for t-basis columns
    cm <- colMeans(Bt_s); cs <- apply(Bt_s, 2, sd)
    # Write CSV with first few moments and knots
    kn <- spec_h$knots_t %||% numeric(0)
    basis_df$knots <- paste(sprintf("%.6g", kn), collapse = ",")
    basis_df$colmean_head <- paste(sprintf("%.6g", head(cm, min(5, length(cm)))), collapse = ",")
    basis_df$colsd_head <- paste(sprintf("%.6g", head(cs, min(5, length(cs)))), collapse = ",")
    utils::write.csv(basis_df, file = file.path("artifacts", "cross_basis_dims.csv"), row.names = FALSE)
  })[["elapsed"]]

  # Predict-once timing and numeric sanity
  time_pred <- system.time({ invisible(predict_ttm(S, X_te, type = "logdensity_by_dim")) })[["elapsed"]]
  LD_tr <- predict_ttm(S, X_tr, type = "logdensity_by_dim")
  LD_te <- predict_ttm_crossterm(S, X_te, type = "logdensity_by_dim")
  stopifnot(is.matrix(LD_tr), is.matrix(LD_te), all(is.finite(LD_tr)), all(is.finite(LD_te)))

  # Jacobian positivity check (train/test)
  J_tr <- predict_ttm(S, X_tr, type = "jac_diag")
  J_te <- predict_ttm(S, X_te, type = "jac_diag")
  if (any(!(J_tr > 0)) || any(!(J_te > 0))) {
    bad <- which(!(J_te > 0), arr.ind = TRUE)
    if (length(bad) == 0) bad <- which(!(J_tr > 0), arr.ind = TRUE)
    i <- bad[1, 1]; k <- bad[1, 2]
    # Recompute raw h for offending point on test
    mu <- S$mu; sigma <- S$sigma
    U_te <- .standardize(X_te, mu, sigma)
    u <- U_te[i, k]
    Uprev <- if (k > 1) U_te[i, 1:(k - 1), drop = FALSE] else matrix(0, 1, 0)
    Hstar <- build_h(u, Uprev, S$spec_h)
    h_val <- as.numeric(Hstar %*% S$coeffs[[k]]$beta)
    stop(sprintf("Nonpositive Jacobian detected at i=%d, k=%d, u=%.6g, h(u)=%.6g", i, k, u, h_val))
  }

  # Oriented sign checks on held-out set
  U_te <- .standardize(X_te, S$mu, S$sigma)
  N_te <- nrow(U_te)
  rows <- list()
  for (k in seq_len(ncol(U_te))) {
    Uprev <- if (k > 1) U_te[, 1:(k - 1), drop = FALSE] else matrix(0, N_te, 0)
    u <- U_te[, k]
    Iu <- .oriented_integral(u, Uprev, S$coeffs[[k]]$beta, S$spec_h, S$gl_nodes, S$gl_weights, S$Hmax)
    int_code <- sign(Iu)
    sgn_u <- sign(u)
    sign_ok <- (int_code == sgn_u) | ((u == 0) & (Iu == 0))
    rows[[k]] <- data.frame(i = seq_len(N_te), k = k, u_k = as.numeric(u), int_code = as.integer(int_code), sign_ok = as.logical(sign_ok))
  }
  sign_df <- do.call(rbind, rows)
  dir.create("artifacts", showWarnings = FALSE)
  utils::write.csv(sign_df, file = file.path("artifacts", "cross_sign_checks.csv"), row.names = FALSE)

  # Monotonicity-of-integral check: u grid, per-sample curves must be non-decreasing
  grid_u <- seq(-4, 4, length.out = 41)
  viol <- list()
  for (k in seq_len(ncol(U_te))) {
    Uprev <- if (k > 1) U_te[, 1:(k - 1), drop = FALSE] else matrix(0, N_te, 0)
    for (i in seq_len(N_te)) {
      up <- if (ncol(Uprev) > 0) matrix(Uprev[i, , drop = FALSE], nrow = length(grid_u), ncol = ncol(Uprev), byrow = TRUE) else matrix(0, length(grid_u), 0)
      Igrid <- .oriented_integral(grid_u, up, S$coeffs[[k]]$beta, S$spec_h, S$gl_nodes, S$gl_weights, S$Hmax)
      ok <- all(diff(Igrid) >= -1e-8)  # numerical tolerance
      if (!ok) viol[[length(viol) + 1L]] <- data.frame(i = i, k = k, violations = sum(diff(Igrid) < -1e-8), total_grid = length(grid_u))
    }
  }
  vio_df <- if (length(viol) > 0) do.call(rbind, viol) else data.frame(i = integer(0), k = integer(0), violations = integer(0), total_grid = integer(0))
  utils::write.csv(vio_df, file = file.path("artifacts", "cross_integral_checks.csv"), row.names = FALSE)
  if (nrow(vio_df) > 0) {
    msg <- sprintf("Monotonicity-of-integral violated in %d cases", nrow(vio_df))
    if (isTRUE(getOption("cross.strict_monotone", FALSE))) stop(msg) else {
      warning(msg)
      try(message(paste("[WARN]", msg)), silent = TRUE)
    }
  }

  # Tail slope report for |u|>4: slope b = exp(h) in tails
  for (k in seq_len(ncol(U_te))) {
    u <- U_te[, k]
    idx_tail <- which(abs(u) >= 4)
    if (length(idx_tail) > 0) {
      Uprev <- if (k > 1) U_te[idx_tail, 1:(k - 1), drop = FALSE] else matrix(0, length(idx_tail), 0)
      Hst <- build_h(u[idx_tail], Uprev, S$spec_h)
      h_raw <- as.numeric(Hst %*% S$coeffs[[k]]$beta)
      h_til <- pmin(pmax(h_raw, -S$Hmax), S$Hmax)
      b <- exp(h_til)
      try(message(sprintf("[TAIL][k=%d] |u|>=4 slope b range: [%.6g, %.6g]", k, min(b), max(b))), silent = TRUE)
    } else {
      try(message(sprintf("[TAIL][k=%d] no samples with |u|>=4 in test set", k)), silent = TRUE)
    }
  }

  list(S = S,
       NLL_train = mean(-predict_ttm(S, X_tr, type = "logdensity")),
       NLL_test  = mean(-predict_ttm_crossterm(S, X_te,  type = "logdensity")),
       time_train = time_train, time_pred = time_pred)
}

# Convenience evaluator; delegates to generic predict_ttm
predict_ttm_crossterm <- function(object, newdata, type = c("logdensity_by_dim", "logdensity")) {
  predict_ttm(object, newdata, match.arg(type))
}

# S3 predict wrapper for cross-term models
predict.ttm_cross_term <- function(object, newdata, type = c("logdensity_by_dim", "logdensity"), ...) {
  type <- match.arg(type)
  S <- object
  X <- as.matrix(newdata)
  K <- ncol(X)
  # Permute columns to training order if available
  Xp <- X
  used_perm <- FALSE
  if (!is.null(S$feat_names) && !is.null(colnames(X)) && length(S$feat_names) == K) {
    idx <- match(S$feat_names, colnames(X))
    if (all(is.finite(idx))) { Xp <- X[, idx, drop = FALSE]; used_perm <- TRUE }
  } else if (!is.null(S$order) && length(S$order) == K) {
    Xp <- X[, S$order, drop = FALSE]; used_perm <- TRUE
  }
  LDp <- predict_ttm(S, Xp, type = "logdensity_by_dim")
  stopifnot(is.matrix(LDp), ncol(LDp) == K)
  # Map back to input order
  if (used_perm) {
    if (!is.null(colnames(newdata)) && !is.null(S$feat_names) && length(S$feat_names) == K) {
      idx_back <- match(colnames(newdata), S$feat_names)
      LD <- LDp[, idx_back, drop = FALSE]
    } else if (!is.null(S$order) && length(S$order) == K) {
      inv <- integer(K); inv[S$order] <- seq_len(K)
      LD <- LDp[, inv, drop = FALSE]
    } else {
      LD <- LDp
    }
  } else {
    LD <- LDp
  }
  if (type == "logdensity_by_dim") return(LD)
  rowSums(LD)
}

# Compatibility wrapper mirroring separable/marginal helpers
trainCrossTermMap <- function(S, seed = 42, ...) {
  fit_ttm(S, algo = "crossterm", seed = seed, ...)
}


# order_heuristics.R
# Order learning heuristics for triangular transport maps (base R only)

#' Save permutation as CSV
#' @param perm integer permutation vector (length K)
#' @param path output CSV path (default 'artifacts/order_perm.csv')
save_perm <- function(perm, path = "artifacts/order_perm.csv") {
  stopifnot(is.integer(perm) || is.numeric(perm))
  perm <- as.integer(perm)
  dir.create(dirname(path), showWarnings = FALSE, recursive = TRUE)
  df <- data.frame(k = seq_along(perm), perm = perm)
  utils::write.csv(df, file = path, row.names = FALSE)
  invisible(path)
}

#' Learn a variable ordering via pivoted Cholesky
#'
#' @param S_or_X list with X_tr, X_te or numeric matrix X (train-only used)
#' @param seed integer RNG seed for determinism
#' @param method one of c("chol_pivot","identity")
#' @param gaussianize logical; if TRUE apply PIT->qnorm, else z-standardize only
#' @return list(perm=integer, method=character, gaussianize=logical)
learn_ordering <- function(S_or_X, seed = 42L, method = c("chol_pivot", "identity"), gaussianize = TRUE) {
  method <- match.arg(method)
  set.seed(as.integer(seed))

  # Extract training matrix
  if (is.list(S_or_X) && !is.null(S_or_X$X_tr)) {
    X_tr <- as.matrix(S_or_X$X_tr)
  } else {
    X_tr <- as.matrix(S_or_X)
  }
  stopifnot(is.matrix(X_tr), is.numeric(X_tr))
  N <- nrow(X_tr); K <- ncol(X_tr)
  if (K < 2L || method == "identity") {
    perm <- seq_len(K)
    save_perm(perm)
    return(list(perm = as.integer(perm), method = method, gaussianize = gaussianize))
  }

  # Train-only standardization
  mu <- colMeans(X_tr)
  sigma <- apply(X_tr, 2, sd) + .Machine$double.eps
  Xs <- sweep(sweep(X_tr, 2, mu, "-"), 2, sigma, "/")

  # Gaussianize via PIT with ties.average and clipping to (0,1)
  if (isTRUE(gaussianize)) {
    eps <- 1 / (N + 1)
    U <- matrix(NA_real_, N, K)
    for (k in seq_len(K)) {
      r <- rank(Xs[, k], ties.method = "average")
      u <- r / (N + 1)
      # Clip to [eps, 1-eps]
      u <- pmin(pmax(u, eps), 1 - eps)
      U[, k] <- u
    }
    Z <- stats::qnorm(U)
  } else {
    Z <- Xs
  }

  # Correlation and pivoted Cholesky
  R <- tryCatch(stats::cor(Z), error = function(e) NULL)
  perm <- seq_len(K)
  if (is.null(R) || any(!is.finite(R))) {
    warning("cor() failed or produced non-finite values; returning identity permutation")
  } else {
    cp <- tryCatch(chol(R, pivot = TRUE), error = function(e) NULL)
    if (is.null(cp)) {
      warning("chol(pivot=TRUE) failed; returning identity permutation")
    } else {
      pv <- attr(cp, "pivot")
      if (is.null(pv) || length(pv) != K) {
        warning("pivot attribute missing or invalid; returning identity permutation")
      } else {
        perm <- as.integer(pv)
      }
    }
  }

  save_perm(perm)
  list(perm = as.integer(perm), method = method, gaussianize = gaussianize)
}



# rbf_basis.R
# Radial Basis helpers for t-basis (base R only)

# Local coordinates (dimensionless)
rbf_local_coords <- function(x, mu, sigma) {
  s <- pmax(as.numeric(sigma), .Machine$double.eps)
  (as.numeric(x) - as.numeric(mu)) / s
}

# Gaussian RBF in local coords
rbf <- function(xloc) {
  exp(-0.5 * (as.numeric(xloc)^2))
}

# Integral of Gaussian RBF in local coords, such that d/dxloc irbf = rbf
# Using relation erf(x/sqrt(2)) = 2*pnorm(x) - 1
irbf <- function(xloc) {
  sqrt(pi / 2) * (2 * stats::pnorm(as.numeric(xloc)) - 1)
}

# Left-end tail term (smooth, asymptotically linear with slope 1 as x -> -inf)
# xloc = (x - a) / sigma  (a is left boundary), tau = sigma
let_term <- function(xloc, sigma) {
  tau <- pmax(as.numeric(sigma), .Machine$double.eps)
  # LET(x) = -tau * log1p(exp(-xloc)) + tau * log(2)
  -tau * log1p(exp(-as.numeric(xloc))) + tau * log(2)
}

# Right-end tail term (smooth, asymptotically linear with slope 1 as x -> +inf)
# xloc = (x - b) / sigma  (b is right boundary), tau = sigma
ret_term <- function(xloc, sigma) {
  tau <- pmax(as.numeric(sigma), .Machine$double.eps)
  # RET(x) =  tau * log1p(exp( xloc)) - tau * log(2)
  tau * log1p(exp(as.numeric(xloc))) - tau * log(2)
}

# Place RBF knots by empirical quantiles; local scales by neighbor distances
# Returns list(mu = centers, sigma = local_scales)
place_rbf_knots <- function(xk, m) {
  xk <- as.numeric(xk)
  m <- as.integer(m)
  stopifnot(m >= 1L)
  if (length(xk) == 0L) return(list(mu = numeric(0), sigma = numeric(0)))
  # Probabilities avoid exact 0/1 to keep centers within data range
  probs <- if (m == 1L) 0.5 else seq(1, m) / (m + 1)
  mu <- as.numeric(stats::quantile(xk, probs = probs, names = FALSE, type = 7))
  mu <- sort(unique(mu))
  L <- length(mu)
  if (L == 0L) return(list(mu = numeric(0), sigma = numeric(0)))
  # Local sigma from neighbor distances (half of adjacent gaps)
  sig <- rep(NA_real_, L)
  if (L == 1L) {
    r <- stats::IQR(xk)
    if (!is.finite(r) || r <= 0) r <- sd(xk)
    if (!is.finite(r) || r <= 0) r <- max(1.0, diff(range(xk)))
    sig[1] <- r / 6
  } else {
    gaps <- diff(mu)
    sig[1] <- gaps[1] / 2
    sig[L] <- gaps[L - 1] / 2
    if (L > 2L) for (j in 2:(L - 1)) sig[j] <- 0.5 * (mu[j + 1] - mu[j - 1]) / 2
  }
  sig <- pmax(sig, .Machine$double.eps)
  list(mu = mu, sigma = sig)
}

# Build t-basis with integrated RBF columns + smooth tail-linear terms
# df: number of RBF centers; prob_hi: sets tail boundaries via quantiles
build_t_rbf <- function(t, df = 8L, prob_hi = 0.995) {
  t <- as.numeric(t)
  N <- length(t)
  m <- as.integer(df)
  # RBF centers/scales
  knots <- place_rbf_knots(t, m)
  mu <- knots$mu; sg <- knots$sigma
  M <- length(mu)
  # iRBF columns (scale back to t via sigma_j)
  Phi <- if (M > 0L) {
    cols <- lapply(seq_len(M), function(j) {
      xloc <- rbf_local_coords(t, mu[j], sg[j])
      sg[j] * irbf(xloc)
    })
    do.call(cbind, cols)
  } else matrix(0, N, 0)
  # Tail boundaries a,b from quantiles
  a <- as.numeric(stats::quantile(t, probs = 1 - prob_hi, names = FALSE))
  b <- as.numeric(stats::quantile(t, probs = prob_hi, names = FALSE))
  if (!is.finite(a)) a <- min(t)
  if (!is.finite(b)) b <- max(t)
  if (a > b) { tmp <- a; a <- b; b <- tmp }
  # Tail smoothing scale from global spread
  tau <- max(.Machine$double.eps, 0.5 * stats::IQR(t))
  if (!is.finite(tau) || tau <= 0) tau <- max(.Machine$double.eps, sd(t))
  if (!is.finite(tau) || tau <= 0) tau <- 1.0
  # LET/RET terms
  xloc_L <- (t - a) / tau
  xloc_R <- (t - b) / tau
  LET <- let_term(xloc_L, tau)
  RET <- ret_term(xloc_R, tau)
  # Assemble basis: [iRBF..., LET, RET]
  out <- cbind(Phi, LET, RET)
  colnames(out) <- c(if (M > 0L) paste0("iRBF_", seq_len(M)) else character(0), "LET", "RET")
  stopifnot(is.matrix(out), nrow(out) == N, all(is.finite(out)))
  out
}



# ttm_bases.R
# Unified design/basis builders for TTM variants (pure base R)

# Robust repo-local source helper (works from tests or scripts)
.source_repo_file <- function(rel_path) {
  cand <- character(0)
  # 1) relative to current working directory
  cand <- c(cand, rel_path)
  # 2) relative to this file if available
  this_file <- tryCatch(normalizePath(sys.frames()[[1]]$ofile, winslash = "/", mustWork = FALSE), error = function(e) NA_character_)
  if (is.character(this_file) && nzchar(this_file) && !is.na(this_file)) {
    cand <- c(cand, file.path(dirname(this_file), basename(rel_path)))
  }
  # 3) via root_path if defined
  if (exists("root_path")) cand <- c(cand, file.path(root_path, rel_path))
  # 4) walk up to find 00_globals.R as repo root
  p <- normalizePath(getwd(), winslash = "/", mustWork = FALSE)
  for (i in 1:8) {
    if (file.exists(file.path(p, "00_globals.R"))) { cand <- c(cand, file.path(p, rel_path)); break }
    np <- dirname(p); if (identical(np, p)) break; p <- np
  }
  cand <- unique(cand)
  for (pth in cand) { if (file.exists(pth)) { source(pth); return(invisible(TRUE)) } }
  invisible(FALSE)
}

# Helper: replicate a row vector into an n-row matrix
rep_row <- function(vec, n) {
  v <- as.numeric(vec)
  matrix(rep(v, each = n), nrow = n, ncol = length(v))
}

# Gaussâ€“Legendre nodes and weights on [0,1]
gauss_legendre_nodes <- function(Q) {
  stopifnot(is.numeric(Q), length(Q) == 1L, Q >= 1, Q == as.integer(Q))
  Q <- as.integer(Q)
  if (Q == 1L) return(list(nodes = 0.5, weights = 1.0))
  i <- seq_len(Q - 1L)
  b <- i / sqrt(4 * i^2 - 1)
  J <- matrix(0, Q, Q)
  for (k in i) {
    J[k, k + 1L] <- b[k]
    J[k + 1L, k] <- b[k]
  }
  e <- eigen(J, symmetric = TRUE)
  # Map from [-1,1] to [0,1]
  x <- (e$values + 1) / 2
  w <- (2 * (e$vectors[1L, ]^2)) / 2
  list(nodes = as.numeric(x), weights = as.numeric(w))
}

# Alias commonly used name
nodes_GL <- gauss_legendre_nodes

# Monotone 1-D basis in x: columns [x, erf(x)]
build_f <- function(x) {
  x <- as.numeric(x)
  cbind(x, 2 * stats::pnorm(x * sqrt(2)) - 1)
}

# Derivative of build_f: columns [1, 2/sqrt(pi)*exp(-x^2)]
d_build_f <- function(x) {
  x <- as.numeric(x)
  cbind(rep(1, length(x)), 2 / sqrt(pi) * exp(-x^2))
}

# Polynomial features in predecessors (without x_k)
# Returns intercept + powers up to degree 'deg' for each column in X_prev
build_g <- function(X_prev, deg = 3L, mask = NULL) {
  X_prev <- as.matrix(X_prev)
  stopifnot(deg == as.integer(deg), deg >= 0)
  N <- nrow(X_prev)
  p <- if (is.null(dim(X_prev))) 1L else ncol(X_prev)
  if (p == 0L) return(matrix(0, nrow = N, ncol = 0))
  out <- matrix(1, nrow = N, ncol = 1)
  if (deg >= 1L) {
    for (j in seq_len(p)) {
      xj <- X_prev[, j]
      for (d in seq_len(deg)) {
        out <- cbind(out, xj^d)
      }
    }
  }
  # Optional masking of polynomial powers per predecessor (p x deg logical)
  if (!is.null(mask)) {
    M <- try(as.matrix(mask), silent = TRUE)
    if (is.matrix(M) && nrow(M) == p && ncol(M) == deg) {
      # Columns layout after intercept: for j=1..p, blocks of size 'deg' in order d=1..deg
      keep <- rep(TRUE, 1 + p * deg)
      idx <- 2L
      for (j in seq_len(p)) {
        for (d in seq_len(deg)) {
          if (isFALSE(M[j, d])) keep[idx] <- FALSE
          idx <- idx + 1L
        }
      }
      # Zero-out excluded columns to keep API/stats consistent (avoid changing dims)
      if (any(!keep)) {
        out[, !keep, drop = FALSE] <- 0
      }
    }
  }
  out
}

# Row-wise Khatriâ€“Rao (row-wise Kronecker): for each row i, vec(A[i,] âŠ— B[i,])
.kr_rowwise <- function(A, B) {
  stopifnot(is.matrix(A), is.matrix(B), nrow(A) == nrow(B))
  N <- nrow(A); a <- ncol(A); b <- ncol(B)
  C <- matrix(0, nrow = N, ncol = a * b)
  for (j in seq_len(a)) {
    cols <- ((j - 1L) * b + 1L):(j * b)
    C[, cols] <- B * A[, j]
  }
  C
}

# Uniform open B-spline basis of degree 'degree' with 'df' basis functions
.bspline_basis_uniform <- function(t, df = 8L, degree = 3L, boundary = NULL) {
  stopifnot(df == as.integer(df), degree == as.integer(degree), df >= degree + 1L)
  t <- as.numeric(t)
  if (is.null(boundary)) boundary <- range(t)
  a <- boundary[1]; b <- boundary[2]
  # Open uniform knot vector
  n_basis <- as.integer(df)
  p <- as.integer(degree)
  n_knots <- n_basis + p + 1L
  n_interior <- n_basis - p - 1L
  if (n_interior <= 0L) {
    interior <- numeric(0)
  } else {
    interior <- seq(a, b, length.out = n_interior + 2L)[-c(1L, n_interior + 2L)]
  }
  knots <- c(rep(a, p + 1L), interior, rep(b, p + 1L))
  # Coxâ€“de Boor recursion
  N0 <- function(x, i) as.numeric(knots[i] <= x & x < knots[i + 1L])
  # Handle x==b to be included in last basis
  N0b <- function(x, i) {
    y <- N0(x, i)
    if (i == (length(knots) - 1L)) y[x >= knots[length(knots)]] <- 1
    y
  }
  # Initialize basis for degree 0
  M <- length(t)
  n <- n_basis
  B <- matrix(0, nrow = M, ncol = n)
  for (i in seq_len(n)) {
    B[, i] <- N0b(t, i)
  }
  # Higher degrees
  if (p >= 1L) {
    for (q in 1:p) {
      Bq <- matrix(0, nrow = M, ncol = n)
      for (i in seq_len(n)) {
        denom1 <- knots[i + q] - knots[i]
        denom2 <- knots[i + q + 1L] - knots[i + 1L]
        a1 <- if (denom1 > 0) (t - knots[i]) / denom1 else 0
        a2 <- if (denom2 > 0) (knots[i + q + 1L] - t) / denom2 else 0
        Bq[, i] <- a1 * B[, i] + a2 * if (i + 1L <= n) B[, i + 1L] else 0
      }
      B <- Bq
    }
  }
  B
}

if (!exists(".bspline_basis_open_knots") || !exists("build_symmetric_knots")) {
  .source_repo_file(file.path("models", "ttm", "crossterm_basis.R"))
}
if (!exists("build_t_rbf")) {
  .source_repo_file(file.path("models", "ttm", "rbf_basis.R"))
}

# build_h: tensor basis between B-splines in t and polynomial features in X_prev
build_h <- function(t, X_prev, spec = list(df = 8L, degree = 3L, deg_g = 3L)) {
  t <- as.numeric(t)
  X_prev <- as.matrix(X_prev)
  N <- length(t)
  stopifnot(nrow(X_prev) == N || ncol(X_prev) == 0L)
  df <- if (!is.null(spec$df)) as.integer(spec$df) else 8L
  degree <- if (!is.null(spec$degree)) as.integer(spec$degree) else 3L
  deg_g <- if (!is.null(spec$deg_g)) as.integer(spec$deg_g) else 3L
  # Basis in t: switch on spec$kind; default is B-spline
  kind <- if (!is.null(spec$kind)) as.character(spec$kind) else "bspline"
  if (identical(kind, "rbf") && exists("build_t_rbf")) {
    # RBF integrated basis with smooth tail-linear behavior internally
    Bt <- build_t_rbf(t, df = df, prob_hi = 0.999)
  } else {
    # Support symmetric knots and constant tails if provided in spec
    if (!is.null(spec$knots_t) || !is.null(spec$boundary_t)) {
      interior <- spec$knots_t %||% numeric(0)
      boundary <- spec$boundary_t %||% range(t)
      Bt <- .bspline_basis_open_knots(t, interior = interior, degree = degree, boundary = boundary)
      if (isTRUE(spec$tail_const %||% TRUE)) {
        Tfeat <- tail_plateau_features(t, boundary)
        Bt <- cbind(Bt, Tfeat)
      }
    } else {
      Bt <- .bspline_basis_uniform(t, df = df, degree = degree)
    }
  }
  Gx <- if (ncol(X_prev) > 0L) build_g(X_prev, deg = deg_g) else matrix(1, nrow = N, ncol = 1L)
  .kr_rowwise(Bt, Gx)
}


# ttm_conditional.R
# Conditional sampling via truncated triangular inversion (composite reference)

# Helper: extract model (S) and optional training X
.extract_S_and_X <- function(model) {
  S <- if (is.list(model) && !is.null(model$S)) model$S else model
  X_tr <- NULL
  if (is.list(model) && !is.null(model$X_tr)) X_tr <- as.matrix(model$X_tr)
  if (is.list(S) && !is.null(S$X_tr)) X_tr <- as.matrix(S$X_tr)
  list(S = S, X_tr = X_tr)
}

# Core: invert k-th component for a single sample, given current x_prev (raw scale)
.invert_component_k <- function(S, k, z_target, x_prev_raw, bracket = c(-6, 6)) {
  mu <- S$mu; sigma <- S$sigma
  algo <- if (!is.null(S$algo)) S$algo else "marginal"
  K <- length(mu)
  # Fast path: marginal linear map
  if (identical(algo, "marginal")) {
    # Support both coeffs list (ttm_marginal2) and coeffA/coeffB legacy
    if (!is.null(S$coeffs)) {
      a <- as.numeric(S$coeffs[[k]]["a"])
      b <- as.numeric(S$coeffs[[k]]["b"])
    } else {
      b <- exp(S$coeffA[k]); a <- S$coeffB[k]
    }
    u_std <- (z_target - a) / b
    return(mu[k] + sigma[k] * u_std)
  }

  # Generic 1D monotone inversion via uniroot on standardized u
  f_eval <- function(u_std) {
    # Build a 1xK raw vector consistent with u_std at k and x_prev_raw for <k; others at training mean
    xr <- rep(0, K)
    # previous raw already supplied
    if (k > 1) xr[1:(k - 1)] <- as.numeric(x_prev_raw)
    xr[k] <- mu[k] + sigma[k] * u_std
    if (k < K) {
      # fill remaining with mu (u_std=0)
      xr[(k + 1):K] <- mu[(k + 1):K]
    }
    zk <- as.numeric(predict_ttm(S, matrix(xr, nrow = 1), type = "transform")[1, k])
    zk - z_target
  }
  lo <- bracket[1]; hi <- bracket[2]
  flo <- f_eval(lo); fhi <- f_eval(hi)
  # Expand bracket if no sign change (up to a few times)
  iters <- 0L
  while (flo * fhi > 0 && iters < 4L) {
    lo <- lo - 2; hi <- hi + 2
    flo <- f_eval(lo); fhi <- f_eval(hi)
    iters <- iters + 1L
  }
  # Final safeguard: if still no sign change, pick closer end
  if (flo * fhi > 0) {
    u_star <- if (abs(flo) < abs(fhi)) lo else hi
    return(mu[k] + sigma[k] * u_star)
  }
  rt <- uniroot(function(u) f_eval(u), lower = lo, upper = hi)
  mu[k] + sigma[k] * rt$root
}

#' Sample conditional using composite reference transport
#'
#' @param model TTM model or fit list with $S
#' @param x_cond numeric vector of conditioned values (raw scale)
#' @param idx_cond integer indices of conditioned coordinates (1-based)
#' @param n number of samples to draw
#' @param seed RNG seed for reproducibility
#' @return matrix of size n x |idx_free| in the original model order
sample_conditional_composite <- function(model, x_cond, idx_cond, n, seed = 42L) {
  stopifnot(is.numeric(x_cond), is.numeric(idx_cond), length(x_cond) == length(idx_cond))
  ex <- .extract_S_and_X(model); S <- ex$S; X_tr <- ex$X_tr
  if (!exists("predict_ttm")) {
    src <- file.path("models", "ttm", "ttm_marginal.R"); if (file.exists(src)) source(src)
  }
  if (is.null(S$mu) || is.null(S$sigma)) stop("Model must contain mu/sigma")
  K <- length(S$mu)
  idx_cond <- as.integer(idx_cond)
  if (length(idx_cond) == 0L) stop("idx_cond must be non-empty")
  if (any(idx_cond < 1 | idx_cond > K)) stop("idx_cond out of bounds")
  idx_free <- setdiff(seq_len(K), idx_cond)
  set.seed(as.integer(seed))
  # Choose source X rows
  if (is.null(X_tr)) stop("No training data found in model; attach as model$X_tr before calling")
  X_tr <- as.matrix(X_tr)
  stopifnot(ncol(X_tr) == K)
  sel <- sample.int(nrow(X_tr), size = n, replace = n > nrow(X_tr))
  X0 <- X_tr[sel, , drop = FALSE]
  # Composite reference Z_tilde
  Z_tilde <- predict_ttm(S, X0, type = "transform")
  out <- matrix(NA_real_, nrow = n, ncol = length(idx_free))
  colnames(out) <- paste0("x", idx_free)
  x_fix <- rep(NA_real_, K); x_fix[idx_cond] <- as.numeric(x_cond)
  for (i in seq_len(n)) {
    x_curr <- x_fix
    # Walk through triangular order k=1..K
    for (k in seq_len(K)) {
      if (!is.na(x_curr[k])) {
        # conditioned coordinate, keep fixed
        next
      }
      # build x_prev_raw (for j<k)
      if (k > 1) x_prev_raw <- x_curr[1:(k - 1)] else x_prev_raw <- numeric(0)
      z_target <- Z_tilde[i, k]
      xk_raw <- .invert_component_k(S, k, z_target, x_prev_raw)
      x_curr[k] <- xk_raw
    }
    out[i, ] <- x_curr[idx_free]
  }
  stopifnot(all(is.finite(out)))
  out
}



# ttm_core.R
# Core helpers shared by TTM variants (marginal, separable, cross-term)

.ttm_std_apply <- function(mu, sigma, X) {
  X1 <- sweep(X, 2, mu, "-")
  sweep(X1, 2, sigma, "/")
}

# Minimal infix helper and config reader used across files
if (!exists("%||%")) `%||%` <- function(a, b) if (is.null(a)) b else a
.get_cross_maxit <- function() as.integer(getOption("cross.maxit", 200L))
.get_cross_quad_nodes <- function(fallback = NULL) {
  q <- getOption("cross.quad_nodes", NULL)
  if (!is.null(q)) return(as.integer(q))
  if (!is.null(fallback)) return(as.integer(fallback))
  as.integer(32L)
}
.get_cross_df_t <- function(fallback = NULL) {
  d <- getOption("cross.df_t", NULL)
  if (!is.null(d)) return(as.integer(d))
  if (!is.null(fallback)) return(as.integer(fallback))
  as.integer(8L)
}

.get_cross_lambda_non <- function(Q_used, N, fallback = NULL) {
  ln <- getOption("cross.lambda_non", NULL)
  if (!is.null(ln)) return(as.numeric(ln))
  if (!is.null(fallback) && is.finite(fallback)) return(as.numeric(fallback))
  0.05 * (as.numeric(Q_used) / as.numeric(N))
}

.get_cross_lambda_mon <- function(lam_non, Q_used, N, fallback = NULL) {
  lm <- getOption("cross.lambda_mon", NULL)
  if (!is.null(lm)) return(as.numeric(lm))
  if (!is.null(fallback) && is.finite(fallback)) return(as.numeric(fallback))
  0.5 * as.numeric(lam_non)
}

.use_cross_analytic_grad <- function() {
  isTRUE(getOption("cross.use_analytic_grad", TRUE))
}

.get_cross_workers <- function() {
  w <- getOption("cross.workers", NULL)
  if (!is.null(w)) return(as.integer(max(1L, w)))
  cw <- tryCatch(parallel::detectCores(), error = function(e) 1L)
  as.integer(max(1L, cw - 1L))
}
.get_cross_deg_g <- function() as.integer(getOption("cross.deg_g", 3L))

# Determine algo tag from model object
.ttm_algo_of <- function(model) {
  if (!is.null(model$algo)) return(model$algo)
  cl <- class(model)
  if ("ttm_marginal2" %in% cl || "ttm_marginal" %in% cl) return("marginal")
  if ("ttm_separable" %in% cl) return("separable")
  if ("ttm_cross_term" %in% cl) return("crossterm")
  stop("Unknown TTM model class for core forward computation")
}

# ttm_forward: returns standardized forward map outputs and standardized Jacobian diagonal
# Z âˆˆ R^{NÃ—K}, J âˆˆ R^{NÃ—K} where J is âˆ‚S/âˆ‚x_std (i.e., derivative w.r.t standardized input)
ttm_forward <- function(model, X) {
  stopifnot(is.matrix(X))
  mu <- model$mu; sigma <- model$sigma
  stopifnot(is.numeric(mu), is.numeric(sigma), length(mu) == ncol(X), length(sigma) == ncol(X))
  Xs <- .ttm_std_apply(mu, sigma, X)
  N <- nrow(Xs); K <- ncol(Xs)
  algo <- .ttm_algo_of(model)
  Z <- matrix(0, N, K)
  J <- matrix(0, N, K)
  if (identical(algo, "marginal")) {
    # Support both new (ttm_marginal2) and legacy (ttm_marginal) parameterizations
    if (!is.null(model$coeffs)) {
      for (k in seq_len(K)) {
        a <- as.numeric(model$coeffs[[k]]["a"])
        b <- as.numeric(model$coeffs[[k]]["b"])
        Z[, k] <- a + b * Xs[, k]
        J[, k] <- b
      }
    } else if (!is.null(model$coeffA) && !is.null(model$coeffB)) {
      b <- exp(model$coeffA)
      a <- model$coeffB
      for (k in seq_len(K)) {
        Z[, k] <- a[k] + b[k] * Xs[, k]
        J[, k] <- b[k]
      }
    } else {
      stop("Marginal model missing required coefficients")
    }
  } else if (identical(algo, "separable")) {
    # Build per-k using unified bases if available; fallback to legacy names
    if (!exists("build_g") || !exists("build_f") || !exists("d_build_f")) {
      # try load unified bases (new location)
      pth <- file.path("models", "ttm", "ttm_bases.R"); if (file.exists(pth)) source(pth)
    }
    use_new <- exists("build_g") && exists("build_f") && exists("d_build_f")
    deg <- if (!is.null(model$degree_g)) model$degree_g else 2L
    for (k in seq_len(K)) {
      x_prev <- if (k > 1) Xs[, 1:(k-1), drop = FALSE] else matrix(0, N, 0)
      xk <- Xs[, k]
      if (use_new) {
        if (k > 1) {
          P_non <- build_g(x_prev, deg)
        } else {
          # allow intercept-only for deg==0 to match marginal
          if (!is.null(model$degree_g) && model$degree_g == 0L && !is.null(model$coeffs[[k]]$c_non)) {
            P_non <- matrix(1, nrow = N, ncol = 1L)
          } else {
            P_non <- matrix(0, N, 0)
          }
        }
        P_mon <- build_f(xk)
        B <- d_build_f(xk)
        if (!is.null(model$coeffs[[k]]$c_mon) && length(model$coeffs[[k]]$c_mon) < ncol(P_mon)) {
          m_keep <- length(model$coeffs[[k]]$c_mon)
          P_mon <- P_mon[, seq_len(m_keep), drop = FALSE]
          B <- B[, seq_len(m_keep), drop = FALSE]
        }
      } else {
        if (!exists("basis_g") || !exists("basis_f") || !exists("dbasis_f")) {
          stop("Separable basis functions not found (build_* or basis_* set)")
        }
        P_non <- if (k > 1) basis_g(x_prev, deg) else matrix(0, N, 0)
        P_mon <- basis_f(xk)
        B <- dbasis_f(xk)
      }
      c_non <- model$coeffs[[k]]$c_non
      c_mon <- model$coeffs[[k]]$c_mon
      gk <- if (ncol(P_non) > 0) as.numeric(P_non %*% c_non) else rep(0, N)
      fk <- as.numeric(P_mon %*% c_mon)
      Z[, k] <- gk + fk
      J[, k] <- as.numeric(B %*% c_mon)
    }
  } else if (identical(algo, "crossterm")) {
    # Prefer unified bases build_g/build_h if present in model (new minimal crossterm)
    if (!is.null(model$spec_h) && !is.null(model$gl_nodes) && !is.null(model$gl_weights)) {
      nodes <- model$gl_nodes; weights <- model$gl_weights; Hmax <- if (!is.null(model$Hmax)) model$Hmax else 20
      for (k in seq_len(K)) {
        Xprev <- if (k > 1) Xs[, 1:(k - 1), drop = FALSE] else matrix(0, N, 0)
        xk <- Xs[, k]
        Gx <- if (ncol(Xprev) > 0) build_g(Xprev, deg = model$deg_g) else matrix(1, N, 1L)
        alpha <- model$coeffs[[k]]$alpha
        beta  <- model$coeffs[[k]]$beta
        gk <- if (length(alpha) > 0) as.numeric(Gx %*% alpha) else rep(0, N)
        # Integral via GL on [0, xk]
        I <- rep(0, N)
        for (q in seq_along(nodes)) {
          tq <- xk * nodes[q]
          Hq <- build_h(tq, Xprev, model$spec_h)
          v <- as.numeric(Hq %*% beta)
          v <- pmax(pmin(v, Hmax), -Hmax)
          I <- I + weights[q] * exp(v)
        }
        I <- sign(xk) * abs(xk) * I
        Z[, k] <- gk + I
        # Jacobian diag: exp(h(xk, xprev))
        Hstar <- build_h(xk, Xprev, model$spec_h)
        hstar <- as.numeric(Hstar %*% beta)
        hstar <- pmax(pmin(hstar, Hmax), -Hmax)
        J[, k] <- exp(hstar)
      }
    } else {
      stop("Cross-term model not recognized by core; missing spec_h/gl_nodes")
    }
  } else stop("Unsupported algo")
  list(Z = Z, J = J)
}

# ttm_ld_by_dim: universal per-dim LD aggregator in log-space
ttm_ld_by_dim <- function(model, X) {
  out <- ttm_forward(model, X)
  Z <- out$Z; J <- out$J
  stopifnot(is.matrix(Z), is.matrix(J), all(dim(Z) == dim(J)))
  N <- nrow(Z); K <- ncol(Z)
  C <- -0.5 * log(2 * pi)
  LJ <- log(J) - matrix(log(model$sigma), nrow = N, ncol = K, byrow = TRUE)
  (-0.5) * (Z^2) + C + LJ
}



# ttm_crossterm.R
## Bridge loader for Cross-term TTM implementation (repo-local, no proxies)

if (!exists("%||%")) `%||%` <- function(a, b) if (is.null(a)) b else a

if (!exists("fit_ttm_crossterm")) {
  # Try multiple candidate paths to be robust against varying working dirs
  cand <- character(0)
  # 1) Relative to current working directory
  cand <- c(cand, file.path("models", "ttm", "fit_ttm_crossterm.R"))
  # 2) If this file is sourced by absolute path, use its directory
  this_file <- tryCatch(normalizePath(sys.frames()[[1]]$ofile, winslash = "/", mustWork = FALSE), error = function(e) NA_character_)
  if (is.character(this_file) && nzchar(this_file) && !is.na(this_file)) {
    cand <- c(cand, file.path(dirname(this_file), "fit_ttm_crossterm.R"))
  }
  # 3) If a root_path variable is defined (used elsewhere in repo)
  if (exists("root_path")) {
    cand <- c(cand, file.path(root_path, "models", "ttm", "fit_ttm_crossterm.R"))
  }
  # 4) Walk up a few parents to find repo root containing 00_globals.R
  p <- normalizePath(getwd(), winslash = "/", mustWork = FALSE)
  for (i in 1:8) {
    if (file.exists(file.path(p, "00_globals.R"))) {
      cand <- c(cand, file.path(p, "models", "ttm", "fit_ttm_crossterm.R"))
      break
    }
    np <- dirname(p); if (identical(np, p)) break; p <- np
  }
  # Deduplicate and source first hit
  cand <- unique(cand)
  hit <- NULL
  for (pth in cand) {
    if (file.exists(pth)) { hit <- pth; break }
  }
  if (!is.null(hit)) {
    source(hit)
    try(message(sprintf("[WIRE] sourced=%s", normalizePath(hit, winslash = "/", mustWork = FALSE))), silent = TRUE)
  } else {
    stop("fit_ttm_crossterm not found. Required file missing: models/ttm/fit_ttm_crossterm.R")
  }
}

# Provide a stable evaluator symbol that wraps the generic predict_ttm
if (!exists("predict_ttm_crossterm")) {
  predict_ttm_crossterm <- function(object, newdata, type = c("logdensity_by_dim", "logdensity")) {
    predict_ttm(object, newdata, match.arg(type))
  }
}


# ttm_marginal.R
# Marginal Triangular Transport Map (TTM) â€” Minimal, monotone 1-D basis
# Implements fit_ttm, predict_ttm (with multiple types), and a tiny CLI for train/eval.

# try to source shared standardization util if available
if (!exists("standardize_train_only")) {
  p1 <- file.path("models", "ttm", "utils_standardize.R")
  if (file.exists(p1)) {
    source(p1)
  } else if (exists("root_path")) {
    p2 <- file.path(root_path, "models", "ttm", "utils_standardize.R")
    if (file.exists(p2)) source(p2)
  }
}

.std_stats <- function(X) {
  mu <- colMeans(X)
  sigma <- apply(X, 2, sd) + .Machine$double.eps
  list(mu = mu, sigma = sigma)
}

.standardize <- function(X, mu, sigma) {
  X1 <- sweep(X, 2, mu, "-")
  sweep(X1, 2, sigma, "/")
}

# Simple monotone f_k(x) = a_k + b_k x with b_k > 0
.fit_marginal_per_k <- function(x_std, eps = 1e-6) {
  # Optimize J(b,a) = 0.5 * sum (a + b x)^2 - n log b
  n <- length(x_std)
  xbar <- mean(x_std)
  v <- stats::var(x_std) + 1e-12
  b <- max(eps, 1 / sqrt(v))
  a <- -b * xbar
  c(a = a, b = b)
}

fit_ttm_marginal <- function(data, seed = 42) {
  set.seed(seed)
  S_in <- if (is.list(data) && !is.null(data$X_tr) && !is.null(data$X_te)) data else {
    stopifnot(is.matrix(data))
    if (!exists("split_data")) source("02_split.R")
    list(X_tr = split_data(data, seed)$X_tr, X_te = split_data(data, seed)$X_te)
  }
  X_tr <- S_in$X_tr; X_te <- S_in$X_te
  K <- ncol(X_tr)

  t_tr <- system.time({
    if (exists("standardize_train_only")) {
      st <- standardize_train_only(X_tr)
      mu <- st$mu; sigma <- st$sigma; X_tr_std <- st$Xs
    } else {
      st <- .std_stats(X_tr)
      mu <- st$mu; sigma <- st$sigma
      X_tr_std <- .standardize(X_tr, mu, sigma)
    }
    coeffs <- lapply(seq_len(K), function(k) .fit_marginal_per_k(X_tr_std[, k]))
    S <- list(algo = "marginal", mu = mu, sigma = sigma, coeffs = coeffs, order = seq_len(K))
    class(S) <- "ttm_marginal2"
  })[["elapsed"]]

  t_te <- system.time({ invisible(predict_ttm(S, X_te, type = "logdensity_by_dim")) })[["elapsed"]]

  list(S = S,
       NLL_train = mean(-predict_ttm(S, X_tr, type = "logdensity")),
       NLL_test  = mean(-predict_ttm(S, X_te,  type = "logdensity")),
       time_train = t_tr, time_pred = t_te)
}

# Dispatcher for different TTM algos
fit_ttm <- function(data, algo = c("marginal","separable","crossterm"), seed = 42, ...) {
  algo <- match.arg(algo)
  if (algo == "marginal") return(fit_ttm_marginal(data, seed = seed))
  if (algo == "separable") {
    src <- file.path("models", "ttm", "ttm_separable.R"); if (file.exists(src)) source(src)
    return(fit_ttm_separable(data, seed = seed, ...))
  }
  if (algo == "crossterm") {
    src <- file.path("models", "ttm", "ttm_crossterm.R"); if (file.exists(src)) source(src)
    return(fit_ttm_crossterm(data, seed = seed, ...))
  }
  stop(paste0("algo='", algo, "' not implemented in dispatcher"))
}

# Compatibility wrapper for existing scripts/tests
# Returns the same structure as fit_ttm_marginal (list with $S, times, NLLs)
trainMarginalMap <- function(S, seed = 42) {
  fit_ttm(S, algo = "marginal", seed = seed)
}

predict_ttm <- function(model, X, type = c("transform", "jac_diag", "logdensity_by_dim", "logdensity")) {
  type <- match.arg(type)
  # accept either a fitted list (with $S) or the model itself
  m <- if (is.list(model) && !is.null(model$S)) model$S else model
  if (!is.list(m) || is.null(m$mu) || is.null(m$sigma)) {
    stop("need TTM model or a fit with $S containing mu/sigma")
  }
  stopifnot(is.matrix(X))
  # Delegate to core
  if (!exists("ttm_forward")) {
    src <- if (exists("root_path")) file.path(root_path, "models", "ttm", "ttm_core.R") else file.path("models", "ttm", "ttm_core.R")
    if (file.exists(src)) source(src)
  }
  forward <- ttm_forward(m, X)
  if (type == "transform") return(forward$Z)
  if (type == "jac_diag") return(forward$J)
  LD <- ttm_ld_by_dim(m, X)
  stopifnot(is.matrix(LD), all(dim(LD) == dim(X)), all(is.finite(LD)))
  if (type == "logdensity_by_dim") return(LD)
  LDj <- rowSums(LD)
  stopifnot(all(is.finite(LDj)), max(abs(LDj - rowSums(LD))) <= 1e-10)
  LDj
}

# S3 predict wrapper
predict.ttm_marginal2 <- function(object, newdata, type = c("logdensity_by_dim", "logdensity"), ...) {
  predict_ttm(object, newdata, match.arg(type))
}

# Tiny CLI: Rscript R/ttm_marginal.R mode=train data=path.rds seed=42 out=mod.rds
train_val_test_cli <- function() {
  args <- commandArgs(trailingOnly = TRUE)
  if (length(args) == 0) return(invisible(NULL))
  kv <- strsplit(args, "=", fixed = TRUE)
  amap <- setNames(lapply(kv, function(x) paste(x[-1], collapse = "=")), vapply(kv, `[`, "", 1))
  mode <- amap[["mode"]] %||% "train_eval"
  seed <- as.integer(amap[["seed"]] %||% 42)
  data_path <- amap[["data"]]
  out <- amap[["out"]] %||% "ttm_marginal_model.rds"
  X <- NULL
  if (!is.null(data_path) && file.exists(data_path)) {
    obj <- readRDS(data_path)
    if (is.matrix(obj)) X <- obj else if (is.list(obj) && !is.null(obj$X)) X <- obj$X
  }
  if (is.null(X)) {
    N <- as.integer(amap[["N"]] %||% 400)
    K <- as.integer(amap[["K"]] %||% 3)
    set.seed(seed)
    X <- matrix(rnorm(N * K), ncol = K)
  }
  if (!exists("split_data")) source("02_split.R")
  S <- split_data(X, seed)
  if (mode %in% c("train", "train_eval", "fit")) {
    fit <- fit_ttm(S, seed = seed)
    saveRDS(fit$S, file = out)
    message(sprintf("[train] saved model to %s", out))
    if (mode == "train") return(invisible(TRUE))
  }
  if (mode %in% c("eval", "train_eval")) {
    M <- if (exists("fit") && is.list(fit)) fit$S else readRDS(out)
    nll <- mean(-predict_ttm(M, S$X_te, type = "logdensity"))
    cat(sprintf("NLL (nats): %.6f\n", nll))
  }
  invisible(TRUE)
}

if (sys.nframe() == 0L) {
  # allow running as a script
  `%||%` <- function(a, b) if (is.null(a)) b else a
  train_val_test_cli()
}


# ttm_optimize.R
# Per-k training driver for TTM variants (maps-from-samples, Eq. 38/39)
# Returns parameter list per dimension and meta.

# Robust source helper (works when called from tests/scripts)
if (!exists(".source_repo_file")) {
  .source_repo_file <- function(rel_path) {
    cand <- character(0)
    cand <- c(cand, rel_path)
    this_file <- tryCatch(normalizePath(sys.frames()[[1]]$ofile, winslash = "/", mustWork = FALSE), error = function(e) NA_character_)
    if (is.character(this_file) && nzchar(this_file) && !is.na(this_file)) cand <- c(cand, file.path(dirname(this_file), basename(rel_path)))
    if (exists("root_path")) cand <- c(cand, file.path(root_path, rel_path))
    p <- normalizePath(getwd(), winslash = "/", mustWork = FALSE)
    for (i in 1:8) {
      if (file.exists(file.path(p, "00_globals.R"))) { cand <- c(cand, file.path(p, rel_path)); break }
      np <- dirname(p); if (identical(np, p)) break; p <- np
    }
    cand <- unique(cand)
    for (pth in cand) { if (file.exists(pth)) { source(pth); return(invisible(TRUE)) } }
    invisible(FALSE)
  }
}

.source_repo_file(file.path("models", "ttm", "ttm_bases.R"))
.source_repo_file(file.path("models", "ttm", "ttm_core.R"))

.opt_marginal_k <- function(xk, lambda = 0.0, eps = 1e-6) {
  # Closed-form matching marginal fit implementation (uses sample variance)
  v <- stats::var(xk) + 1e-12
  b <- max(eps, 1 / sqrt(v))
  a <- -b * mean(xk)
  list(a = a, b = b)
}

.opt_separable_k <- function(x_prev, xk, degree_g = 2L, lambda = 0.0, eps = 1e-6) {
  N <- length(xk)
  Phi_non <- if (ncol(x_prev) > 0) build_g(x_prev, deg = degree_g) else if (degree_g == 0L) matrix(1, N, 1L) else matrix(0, N, 0)
  if (degree_g == 0L) {
    v <- stats::var(xk) + 1e-12
    c_mon <- max(eps, 1 / sqrt(v))
    c_non <- matrix(-mean(xk) * c_mon, ncol = 1L)
    return(list(c_non = c_non, c_mon = c_mon))
  }
  Phi_mon <- build_f(xk)
  B <- d_build_f(xk)
  m_non <- ncol(Phi_non); m_mon <- ncol(Phi_mon); N <- nrow(Phi_mon)
  if (m_non > 0) {
    M <- solve(crossprod(Phi_non) + lambda * diag(m_non), t(Phi_non))
    A <- (diag(N) - Phi_non %*% M) %*% Phi_mon
    D <- M %*% Phi_mon
  } else {
    M <- matrix(0, 0, N)
    A <- Phi_mon
    D <- matrix(0, 0, m_mon)
  }
  fn <- function(c) {
    r <- A %*% c
    Bc <- B %*% c
    if (any(Bc <= 0)) return(Inf)
    q <- D %*% c
    0.5 * sum(r^2) - sum(log(Bc)) + 0.5 * lambda * (sum(q^2) + sum(c^2))
  }
  gr <- function(c) {
    r <- A %*% c
    Bc <- B %*% c
    q <- D %*% c
    as.numeric(t(A) %*% r - t(B) %*% (1 / Bc) + lambda * (t(D) %*% q + c))
  }
  c0 <- rep(1, m_mon)
  opt <- optim(c0, fn, gr, method = "L-BFGS-B", lower = rep(eps, m_mon))
  c_mon <- opt$par
  c_non <- if (m_non > 0) -M %*% (Phi_mon %*% c_mon) else numeric(0)
  list(c_non = c_non, c_mon = c_mon)
}

.opt_crossterm_k <- function(x_prev, xk, deg_g = 2L, df_t = 6L, lambda = 1e-3, Q = 16L, Hmax = 20, k = NULL,
                             lam_non = NA_real_, lam_mon = NA_real_, spec_h = NULL, nodes_override = NULL, weights_override = NULL) {
  N <- length(xk)
  # Respect user-specified degree for g (predecessor polys)
  deg_g_used <- as.integer(deg_g)
  Gx <- if (ncol(x_prev) > 0) build_g(x_prev, deg = deg_g_used) else matrix(1, N, 1L)
  m_g <- ncol(Gx)
  p_prev <- ncol(x_prev)
  m_g_alt <- if (p_prev > 0) 1L + p_prev * as.integer(deg_g) else 1L
  # Choose t-spline df via option with N-aware cap; degree remains 3L
  df_opt <- .get_cross_df_t(df_t)
  df_eff <- as.integer(min(df_opt, max(4L, floor(N / 10))))
  df_t_alt <- as.integer(df_t)
  if (is.null(spec_h)) spec_h <- list(df = df_eff, degree = 3L, deg_g = as.integer(deg_g))
  Q_used <- .get_cross_quad_nodes(Q)
  if (is.null(nodes_override) || is.null(weights_override)) {
    gl <- gauss_legendre_nodes(Q_used)
    nodes <- gl$nodes; weights <- gl$weights
  } else {
    nodes <- as.numeric(nodes_override); weights <- as.numeric(weights_override)
  }
  Hstar <- build_h(xk, x_prev, spec_h)
  m_h <- ncol(Hstar)
  integ_calls <- 0L; integ_time <- 0
  # Separate ridge weights for g/h
  ln <- .get_cross_lambda_non(length(nodes), N, fallback = if (is.finite(lambda)) lambda else NA_real_)
  lm <- .get_cross_lambda_mon(ln, length(nodes), N, fallback = if (is.finite(lambda)) lambda else NA_real_)
  # Short scalar search when lambda is NA and no explicit option overrides are set
  opt_ln_over <- getOption("cross.lambda_non", NULL)
  opt_lm_over <- getOption("cross.lambda_mon", NULL)
  if (!is.finite(lambda) && is.null(opt_ln_over) && is.null(opt_lm_over)) {
    lambda0 <- .get_cross_lambda_non(length(nodes), N, fallback = NA_real_)
    cands <- as.numeric(c(0.5 * lambda0, lambda0, 2 * lambda0))
    target <- 0.12
    best <- list(L = lambda0, err = Inf, ln = ln, lm = lm)
    # helper: quick reg_share evaluation with reduced maxit
    quick_share <- function(ln_q, lm_q, maxit_q = 25L) {
      fn_q <- function(theta) {
        a <- if (m_g > 0) theta[seq_len(m_g)] else numeric(0)
        b <- theta[(m_g + 1L):length(theta)]
        g <- if (m_g > 0) as.numeric(Gx %*% a) else rep(0, N)
        I <- rep(0, N)
        for (q in seq_along(nodes)) {
          tq <- xk * nodes[q]
          Hq <- build_h(tq, x_prev, spec_h)
          v <- as.numeric(Hq %*% b)
          v <- pmax(pmin(v, Hmax), -Hmax)
          I <- I + weights[q] * exp(v)
        }
        I <- sign(xk) * abs(xk) * I
        S <- g + I
        h_star <- as.numeric(Hstar %*% b)
        h_star <- pmax(pmin(h_star, Hmax), -Hmax)
        0.5 * sum(S^2) - sum(h_star) + 0.5 * (ln_q * sum(a^2) + lm_q * sum(b^2))
      }
      grad_q <- NULL
      if (.use_cross_analytic_grad()) {
        grad_q <- function(theta) {
          a <- if (m_g > 0) theta[seq_len(m_g)] else numeric(0)
          b <- theta[(m_g + 1L):length(theta)]
          g <- if (m_g > 0) as.numeric(Gx %*% a) else rep(0, N)
          I <- rep(0, N)
          HqL <- vector("list", length(nodes)); EVL <- vector("list", length(nodes))
          for (q in seq_along(nodes)) {
            tq <- xk * nodes[q]
            Hq <- build_h(tq, x_prev, spec_h)
            v <- as.numeric(Hq %*% b)
            v <- pmax(pmin(v, Hmax), -Hmax)
            ev <- exp(v)
            I <- I + weights[q] * ev
            HqL[[q]] <- Hq; EVL[[q]] <- ev
          }
          I <- sign(xk) * abs(xk) * I
          S <- g + I
          ga <- if (m_g > 0) as.numeric(t(Gx) %*% S) + ln_q * a else numeric(0)
          gb <- rep(0, m_h)
          sgnx <- sign(xk) * abs(xk)
          for (q in seq_along(nodes)) {
            wfac <- weights[q] * EVL[[q]] * sgnx * S
            gb <- gb + as.numeric(t(HqL[[q]]) %*% wfac)
          }
          gb <- gb - colSums(Hstar) + lm_q * b
          c(ga, gb)
        }
      }
      theta0 <- c(rep(0, m_g), rep(0, m_h))
      ctrl_q <- list(maxit = as.integer(max(1L, min(.get_cross_maxit(), maxit_q))), trace = 0L)
      opt_q <- if (is.null(grad_q)) optim(theta0, fn_q, method = "L-BFGS-B", control = ctrl_q)
               else optim(theta0, fn_q, gr = grad_q, method = "L-BFGS-B", control = ctrl_q)
      a_hat <- if (m_g > 0) opt_q$par[seq_len(m_g)] else numeric(0)
      b_hat <- opt_q$par[(m_g + 1L):length(opt_q$par)]
      reg <- 0.5 * (ln_q * sum(a_hat^2) + lm_q * sum(b_hat^2))
      if (isTRUE(is.finite(opt_q$value)) && opt_q$value != 0) reg / opt_q$value else NA_real_
    }
    for (L in cands) {
      ln_q <- .get_cross_lambda_non(length(nodes), N, fallback = L)
      lm_q <- .get_cross_lambda_mon(ln_q, length(nodes), N, fallback = L)
      rs <- tryCatch(quick_share(ln_q, lm_q, maxit_q = 25L), error = function(e) NA_real_)
      if (is.finite(rs)) {
        err <- abs(rs - target)
        if (err < best$err) best <- list(L = L, err = err, ln = ln_q, lm = lm_q)
      }
    }
    ln <- best$ln; lm <- best$lm
  }
  fn <- function(theta) {
    a <- if (m_g > 0) theta[seq_len(m_g)] else numeric(0)
    b <- theta[(m_g + 1L):length(theta)]
    g <- if (m_g > 0) as.numeric(Gx %*% a) else rep(0, N)
    t0 <- proc.time()[3]
    I <- rep(0, N)
    for (q in seq_along(nodes)) {
      tq <- xk * nodes[q]
      Hq <- build_h(tq, x_prev, spec_h)
      v <- as.numeric(Hq %*% b)
      v <- pmax(pmin(v, Hmax), -Hmax)
      I <- I + weights[q] * exp(v)
    }
    I <- sign(xk) * abs(xk) * I
    integ_time <<- integ_time + (proc.time()[3] - t0)
    integ_calls <<- integ_calls + 1L
    S <- g + I
    h_star <- as.numeric(Hstar %*% b)
    h_star <- pmax(pmin(h_star, Hmax), -Hmax)
    0.5 * sum(S^2) - sum(h_star) + 0.5 * (ln * sum(a^2) + lm * sum(b^2))
  }
  grad <- NULL
  if (.use_cross_analytic_grad()) {
    grad <- function(theta) {
      a <- if (m_g > 0) theta[seq_len(m_g)] else numeric(0)
      b <- theta[(m_g + 1L):length(theta)]
      g <- if (m_g > 0) as.numeric(Gx %*% a) else rep(0, N)
      I <- rep(0, N)
      HqL <- vector("list", length(nodes)); EVL <- vector("list", length(nodes))
      for (q in seq_along(nodes)) {
        tq <- xk * nodes[q]
      Hq <- build_h(tq, x_prev, spec_h)
        v <- as.numeric(Hq %*% b)
        v <- pmax(pmin(v, Hmax), -Hmax)
        ev <- exp(v)
        I <- I + weights[q] * ev
        HqL[[q]] <- Hq; EVL[[q]] <- ev
      }
      I <- sign(xk) * abs(xk) * I
      S <- g + I
      ga <- if (m_g > 0) as.numeric(t(Gx) %*% S) + ln * a else numeric(0)
      gb <- rep(0, m_h)
      sgnx <- sign(xk) * abs(xk)
      for (q in seq_along(nodes)) {
        wfac <- weights[q] * EVL[[q]] * sgnx * S
        gb <- gb + as.numeric(t(HqL[[q]]) %*% wfac)
      }
      gb <- gb - colSums(Hstar) + lm * b
      gt <- c(ga, gb)
      if (any(!is.finite(gt))) stop(sprintf("[CTM] NaN/Inf gradient at k=%s", if (is.null(k)) "?" else as.character(k)))
      if (isTRUE(getOption("cross.grad_check", FALSE))) {
        idx <- seq_len(min(2L, length(gt)))
        eps <- 1e-6
        ng <- rep(0, length(idx))
        for (j in seq_along(idx)) {
          e <- rep(0, length(gt)); e[idx[j]] <- eps
          ng[j] <- (fn(theta + e) - fn(theta - e)) / (2 * eps)
        }
        re <- max(abs(ng - gt[idx]) / (abs(ng) + 1e-8))
        if (re > 1e-3) stop(sprintf("[CTM] grad-check failed at k=%s relErr=%.3e", if (is.null(k)) "?" else as.character(k), re))
      }
      gt
    }
  }
  theta0 <- c(rep(0, m_g), rep(0, m_h))
  ctrl0 <- tryCatch(control, error = function(e) NULL)
  mx <- tryCatch(ctrl0$maxit, error = function(e) NULL) %||% .get_cross_maxit()
  control <- modifyList(ctrl0 %||% list(), list(maxit = mx))
  control$trace <- 0L
  opt <- if (is.null(grad)) optim(theta0, fn, method = "L-BFGS-B", control = control)
         else optim(theta0, fn, gr = grad, method = "L-BFGS-B", control = control)
  a_hat <- if (m_g > 0) opt$par[seq_len(m_g)] else numeric(0)
  b_hat <- opt$par[(m_g + 1L):length(opt$par)]
  if (isTRUE(getOption("cross.verbose", FALSE)) || interactive()) {
    num <- sum(a_hat^2); den <- num + sum(b_hat^2)
    g_share <- if (den > 0) num / den else NA_real_
    ms <- round(integ_time * 1000)
    reg <- 0.5 * (ln * sum(a_hat^2) + lm * sum(b_hat^2))
    reg_share <- if (opt$value != 0) reg / opt$value else NA_real_
    message(sprintf("[CTM] k=%s end_loss=%.6f conv=%d Q=%d df_t_old=%d df_t_new=%d h_coefs=%d integ_ms=%d calls=%d g_feats_old=%d g_feats_new=%d g_share=%.3f lam_non=%.3g lam_mon=%.3g reg_share=%.3f ||g||2=%.3f ||h||2=%.3f",
                    k %||% "?", opt$value, opt$convergence, length(nodes), df_t_alt, spec_h$df, m_h, ms, integ_calls, m_g_alt, m_g, g_share, ln, lm, reg_share, sum(a_hat^2), sum(b_hat^2)))
  }
  list(alpha = a_hat, beta = b_hat, spec_h = spec_h, nodes = nodes, weights = weights, Hmax = Hmax,
       opt_info = list(convergence = opt$convergence, counts = opt$counts, value = opt$value, maxit = control$maxit))
}

# Main training driver (per-k, pure loop)
train_per_k <- function(algo = c("marginal","separable","crossterm"), X_std, lambda = 0.0, deg = 2L, Q = 16L, seed = 42, Hmax = 20, mu = NULL, sigma = NULL) {
  set.seed(seed)
  algo <- match.arg(algo)
  stopifnot(is.matrix(X_std))
  N <- nrow(X_std); K <- ncol(X_std)
  params <- vector("list", K)
  t_train <- system.time({
    for (k in seq_len(K)) {
      x_prev <- if (k > 1) X_std[, 1:(k - 1), drop = FALSE] else matrix(0, N, 0)
      xk <- X_std[, k]
      if (algo == "marginal") params[[k]] <- .opt_marginal_k(xk, lambda)
      if (algo == "separable") params[[k]] <- .opt_separable_k(x_prev, xk, degree_g = deg, lambda = lambda)
      if (algo == "crossterm") params[[k]] <- .opt_crossterm_k(x_prev, xk, deg_g = deg, df_t = 6L, lambda = lambda, Q = Q, Hmax = Hmax, k = k)
    }
  })[["elapsed"]]
  meta <- list(algo = algo,
               mu = if (is.null(mu)) rep(0, K) else mu,
               sigma = if (is.null(sigma)) rep(1, K) else sigma,
               time_train = t_train,
               lambda = lambda, deg = deg, Q = Q, seed = seed)
  list(params = params, meta = meta)
}


# ttm_separable.R
# Separable triangular map: S_k(x) = g_k(x_<k>) + f_k(x_k)
# Maps-from-samples (Eq. 38/39). Uses unified bases and core predictor.

# Be robust to different working directories in tests/scripts
if (!exists("build_f") || !exists("d_build_f") || !exists("build_g")) {
  if (exists("root_path")) {
    src1 <- file.path(root_path, "models", "ttm", "ttm_bases.R")
  } else {
    src1 <- file.path("models", "ttm", "ttm_bases.R")
  }
  if (file.exists(src1)) source(src1)
}
if (!exists("ttm_forward")) {
  if (exists("root_path")) {
    src2 <- file.path(root_path, "models", "ttm", "ttm_core.R")
  } else {
    src2 <- file.path("models", "ttm", "ttm_core.R")
  }
  if (file.exists(src2)) source(src2)
}

.std_stats <- function(X) {
  mu <- colMeans(X)
  sigma <- apply(X, 2, sd) + .Machine$double.eps
  list(mu = mu, sigma = sigma)
}

.standardize <- function(X, mu, sigma) {
  sweep(sweep(X, 2, mu, "-"), 2, sigma, "/")
}

fit_ttm_separable <- function(data, degree_g = 2L, lambda = 0.0, eps = 1e-6, seed = 42) {
  set.seed(seed)
  S_in <- if (is.list(data) && !is.null(data$X_tr) && !is.null(data$X_te)) data else {
    stopifnot(is.matrix(data))
    if (!exists("split_data")) source("02_split.R")
    list(X_tr = split_data(data, seed)$X_tr, X_te = split_data(data, seed)$X_te)
  }
  X_tr <- S_in$X_tr; X_te <- S_in$X_te
  K <- ncol(X_tr); N <- nrow(X_tr)

  time_train <- system.time({
    st <- .std_stats(X_tr); mu <- st$mu; sigma <- st$sigma
    Xs <- .standardize(X_tr, mu, sigma)
    coeffs <- vector("list", K)
    I_N <- diag(N)
    for (k in seq_len(K)) {
      x_prev <- if (k > 1) Xs[, 1:(k - 1), drop = FALSE] else matrix(0, N, 0)
      xk <- Xs[, k]
      Phi_non <- if (ncol(x_prev) > 0) build_g(x_prev, deg = degree_g) else if (degree_g == 0L) matrix(1, N, 1L) else matrix(0, N, 0)
      # For deg_g==0, use linear-only f and closed-form c for exact marginal equivalence
      if (degree_g == 0L) {
        # Exact marginal equivalence: use same closed-form as marginal per-dim
        v <- stats::var(xk) + 1e-12
        c_mon <- max(eps, 1 / sqrt(v))
        c_non <- matrix(-mean(xk) * c_mon, ncol = 1L)
        coeffs[[k]] <- list(c_non = c_non, c_mon = c_mon)
        next
      }
      Phi_mon <- build_f(xk)
      B <- d_build_f(xk)
      m_non <- ncol(Phi_non); m_mon <- ncol(Phi_mon)
      if (m_non > 0) {
        M <- solve(crossprod(Phi_non) + lambda * diag(m_non), t(Phi_non))
        A <- (I_N - Phi_non %*% M) %*% Phi_mon
        D <- M %*% Phi_mon
      } else {
        M <- matrix(0, 0, N)
        A <- Phi_mon
        D <- matrix(0, 0, m_mon)
      }
      fn <- function(c) {
        r <- A %*% c
        Bc <- B %*% c
        if (any(Bc <= 0)) return(Inf)
        q <- D %*% c
        0.5 * sum(r^2) - sum(log(Bc)) + 0.5 * lambda * (sum(q^2) + sum(c^2))
      }
      gr <- function(c) {
        r <- A %*% c
        Bc <- B %*% c
        q <- D %*% c
        as.numeric(t(A) %*% r - t(B) %*% (1 / Bc) + lambda * (t(D) %*% q + c))
      }
      c0 <- rep(1, m_mon)
      opt <- optim(c0, fn, gr, method = "L-BFGS-B", lower = rep(eps, m_mon))
      c_mon <- opt$par
      c_non <- if (m_non > 0) -M %*% (Phi_mon %*% c_mon) else numeric(0)
      coeffs[[k]] <- list(c_non = c_non, c_mon = c_mon)
    }
    S <- list(algo = "separable", mu = mu, sigma = sigma, coeffs = coeffs, degree_g = degree_g)
    class(S) <- "ttm_separable"
  })[["elapsed"]]

  time_pred <- system.time({ invisible(predict_ttm(S, X_te, type = "logdensity_by_dim")) })[["elapsed"]]

  list(S = S,
       NLL_train = mean(-predict_ttm(S, X_tr, type = "logdensity")),
       NLL_test  = mean(-predict_ttm(S, X_te,  type = "logdensity")),
       time_train = time_train, time_pred = time_pred)
}

# S3 predict wrapper for separable TTM
predict.ttm_separable <- function(object, newdata, type = c("logdensity_by_dim", "logdensity"), ...) {
  predict_ttm(object, newdata, match.arg(type))
}

# Compatibility wrapper for existing scripts/tests
# Mirrors the structure returned by fit_ttm_separable
trainSeparableMap <- function(S, degree_g = 2L, lambda = 0.0, seed = 42, ...) {
  fit_ttm(S, algo = "separable", degree_g = degree_g, lambda = lambda, seed = seed)
}


# utils_standardize.R
# Standardization utilities (train-only)

standardize_train_only <- function(Xtr, X = NULL) {
  stopifnot(is.matrix(Xtr) || is.data.frame(Xtr))
  Xtr <- as.matrix(Xtr)
  mu <- colMeans(Xtr)
  sigma <- apply(Xtr, 2, sd) + .Machine$double.eps
  if (is.null(X)) {
    Xs <- sweep(sweep(Xtr, 2, mu, "-"), 2, sigma, "/")
  } else {
    Xs <- sweep(sweep(as.matrix(X), 2, mu, "-"), 2, sigma, "/")
  }
  list(Xs = Xs, mu = mu, sigma = sigma)
}



