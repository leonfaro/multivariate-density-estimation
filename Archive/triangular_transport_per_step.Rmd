---
title: "Data generation and Map Transport (Block 1)"
author: "Léon Kia Faro"
date: "06_May_2025"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: united
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = FALSE, warning = FALSE, message = FALSE) 

```

# Einleitung

*   R Skript: Block 1 (Daten) & Block 2 (Modell).
*   **Block 1**: Wahres $K$-dim Ziel $\pi(x)$. Wahre Maps $S$ (Ziel $\to$ Ref) & $S^{-1}$ (Ref $\to$ Ziel). Ref $\eta(z)$ ist $N(0,I_K)$.
    *   Dichte: $\pi(x) = \prod_{k=1}^{K} f_{d_k}(x_k | x_{<k-1})$ (Paper, K2, S4).
    *   Map $S$: $X \sim \pi \to Z \sim \eta$. $U_k = F_{d_k}(X_k | X_{<k-1})$, $Z_k = \Phi^{-1}(U_k)$ (Paper, K3, S9).
    *   $F_{d_k}$ wahre bed. CDF. $\Phi$ Std.-Normal-CDF.
    *   Map $S^{-1}$ erzeugt $X \sim \pi$ aus $Z \sim \eta$.
*   **Block 2**: Modelle aus Daten. Ohne Info aus Block 1 (ausser Eval.).
*   Dokument: Block 1 R Code. Überblick Block 2 & Info-Fluss.

# Block 1: Transport-Mapping

Tabelle: Komponenten Block 1. Definieren Datengenerierung and Map Transport.

| Schritt | Abschnitt                     | Ziel                                                                 | Code                       | Notizen                                                                 |
|------|-------------------------------------|----------------------------------------------------------------------|----------------------------|-------------------------------------------------------------------------|
| **1** | Komponenten-Spez.             | Wahre Struktur $K$-dim Ziel $\pi(x)$ durch bed. Komp. | `config`, `lam()`, `alpha()` | `config` = Wahrheit. Wichtig für Daten $\pi(x)$. Block 2: kein Zugriff. |
| **2** | Verteilungshelfer                | PDF, CDF, Quantilfkt. Std.-Vert.                   | `pdf_k()`, `cdf_k()`, `qtf_k()` | R Basis. Wahrheits-abh. mit Param. aus `config`.                       |
| **3** | Zieldichte                      | Exakte Dichte $\pi(x)$ für $x$.                                       | `pi_density()`             | Wahre bed. Spez. Nur Eval./Bench. Nicht für Training Block 2. (Paper) |
| **4** | Referenzverteilung              | Stichproben aus $K$-dim Gauss $N(0, I_K)$.                           | `eta_sample()`             | Zufall $\to$ Stichproben $\pi(x)$.                                         |
| **5** | Vorwärts-Map $S$                   | Daten $X \sim \pi \to Z \sim N(0, I_K)$ mit wahren bed. CDFs.       | `S_map()`                  | $Z=S(X)$. Braucht wahre $F_{d_k}$. (Paper)                                 |
| **6** | Inverse Map $S^{-1}$                | $Z \sim N(0, I_K) \to X \sim \pi$ mit wahren bed. Quantilfkt.       | `S_inv()`                  | $X=S^{-1}(Z)$. Kern Daten $\pi(x)$. Braucht wahre $F^{-1}_{d_k}$. (Paper) |
| **7** | Jakobi + Likelihood             | $\log|\det\nabla S|$ & $\ell(x)$ unter wahrer Map $S$.                | `det_J()`, `loglik()`      | Wichtig für wahre Likelihood. $\ell(x) = \dots$ (Paper)                   |
| **8** | Diagn. Round-Trip             | Prüft $S^{-1}(S(X)) \approx X$ & $S(S^{-1}(U)) \approx U$.               | `round_trip_test()`        | Plausibilität $S, S^{-1}$.                                              |
| **9** | Wahre Log-Dichte (Wrap)        | Für $\log \pi(x)$.                                                     | `log_pi()`                 | Abkürzung `pi_density(..., log=TRUE)`.                                  |
| **10**| Sampling $\pi$ via $S^{-1}$    | i.i.d. $X \sim \pi$ mit wahrer $S^{-1}$.                             | `pi_sample()`              | Hauptweg für Trainings-/Testdaten aus $\pi$.                           |
| **11**| Sampling-Diagnostik              | $X \sim \pi$ mit wahren $\ell(x)$.                                    | `sample_and_loglik()`      | Debugging / Datensätze mit Wahrheits-Likelihood.                      |
| **12**| Bedingtes Sampling              | $\pi(X_{\text{frei}} | X_{\text{fix}})$ durch partielle Inv. $S$.       | `cond_sample()`            | Bedingte Generierung (wahres Modell). (Paper)                         |

```{r start, include=FALSE}
N_demo <- 3 # Demo Stichproben
eps <- 1e-10 # Num. Stabilität

lam_coeff   <- c(1, 0.1) # Koeffs lam(x1)
alpha_coeff <- c(1, 1)   # Koeffs alpha(x2)

lam <- function(x1){ # Fkt Lambda
  base <- lam_coeff[1] + lam_coeff[2] * x1 # Basiswert
  pmin(20, pmax(0.05, base)) # Klemmen
}

alpha <- function(x2){ # Fkt Alpha
  base <- alpha_coeff[1] + alpha_coeff[2] * x2 # Basiswert
  pmax(1e-3, base) # Klemmen
}

config <- list( # Ziel-Vert. Konfig
  list(distr = "norm",  parm = NULL), # X1 ~ N(0,1)
  list(distr = "exp",   parm = function(d) list(rate  = lam(d$X1))), # X2|X1
  list(distr = "gamma", parm = function(d) list(shape = alpha(d$X2), rate  = 1)) # X3|X2
)



# Step 2 distribution helpers
dist_fun <- function(pref, d) get(paste0(pref, d))

get_pars <- function(k, x_prev, cfg){
  ck <- cfg[[k]]
  if (is.null(ck$parm)) return(list())
  names(x_prev) <- paste0("X", seq_along(x_prev))
  ck$parm(as.data.frame(as.list(x_prev)))
}

pdf_k <- function(k, xk, x_prev, cfg, log = FALSE)
  do.call(dist_fun("d", cfg[[k]]$distr),
          c(list(xk), get_pars(k, x_prev, cfg), list(log = log)))

cdf_k <- function(k, xk, x_prev, cfg)
  do.call(dist_fun("p", cfg[[k]]$distr),
          c(list(xk), get_pars(k, x_prev, cfg)))

qtf_k <- function(k, uk, x_prev, cfg)
  do.call(dist_fun("q", cfg[[k]]$distr),
          c(list(uk), get_pars(k, x_prev, cfg)))



# Step 3 target density
pi_density <- function(x, cfg = config, log = FALSE){
  if (is.null(dim(x))) x <- matrix(x, nrow = 1)
  lp <- apply(x, 1, function(row)
    sum(vapply(seq_along(row),
               function(k) pdf_k(k, row[k], row[seq_len(k - 1)], cfg, log = TRUE),
               numeric(1))))
  if (log) lp else exp(lp)
}


# Step 4 reference distribution
eta_sample <- function(N = 1L, K = length(config)){
  Z <- matrix(rnorm(N * K), nrow = N)
  U <- pnorm(Z)
  list(U = U, Z = Z)
}


# Step 5 forward map S
S_map <- function(X, cfg = config, eps = 1e-10){
  if (is.null(dim(X))) X <- matrix(X, nrow = 1)
  clamp <- function(u) pmin(1 - eps, pmax(eps, u))
  U <- t(apply(X, 1, function(row)
    vapply(seq_along(row),
           function(k) clamp(cdf_k(k, row[k], row[seq_len(k - 1)], cfg)),
           numeric(1))))
  Z <- qnorm(U)
  list(U = U, Z = Z)
}


# Step 6 inverse map S^{-1}
S_inv <- function(U, cfg = config){
  if (is.null(dim(U))) U <- matrix(U, nrow = 1)
  Z <- qnorm(U); n <- nrow(U); K <- ncol(U)
  X <- matrix(NA_real_, n, K)
  for (i in seq_len(n)){
    x_prev <- numeric(0)
    for (k in seq_len(K)){
      x_prev[k] <- qtf_k(k, U[i, k], x_prev[seq_len(k - 1)], cfg)
    }
    X[i, ] <- x_prev
  }
  logd <- matrix(NA_real_, n, K)
  for (i in seq_len(n))
    for (k in seq_len(K))
      logd[i, k] <- pdf_k(k, X[i, k], X[i, seq_len(k - 1)], cfg, log = TRUE) -
    dnorm(Z[i, k], log = TRUE)
  list(X = X, Z = Z, logd = logd)
}


# Step 7 Jacobian and likelihood
det_J <- function(logd) rowSums(logd)



loglik <- function(Z, logdet) -0.5 * rowSums(Z ^ 2) -
  (ncol(Z) / 2) * log(2 * pi) + logdet

# Step 8 diagnostic round‑trip
round_trip_test <- function(U, cfg = config, tol = 1e-10){
  inv  <- S_inv(U, cfg)
  fwd  <- S_map(inv$X, cfg)
  maxZ <- max(abs(fwd$Z - inv$Z))
  inv2 <- S_inv(fwd$U, cfg)
  maxX <- max(abs(inv2$X - inv$X))
  c(maxZ = maxZ, maxX = maxX) <= tol
}


# Step 9 true log density
log_pi <- function(x, cfg = config) pi_density(x, cfg, log = TRUE)



# Step 10 sampling from π via S^{-1}
pi_sample <- function(N = 1L, cfg = config){
  ref <- eta_sample(N, length(cfg))
  inv <- S_inv(ref$U, cfg)
  list(X = inv$X, U = ref$U, Z = inv$Z, logd = inv$logd)
}



# Step 11 sampling diagnostics helper (optional)
sample_and_loglik <- function(N = 1L, cfg = config){
  samp <- pi_sample(N, cfg)
  detJ <- det_J(samp$logd)
  ll   <- loglik(samp$Z, detJ)
  cbind(samp$X, loglik = ll)
}

# Step 12 conditional sampling
cond_sample <- function(x_fixed, k, N = 1L, cfg = config){
  K <- length(cfg)
  stopifnot(k >= 1, k < K, length(x_fixed) == k)
  # CDF values for fixed coordinates
  U_fixed <- numeric(k)
  for (j in seq_len(k))
    U_fixed[j] <- cdf_k(j, x_fixed[j], x_fixed[seq_len(j - 1)], cfg)
  Z_fixed <- qnorm(U_fixed)
  X_out <- matrix(NA_real_, N, K)
  for (i in seq_len(N)){
    z <- rnorm(K); z[1:k] <- Z_fixed
    u <- pnorm(z)
    X_out[i, ] <- S_inv(matrix(u, nrow = 1), cfg)$X
  }
  colnames(X_out) <- paste0("X", 1:K)
  X_out
}

```

```{r start2, include=FALSE, echo=FALSE}
set.seed(42)

## Demo-Parameter & Hilfsfunktionen (für alle Schritte)
N_demo <- 3                 # Anzahl Demo-Stichproben
eps     <- 1e-10            # numerische Stabilität

lam_coeff   <- c(1, 0.1)    # λ-Koeffizienten
alpha_coeff <- c(1, 1)      # α-Koeffizienten

lam <- function(x1){        # λ(x1) – geklemmt
  base <- lam_coeff[1] + lam_coeff[2] * x1
  pmin(20, pmax(0.05, base))
}
alpha <- function(x2){      # α(x2) – geklemmt
  base <- alpha_coeff[1] + alpha_coeff[2] * x2
  pmax(1e-3, base)
}


```
# Block 1: Code-Durchlauf

## Schritt 1: Komponenten-Spez.
Section 2.3, Eq. (5), p. 9 — Definition der triangulären Map $S$ mit Komponenten $S_k(x_{1:k})$. 

*   **Prozess**:
    *   `lam(x1)`: Exp Rate $\lambda(x_1)$. Geklemmt.
    *   `alpha(x2)`: Gamma Shape $\alpha(x_2)$. Geklemmt.
    *   `config`: Liste $K$-dim Vert. $D$. $K=3$: $X_1 \sim N(0,1)$, $X_2|X_1 \sim Exp(\lambda(X_1))$, $X_3|X_2 \sim Gamma(\alpha(X_2),1)$.
*   **Out**: `lam`, `alpha`, `config`. Demo: $K$, Vert.-Namen.
*   **Info**: Def. wahre bed. Vert. für $\pi(x)$. (Paper)


```{r step1_chunk}
N_demo <- 3 # Demo Stichproben
eps <- 1e-10 # Num. Stabilität

lam_coeff   <- c(1, 0.1) # Koeffs lam(x1)
alpha_coeff <- c(1, 1)   # Koeffs alpha(x2)

lam <- function(x1){ # Fkt Lambda
  base <- lam_coeff[1] + lam_coeff[2] * x1 # Basiswert
  pmin(20, pmax(0.05, base)) # Klemmen
}

alpha <- function(x2){ # Fkt Alpha
  base <- alpha_coeff[1] + alpha_coeff[2] * x2 # Basiswert
  pmax(1e-3, base) # Klemmen
}

config <- list( # Ziel-Vert. Konfig
  list(distr = "norm",  parm = NULL), # X1 ~ N(0,1)
  list(distr = "exp",   parm = function(d) list(rate  = lam(d$X1))), # X2|X1
  list(distr = "gamma", parm = function(d) list(shape = alpha(d$X2), rate  = 1)) # X3|X2
)

## Demo
cat("K =", length(config),
    "; distributions =", paste(vapply(config, `[[`, "", "distr"),
                               collapse = ", "), "\n")

```

## Schritt 2: Verteilungshelfer

Section 2.2, Eq. (3), p. 6 — Change-of-variables-Formel, die PDF, CDF $\bigl(F_{d_k}\bigr)$ und Quantil­funktionen verbindet. 

* **Inputs**

  * `pref`, `d` → R-Präfix („d/p/q“) & Verteilungsname
  * `k`, `x_prev` → Index der Komponente & vorherige $X_{<k}$-Werte
  * `cfg` → Konfiguration der wahren Bedingungen
  * `xk`, `uk`, `log` → aktueller Wert, Uniform-Wert, Log-Flag

* **Ablauf**

  1. `dist_fun()` baut Funktionsnamen (z. B. `dnorm`) und holt ihn.
  2. `get_pars()` berechnet Parameter für Komponente *k* aus `x_prev`.
  3. `pdf_k()`, `cdf_k()`, `qtf_k()` rufen die passende R-Funktion mit diesen Parametern auf.

* **Outputs**

  * Gewählte R-Verteilungsfunktion
  * Parameter-Liste
  * Gewünschter Wert: PDF, CDF oder Quantil



```{r step2_chunk}
dist_fun <- function(pref, d) get(paste0(pref, d)) # R Vert. Fkt holen

get_pars <- function(k, x_prev, cfg){ # Param. k-te Komp.
  ck <- cfg[[k]] # k-te Konfig
  if (is.null(ck$parm)) return(list()) # Keine Param -> leer
  names(x_prev) <- paste0("X", seq_along(x_prev)) # X-Werte benennen
  ck$parm(as.data.frame(as.list(x_prev))) # Param. Fkt aufrufen
}

pdf_k <- function(k, xk, x_prev, cfg, log = FALSE) # PDF k-te Komp.
  do.call(dist_fun("d", cfg[[k]]$distr), # Dichte Fkt
          c(list(xk), get_pars(k, x_prev, cfg), list(log = log))) # Aufruf

cdf_k <- function(k, xk, x_prev, cfg) # CDF k-te Komp.
  do.call(dist_fun("p", cfg[[k]]$distr), # Vert. Fkt
          c(list(xk), get_pars(k, x_prev, cfg))) # Aufruf

qtf_k <- function(k, uk, x_prev, cfg) # Quantil k-te Komp.
  do.call(dist_fun("q", cfg[[k]]$distr), # Quantil Fkt
          c(list(uk), get_pars(k, x_prev, cfg))) # Aufruf

## Demo
densities <- vapply(seq_along(config), function(k){
  pars <- get_pars(k, numeric(k - 1), config)
  par_str <- if (length(pars))
    paste0(" (", paste(names(pars), round(unlist(pars), 3),
                       sep = "=", collapse = ", "), ")") else ""
  paste0(k, ": ", config[[k]]$distr, par_str)
}, character(1))
print(densities)
```



## Schritt 3: Zieldichte
Section 2.3.2, Eq. (14), p. 12 — Faktorisierung der Zieldichte $π(x)=\prod_{k}π(x_k\mid x_{<k})$.

* **Input**

  * `x` … Beobachtungsmatrix (Zeilen = Samples, Spalten = Dimensionen)
  * `cfg` … Ziel-Konfiguration aus Schritt 1
  * `log` … TRUE ⇒ log-Dichte, FALSE ⇒ Dichte
* **Berechnung**

  * pro Zeile: log PDF jeder Komponente via `pdf_k`
  * Summe über *k* (Komponenten)
  * falls `log = FALSE` ⇒ Exponentiale für echte Dichte
* **Output**

  * Vektor der Länge `nrow(x)` mit π(xᵢ) bzw. log π(xᵢ)



```{r step3_chunk, include=TRUE}
pi_density <- function(x, cfg = config, log = FALSE){        # Zieldichte π(x)
  
  lp <- apply(x, 1, function(row)                            # Zeilenweise
    sum(vapply(seq_along(row),                               # über k
               function(k) pdf_k(k, row[k], row[seq_len(k-1)],
                                 cfg, log = TRUE), numeric(1))))
  if (log) lp else exp(lp)                                   # Log/Dichte
}



## Demo Drei π-Samples + Dichte
x_demo <- pi_sample(N_demo)$X
print(round(cbind(x_demo, dens = pi_density(x_demo)), 5))


```


## Schritt 4: Referenzverteilung
Section 2.3, Figure 5, p. 8 — Standard-Normal-Referenz $η(z)$ und Beziehung $z=S(x)$. 

* **Input (N, K)** – wie viele Stichproben *N* in welcher Dimension *K* erzeugt werden sollen
* **Ziehe Z** – erzeuge ein *N × K*-Gitter Standardnormal-Rauschen Z ∼ 𝒩(0, 1) (unabhängige Spalten)
* **Transformiere zu U** – wende die Normal-CDF Φ elementweise auf Z an ⇒ gleichverteilte Matrix U = Φ(Z) in \[0, 1]
* **Output** – gib `list(U = …, Z = …)` zurück; beide Matrizen tragen exakt dieselbe Zufalls­information in zwei Skalen
* **Zweck** – (U, Z) bilden die Basisverteilung η(z); Z ist der Startpunkt für die inverse Transport-Map S⁻¹, die daraus echte π-Samples X erzeugt

```{r step4_chunk}
eta_sample <- function(N = 1L, K = length(config)){ # Samples Ref.Vert N(0,I)
  Z <- matrix(rnorm(N * K), nrow = N) # N*K N(0,1) -> Matrix Z
  U <- pnorm(Z) # Z -> U (Uniform) via Normal-CDF
  list(U = U, Z = Z) # Liste U, Z
}


## Demo – Referenz-Samples η(z)
demo_ref <- eta_sample(N_demo)
cat("U (uniform):\n"); print(round(demo_ref$U, 4))
cat("Z (normal):\n");  print(round(demo_ref$Z, 4))

```


## Schritt 5 – Vorwärts-Map S
Section 2.3, Eq. (5), p. 9 — Vorwärtsauswertung $S(x)\to z$ über bedingte CDFs.

* **Input:** Matrix `X`, Konfiguration `cfg`, kleines `eps` zum Klemmen
* wandle jede Zeile: `U_k = F_{d_k}(X_k | X_{<k})`, dann `U_k ← clamp(U_k, eps)`
* Probit-Schritt: `Z_k = Φ⁻¹(U_k)` (Normalisierung)
* verarbeite alle *k* nacheinander, nutzt echte bedingte CDFs
* **Output:** Liste `list(U = …, Z = …)` – uniform + normal Darstellung derselben Daten




```{r step5_chunk}
S_map <- function(X, cfg = config, eps = 1e-10){ # Vorwärts-Map S: X -> Z
  if (is.null(dim(X))) X <- matrix(X, nrow = 1) # X zu Matrix
  clamp <- function(u) pmin(1 - eps, pmax(eps, u)) # Hilfsfkt: Klemmen
  U <- t(apply(X, 1, function(row) # Pro Zeile X (transp. U)
    vapply(seq_along(row), # Pro Komp k
           function(k) clamp(cdf_k(k, row[k], row[seq_len(k - 1)], cfg)), # U_k, klemmen
           numeric(1)))) # Output numerisch
  Z <- qnorm(U) # U -> Z via Inv. Normal-CDF
  list(U = U, Z = Z) # Liste U, Z
}


```

## Schritt 6 – Inverse-Map S⁻¹
Section 2.3.1, Eq. (7), p. 9 — Sequenzielle Inversion $S^{-1}(z)$ zur Rücktransformation $z\mapsto x$.

* **Input:** Matrix `U`, Konfiguration `cfg`
* Probit zurück: `Z = Φ⁻¹(U)`
* autoregressiv: `X_k = F_{d_k}^{-1}(U_k | X_{<k})` über *k*
* log-Jacobi-Diag: `logd_{ik} = log f_{d_k}(X_k|…) − log φ(Z_k)`
* **Output:** Liste `list(X = …, Z = …, logd = …)` für spätere Likelihoods



```{r step6_chunk}
S_inv <- function(U, cfg = config){ # Inverse Map S^-1: U -> X
  if (is.null(dim(U))) U <- matrix(U, nrow = 1) # U zu Matrix
  Z <- qnorm(U); n <- nrow(U); K <- ncol(U) # U -> Z; Dims
  X <- matrix(NA_real_, n, K) # X Matrix init
  for (i in seq_len(n)){ # Pro Sample i
    x_prev <- numeric(0) # X_1..X_{k-1} init
    for (k in seq_len(K)){ # Pro Dim k
      x_prev[k] <- qtf_k(k, U[i, k], x_prev[seq_len(k - 1)], cfg) # X_k berechnen
    }
    X[i, ] <- x_prev # Sample X_i speichern
  }
  logd <- matrix(NA_real_, n, K) # logd Matrix init
  for (i in seq_len(n)) # Pro Sample i
    for (k in seq_len(K)) # Pro Dim k
      logd[i, k] <- pdf_k(k, X[i, k], X[i, seq_len(k - 1)], cfg, log = TRUE) - # log f_k
                    dnorm(Z[i, k], log = TRUE) # - log phi(Z_k)
  list(X = X, Z = Z, logd = logd) # Liste X, Z, logd
}



## Demo – Inverse Map S⁻¹ ⇒ X
demo_inv <- S_inv(demo_ref$U)
cat("X = S_inv(U):\n"); print(round(demo_inv$X, 4))


```

## Schritt 7 – Jakobidet & Log-Likelihood
Section 2.3, Eq. (6), p. 9 — Berechnung von $|\det\nabla S|$ als Produkt der Diagonalelemente (Jacobian & Log-Likelihood).

* **Input:** Matrix `logd`, Matrix `Z`
* summiere Spalten → `log|det J|` pro Zeile
* Log-Likelihood: `ℓ(x) = −½‖Z‖² − (K/2) log(2π) + log|det J|`
* getrennt berechenbar dank Dreiecks-Jacobi
* **Output:** Vektoren `logdet`, `loglik`



```{r step7_chunk}
det_J <- function(logd) rowSums(logd) # Log-Det Jakobi

loglik <- function(Z, logdet_val) -0.5 * rowSums(Z ^ 2) - # Log-Likelihood X
  (ncol(Z) / 2) * log(2 * pi) + # Norm. Log-Gauss
  logdet_val # + log|det J|

## Demo – log|det J| & log-Likelihood
logdet     <- det_J(demo_inv$logd)
loglik_vec <- loglik(demo_ref$Z, logdet)
cat("log|detJ|:\n"); print(round(logdet, 4))
cat("loglik:\n");    print(round(loglik_vec, 4))



```

## Schritt 8 – Round-Trip-Diagnose
Section 2.3, Figure 5, p. 8 — Invertierbarkeit von $S$ ermöglicht Round-Trip-Diagnose $x\to z\to x$.

* **Input:** Matrix `U`, Konfiguration `cfg`, Toleranz `tol`
* erzeuge `X = S⁻¹(U)` → zurück  `U_rec, Z_rec = S(X)`
* erneut invertieren: `X_rec = S⁻¹(U_rec)`
* prüfe Max-Abweichungen < `tol` für Z und X
* **Output:** logisches Paar `(maxZ_ok, maxX_ok)` bestätigt Konsistenz



```{r step8_chunk}
round_trip_test <- function(U, cfg = config, tol = 1e-10){ # Round-Trip Test S, S_inv
  inv  <- S_inv(U, cfg) # X_gen = S_inv(U)
  fwd  <- S_map(inv$X, cfg, eps=eps) # (U_rec,Z_rec)=S(X_gen)
  maxZ <- max(abs(fwd$Z - inv$Z)) # Max Diff Z
  inv2 <- S_inv(fwd$U, cfg) # X_rec = S_inv(U_rec)
  maxX <- max(abs(inv2$X - inv$X)) # Max Diff X
  c(maxZ = maxZ, maxX = maxX) <= tol # Diff <= Tol?
}

## Demo – Round-Trip-Test
deltaZ <- max(abs(S_map(demo_inv$X)$Z - demo_ref$Z))
cat("max|ΔZ| =", signif(deltaZ, 3),
    "; round_trip =", round_trip_test(demo_ref$U), "\n")


```

## Schritt 9 – Wahre Log-Dichte (Wrapper)
Section 2.2, Eq. (4), p. 6 — Log-Dichte-Formel $ \log π(x)=\log η(S(x))+ \log|\det\nabla S(x)|$.

* **Input:** Beobachtungen `x`, Konfiguration `cfg`
* Aufruf `pi_density(x, cfg, log = TRUE)`
* vermeidet Code-Duplikation in späteren Analysen
* **Output:** Vektor `log π(x)` für alle Zeilen
* praktisch für Vergleich mit geschätzten Modellen


```{r step9_chunk}
log_pi <- function(x, cfg = config) pi_density(x, cfg, log = TRUE) # Wrapper wahre Log-Dichte



## Demo – Wahre log π(X)
cat("log_pi:\n"); print(round(log_pi(demo_inv$X), 4))

```


## Schritt 10 – π-Sampling via S⁻¹
 Section 2.3, p. 8 — Sampling von $x\sim π$ durch Ziehen $z\sim η$ und Anwenden von $S^{-1}$.
 
* **Input:** Anzahl `N`, Konfiguration `cfg`
* ziehe `(U, Z)` aus Referenz η mit `eta_sample`
* wandle mit `S_inv` → erhält `X`, `logd`
* liefert i.i.d. Trainings-/Test-Paare aus der wahren Dichte
* **Output:** Liste `X, U, Z, logd`



```{r step10_chunk}
pi_sample <- function(N = 1L, cfg = config){ # Samples Ziel pi via S_inv
  ref <- eta_sample(N, length(cfg)) # Samples (U,Z) Ref.Vert
  inv <- S_inv(ref$U, cfg) # U -> X via S_inv
  list(X = inv$X, U = ref$U, Z = inv$Z, logd = inv$logd) # Liste X,U,Z,logd
}

## Demo – Neue π-Stichprobe
samp <- pi_sample(N_demo)
cat("sample X:\n"); print(round(samp$X, 4))

```

## Schritt 11 – Sampling-Diagnose
Section 2.3, Eq. (6), p. 9 — Log-Likelihood $ℓ(x)= -\tfrac12\|S(x)\|^2 + \log|\det\nabla S|$ für Sample-Diagnostik.

* **Input:** Anzahl `N`, Konfiguration `cfg`
* erstelle Samples `X` via `pi_sample`
* berechne `log|det J|` und `ℓ(X)` mit Funktionen aus Schritt 7
* gebe beides neben `X` zurück – schnelle Qualitätskontrolle
* **Output:** Matrix `[X | loglik]` (Spalten K + 1)



```{r step11_chunk}
sample_and_loglik <- function(N = 1L, cfg = config){ # Diagnose: Samples pi + wahre LogL
  samp_res <- pi_sample(N, cfg) # N Samples X (U,Z,logd)
  detJ_val <- det_J(samp_res$logd) # log|det J|
  ll   <- loglik(samp_res$Z, detJ_val) # Log-Likelihoods
  cbind(samp_res$X, loglik = ll) # X + LogL
}

```

## Schritt 12 – Bedingtes Sampling
Section 2.3.2, Figure 6, p. 11 — Bedingtes Sampling durch Manipulation der Zwischenverteilung im inversen Pfad.

* **Input:** feste Koordinaten `x_fixed` (Länge k), Index `k`, Anzahl `N`, `cfg`
* bestimme zugehörige `U_fixed`, `Z_fixed` über CDF/Probit
* ziehe neue Normal-Zufallszahlen für freie Achsen, kombiniere zu `Z_cond`
* transformiere komplett mit `S_inv` → erhält bedingte Samples
* **Output:** Matrix `X_out (N × K)` – Draws aus π(X\_{k+1\:K} | X\_{1\:k}^\*)\`


```{r step12_chunk}
cond_sample <- function(x_fixed, k, N = 1L, cfg = config){ # Bedingtes Sampling
  K_total <- length(cfg) # Gesamt-Dim X
  stopifnot(k >= 1, k < K_total, length(x_fixed) == k) # Input Validierung
  
  U_fixed <- numeric(k) # U_1..U_k init
  for (j in seq_len(k)) { # Pro feste Komp.
    U_fixed[j] <- cdf_k(j, x_fixed[j], x_fixed[seq_len(j - 1)], cfg) # U_j berechnen
  }
  U_fixed_clamped <- pmin(1 - eps, pmax(eps, U_fixed)) # U_fixed klemmen
  Z_fixed <- qnorm(U_fixed_clamped) # Z_j für feste Komp.
  
  X_output <- matrix(NA_real_, N, K_total) # Output Matrix init
  
  for (i in seq_len(N)){ # Pro bed. Sample
    z_sample <- rnorm(K_total) # Frischer Z Vektor N(0,1)
    z_sample[1:k] <- Z_fixed # Feste Z-Werte setzen
    u_sample <- pnorm(z_sample) # Z -> U
    X_output[i, ] <- S_inv(matrix(u_sample, nrow = 1), cfg)$X # S_inv -> X
  }
  colnames(X_output) <- paste0("X", 1:K_total) # Spalten benennen
  X_output # N bed. Samples
}

## Demo – Bedingtes Sampling (gegeben X₁)
cond <- cond_sample(samp$X[1, 1], k = 1, N = N_demo)
cat("conditional sample given X1 =",
    round(samp$X[1, 1], 3), "\n")
print(round(cond, 4))

```

# Notationsübersicht

Tabelle: Symbole Paper vs. R.

| Symbol | Bedeutung (Paper) | R Name |
|---|---|---|
| **S** | Ziel $\to$ Ref Map $S:\mathbb{R}^K\to\mathbb{R}^K$ | `S` / `S_map()` |
| $S_k$ | $k$-te Map Komp. $u_k = S_k(x_k | x_{<k})$ | `S_k()` / `S_list[[k]]` |
| $S^{-1}$ | Inv. Map (Ref $\to$ Ziel) $S^{-1}: \mathbb{R}^K\to\mathbb{R}^K$ | `S_inv()` |
| $S^{-1}_k$ | Seq. Inv. $x_k = S^{-1}_k(u_k | x_{<k})$ | `S_inv_k()` |
| $\nabla S$ | Jakobi-Matrix $S$ | `J_S` |
| $\det\nabla S$ | Det. Jakobi $\nabla S$ | `det_J` |
| $\partial_{x_k}S_k$ | Diag. Jakobi $\frac{\partial u_k}{\partial x_k} = f_{d_k}(x_k|x_{<k})$ | `dSdx_k` / (in `logd`) |
| **$\pi$** | Zielvert. $\mathbb{R}^K$ | `pi_dist` |
| $\pi(x)$ | Ziel-PDF | `pi_density` |
| **$\eta$** | Ref.-Vert. $\mathbb{R}^K$, oft $\mathcal{N}(0,I_K)$ | `eta_dist` |
| $\eta(z)$ | Ref.-PDF | `eta_density` (`dnorm`) |
| **x** | $X \sim \pi$ | `X` (Matrix $N \times K$) |
| **z** | $Z \sim \eta$ | `Z` (Matrix $N \times K$) |
| $K$ | Dim. Ziel-/Ref.-Raum | `K` |
| $N$ | Anz. Samples | `N` |
| $L(x)$ | Likelihood $L(x) = \eta(S(x))\,|\det\nabla S(x)|$ | `likelihood` |
| $\ell(x)$ | Log-Likelihood $\log L(x)$ | `loglik` |
| $\Phi(\cdot)$ | Std.-Normal CDF | `pnorm()` |
| $\varphi(\cdot)$ | Std.-Normal PDF | `dnorm()` |
| $U_k$ | Unif. Var. $U_k = F_{d_k}(X_k|X_{<k})$ | `U` (Matrix $N \times K$) |
| $Z_k = \Phi^{-1}(U_k)$ | Probit Transf. | `Z_probit` / `qnorm(U)` |
| $\text{condSample}(\cdot)$ | Ziehe von $\pi(X_{k^*+1:K}\mid X_{1:k^*}^*)$ | `cond_sample()` |


# Block 2: Modellschätzung

Block 2: Modellschätzung aus Daten. Ohne Zugriff auf Block 1.

| Schritt   | Beschreibung                                          | Code (Konzept)          | Notizen                                                                 |
|--------|-------------------------------------------------------|--------------------------------------|-------------------------------------------------------------------------|
| **17** | Transf. Forest Schätzung                            | `train_TF()` (pro $k$)               | $S_k$ mit TF anpassen $\to \hat{F}_k(x_k | x_{<k})$. Nonparam. bed. CDFs. (Paper) |
| **18** | TF-basierte Map-Konstr.                               | `S_hat_TF()`                         | $\hat{S}$ aus Forests: $\hat{S}_k(x) = \Phi^{-1}(\hat{F}_k(x_k | x_{<k}))$. (Paper) |
| **19** | TF Log-Likelihood Eval.                               | `loglik_TF()`                        | Modell $\ell_{TF}(x)$ mit $\hat{f}_k$ & Jakobi $\hat{S}$. Modellbewertung.    |
| **20** | Empir. Marg. Transf. (Copula Vorv.)                   | `to_uniforms()`                      | $X_j \to U_j = \hat{F}_j(X_j)$. Copula auf unif. Marginalen.                 |
| **21** | Copula Anpassung                                      | `fit_copula()`                       | Multivariate Copula $\hat{C}(u_1, \dots, u_K)$ an Daten. (Paper)             |
| **22** | Copula Dichte Zusammenbau                             | `pi_hat_C()`                         | Volle Dichte $\hat{\pi}_C(x) = \hat{c}(\hat{u}) \prod \hat{f}_k(x_k)$. (Paper) |
| **23** | Normalizing Flow Arch.                                | `flow_model` (Spez.)         | Invert. neur. Netz $f_{\psi}: \mathbb{R}^K \to \mathbb{R}^K$. (Paper)              |
| **24** | Flow Training                                         | `train_NF()`                         | Parameter $\psi$ von $f_{\psi}$ anpassen. (Paper)                          |
| **25** | Flow Dichte Formel                                    | `density_NF()`                       | $\hat{\pi}_{NF}(x) = \eta(f_{\psi}(x)) \cdot |\det (\dots)|$. (Paper)         |
| **26** | KL-Divergenz Eval.                                    | `eval_KL()`                          | $D_{KL}(\pi \| \hat{\pi}_m)$. Braucht Wahrheit Block 1. (Paper)           |


# Informationsfluss Blöcke

Wichtig: Block 2 unabhängig von Block 1 Wahrheit.

| Schritt | Funktion(en)                      | Wahrheits-Info?                                                       | OK Block 1? | Risiko Block 2?      |
|------|----------------------------------|---------------------------------------------------------------------------|-----------------|-------------------------------------------------|
| 1-2  | `config`, `lam()`, `alpha()`     | Def. *wahre* bed. Param. $\lambda(x_1), \alpha(x_2)$, Vert. $d_k$.      | Ja (DGP Def.)   | **Ja**: Def. $\pi$; Block 2 darf nicht kennen. |
| 3    | `pi_density()`                   | Nutzt wahre $f_{d_k}(x_k|x_{<k})$ aus `config`.                           | Ja (Eval.)      | **Ja**: Falls Ziel für Training.  |
| 4    | `eta_sample()`                   | Nur Samples Ref. $\eta = N(0,I_K)$.                                       | Ja (Std.logik)  | Nein.                                           |
| 5    | `S_map()`                        | Nutzt wahre bed. CDFs $F_{d_k}$ aus `config`.                            | Ja (Map Def.)   | **Ja**: Falls Map Modell beeinflusst. |
| 6    | `S_inv()`                        | Nutzt wahre bed. Quantilfkt. $F_{d_k}^{-1}$ aus `config`.                   | Ja (Map Def.)   | **Ja**: Falls Map Modell beeinflusst. |
| 7    | `det_J()`, `loglik()`            | Wahre $f_{d_k}$, $S(x)$.                                                   | Ja (Wahr.Eval.) | **Ja**: Falls wahre Likelihood Training leitet. |
| 8    | `round_trip_test()`              | Kombiniert `S_map()`, `S_inv()`.                                          | Ja (Map QS)     | Nein (Interne Diagnose B1).                     |
| 9    | `log_pi()`                       | Wrapper `pi_density(..., log=TRUE)`.                                      | Ja (Wahr.Eval.) | **Ja**: Wie `pi_density()`.                  |
| 10   | `pi_sample()`                    | `eta_sample()` dann `S_inv()`.                                           | Ja (Daten Gen.) | Nein (Daten ok für B2).                         |
| 11   | `sample_and_loglik()`            | `pi_sample()` mit wahrer `loglik()`.                                       | Ja (Diag. Gen.) | Nein (wie oben, mit wahren Labels).             |
| 12   | `cond_sample()`                  | Wahre $F_{d_k}$, $F_{d_k}^{-1}$ für part. Inv.                             | Ja (Demo Wahr.) | **Ja**: Falls für bed. Modell genutzt.         |

---

