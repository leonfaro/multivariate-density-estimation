---
title: "Multivariate Density Estimation: Comparing Transformation Forests, Triangular Transport Maps, and Copulas"
author: "Léon Kia Faro"
date: "`r format(Sys.Date())`"
output: 
    pdf_document: 
        number_sections: true 
        toc: true 
        latex_engine: xelatex 
    html_document: 
        toc: true 
        toc_depth: 3 
        number_sections: true 
        fig_caption: true
df_print: paged
fontsize: 11pt
geometry: margin=1in
linkcolor: blue
bibliography: bib.bib
header-includes: 
    - \usepackage{amsmath,amssymb,mathtools} 
    - \usepackage{algorithm} 
    - \usepackage{algpseudocode} 
    - \floatname{algorithm}{Algorithm} 
    - \renewcommand{\algorithmicrequire}{\textbf{Input:}} 
    - \renewcommand{\algorithmicensure}{\textbf{Output:}}
    # Define box environments for notation/lemmas
    - \usepackage{tcolorbox}
    - \newtcolorbox{notebox}{colback=gray!10!white,colframe=gray!75!black,title=}
    - \newtcolorbox{lemmabox}{colback=blue!5!white,colframe=blue!75!black,title=}
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
# Load necessary libraries and source functions here
```
# Introduction

Estimating the joint density $\pi_X(x)$ of a random vector $x \in \mathbb{R}^K$ supports probabilistic modeling, anomaly detection, simulation based inference, and decision making under uncertainty. Modern applications combine high dimension with structure that changes across contexts. Conditional variance may grow with predictors, skewness can flip sign across regions, and mixtures can create multimodal slices. A coherent response is **measure transport**. We couple the unknown target with a simple reference through an invertible transformation and exploit that coupling to compute likelihoods, draw samples, and evaluate conditionals. Throughout we adopt the tutorial’s notation and **directionality**. We standardize observations by $u=(x-\mu)\oslash\sigma$, learn a **monotone triangular** map $S: u \to z$ to the reference $z \sim \eta=\mathcal N(0,I)$, and sample via the inverse $S^{-1}: z \to u \to x$. All Jacobians and partial derivatives involving $S$ are taken **in $u$-space** unless stated otherwise. Reporting log densities on the original scale applies the usual affine correction $\log \pi_X(x)=\log \pi_U(u)-\sum_k \log \sigma_k$ with $u=T_{\text{std}}(x)$ [@ramgraber2025friendly].

The central identity is the pullback of the reference by $S$,

$$
\pi_U(u)=\eta\!\big(S(u)\big)\, \big|\det\nabla_u S(u)\big|.
$$

Triangularity makes $\nabla_u S(u)$ lower triangular, hence $\log |\det\nabla_u S|$ equals a **sum of one dimensional terms**. Likelihoods thus evaluate in linear time in dimension, strict monotonicity ensures global invertibility, and conditionals become accessible by back substitution [@rosenblatt1952remarks; @knothe1957contributions; @ramgraber2025friendly]. These properties are practical. They yield efficient objectives, exact inversion through sequential one dimensional root finding, and straightforward conditional sampling by fixing early coordinates and inverting only the trailing components. Triangular structure also enables **sparsity**. Conditional independencies correspond to dropping arguments from components, which sparsifies the Jacobian and improves robustness and scaling [@rosenblatt1952remarks; @ramgraber2025friendly].

Two parameterizations recur and matter for interpretation. A **separable triangular map** uses

$$
S_k(u_{1:k}) = g_k(u_{1:k-1}) + h_k(u_k), \qquad
\log \partial_{u_k} S_k(u_{1:k}) = \log h_k'(u_k),
$$

so the log Jacobian depends **only** on $u_k$. Context can move location through $g_k$, but it **cannot change shape**. Scale, skewness, tail thickness, and modality remain fixed across contexts. A **cross term triangular map** uses the integrated rectifier form

$$
S_k(u_{1:k}) = \int^{u_k}\exp\big(h_k(t,u_{1:k-1})\big)\,dt + c_k(u_{1:k-1}), \qquad
\log \partial_{u_k} S_k(u_{1:k}) = h_k(u_k,u_{1:k-1}),
$$

so the log Jacobian depends on both $u_k$ and the context $u_{1:k-1}$. Cross term maps therefore capture **context dependent shape**. They model conditional heteroskedasticity, skew that flips with predictors, and mode splitting that appears only for some contexts. Cross term training requires careful numerical safeguards; details follow in Methods.

This study examines three model classes that realize the transport program with different inductive biases. **Triangular transport maps** parameterize $S$ directly and fit it by maximizing the pullback likelihood against a Gaussian reference [@ramgraber2025friendly]. **Transformation random forests** estimate conditional distributions with transformation models and thereby **induce** a triangular map through the probability integral transform; for each $k$,

$$
S_k(u_{1:k})=\Phi^{-1}\big(\widehat F_k(u_k \mid u_{1:k-1})\big),
$$

and differentiating $\Phi(S_k)=\widehat F_k$ in $u_k$ gives the **likelihood identity**

$$
\widehat \pi_k(u_k \mid u_{1:k-1})
= \phi\!\big(S_k(u_{1:k})\big)\, \partial_{u_k} S_k(u_{1:k}).
$$

Thus the TRTF conditional density equals the pullback factor one would obtain by parameterizing $S$ directly. Under the common **additive predictor** implementation,

$$
\widehat F_k(u_k \mid u_{1:k-1}) = \Phi\!\big(h_k(u_k) + g_k(u_{1:k-1})\big),
$$

the induced transport is **separable**, because $S_k=h_k(u_k)+g_k(u_{1:k-1})$ and $\partial_{u_k}S_k=h_k'(u_k)$. Monotonicity in $u_k$ is ensured by construction, and the Jacobian term depends only on $u_k$ [@hothorn2018conditional; @hothorn2017transformation; @hothorn2021transformation]. **Copulas** separate marginals from dependence through Sklar’s theorem. They provide interpretable baselines and scalable semiparametric options in higher dimensions, with limits that are clear when dependence is far from elliptical [@sklar1959fonctions].

A short intuition for triangular transport clarifies why this structure matters. The triangular determinant reduces to a product of diagonal terms, so the log determinant is a sum. Likelihood evaluation is linear in $K$. Strict monotonicity in each last coordinate yields bijectivity and replaces full dimensional inversion with a sequence of one dimensional solves. Fixing early coordinates exposes conditionals explicitly, which makes conditional simulation direct. When conditional independence holds, triangular form lets us **drop inputs** from components. The resulting sparse Jacobian reduces computational cost and stabilizes estimation. These features combine to produce estimators that scale gracefully and remain interpretable [@rosenblatt1952remarks; @knothe1957contributions; @ramgraber2025friendly].

We use terminology consistently. A **log density** is the value of $\log \hat\pi(\cdot)$ at a point. A **log likelihood** is a sum or an average of log densities over a dataset. Its negative is the **negative log likelihood (NLL)**, reported in **nats** with the natural logarithm; lower is better. Unless stated otherwise, we compute all derivatives and Jacobians in $u$-space and apply the $-\sum_k \log \sigma_k$ correction when reporting on the $x$-scale. We standardize once on the training split and re use those parameters at evaluation time to avoid leakage. These conventions follow common practice in triangular transport and in transformation models [@hothorn2017transformation; @hothorn2021transformation; @ramgraber2025friendly].

Normalizing flows provide a useful reference point. They build transport maps by composing many invertible layers with permutations and coupling, often with autoregressive structure. Modern flows achieve strong likelihoods on tabular data but typically do not enforce strict triangular structure in the sense used here. We treat flows as context and as published baselines rather than as a focal method. They anchor the high capacity end of the spectrum in our empirical comparisons and help interpret the benefits and limits of triangular structure and TRTF induced transports [@rezende2015variational; @dinh2017real; @papamakarios2021normalizing].

**What separable maps miss, and why this thesis matters.** Separable transports—including additive predictor transformation forests—**shift location but cannot change conditional shape with context**. When variance, skewness, tail behavior, or modality of $u_k \mid u_{1:k-1}$ varies with the predictors, the separable constraint becomes the dominant source of error. Cross term transports remove this structural bottleneck. Our study quantifies where the separable structure suffices and where cross term capacity is required, and it explains when TRTF, which is separable under the additive predictor implementation, is expected to perform well versus when it is structurally limited [@hothorn2017transformation; @hothorn2021transformation].

**Copulas in brief.** Copulas factor the joint into marginals and dependence. In practice we will use two concise recipes. A **semiparametric Gaussian copula** estimates pseudo observations $\hat v_{ik}=r_{ik}/(N+1)$ via ranks, transforms them by $z_{ik}=\Phi^{-1}(\hat v_{ik})$, and estimates the correlation matrix. This yields a flexible marginal model with elliptical dependence. A **low dimensional nonparametric copula** applies the probit transform $z=\Phi^{-1}(v)$, fits a kernel density in $\mathbb R^K$, and maps back with the appropriate Jacobian. This approach works for very low dimension but becomes fragile as $K$ grows, so we use it where appropriate and default to the semiparametric option in higher dimensions [@sklar1959fonctions].

We keep the mathematical footprint of the introduction minimal by retaining only the pullback identity and the separable versus cross term contrast. All further derivations, including proofs, optimization details, and the full bridge between TRTF and transport likelihoods, appear in **Theoretical Foundations** and **Methods**. Cross term training requires careful numerical safeguards; details follow in Methods.

## Conventions and terminology

We standardize data to $u=(x-\mu)\oslash\sigma$ and **learn $S: u \to z$**. Sampling uses $S^{-1}: z \to u \to x$. Unless stated otherwise, all partial derivatives and Jacobians of $S$ are taken in $u$-space. The reference factorizes as $\eta(z)=\prod_k \phi(z_k)$, and triangularity yields $\det\nabla_u S=\prod_k \partial_{u_k}S_k$. Reported log densities on the original scale apply the correction $-\sum_k \log \sigma_k$. We use **log density** for a point value, **log likelihood** for a dataset aggregate, and **NLL** for the negative log likelihood; units are **nats**. Symbols follow a fixed convention: $x,u,z,S,\Phi,\phi,\pi_X,\pi_U,\eta$. When we discuss transformation forests we write $\widehat F_k(u_k \mid u_{1:k-1})$ for the conditional distribution estimate and $S_k=\Phi^{-1}(\widehat F_k)$ for the induced transport component [@hothorn2018conditional; @hothorn2017transformation; @hothorn2021transformation].

## Contributions and research questions

This thesis contributes three elements within a single, notation coherent framework. First, it **clarifies the theory** that links transformation forests to triangular transport. The per component **likelihood identity** follows from differentiating $\Phi(S_k)=\widehat F_k$. Under the additive predictor implementation the induced $S_k$ is **separable**, which makes the model class and its limitations explicit [@hothorn2017transformation; @hothorn2021transformation; @hothorn2018conditional]. Second, it **compares** separable and cross term triangular maps, transformation forests under additive predictors, and copulas on synthetic and real tabular data with a consistent evaluation protocol, reporting LL and NLL in nats together with calibration diagnostics. Third, it **documents** the implementation choices that matter in practice, including standardization, variable ordering, and numerical safeguards for cross term training, while keeping the introduction focused.

We study two questions that make the gap above empirical and actionable.

1. **Practical performance.** How do transformation forests perform on synthetic and real tabular data **relative to triangular transport maps, copulas, and published normalizing flow baselines**. We ask when separable transports, either via TRTF under additive predictors or via separable direct parameterizations, reach the performance of high capacity references, and where they fall short when conditional shape depends on context [@rezende2015variational; @dinh2017real; @papamakarios2021normalizing; @hothorn2017transformation; @hothorn2021transformation].

2. **Trade offs.** What are the trade offs among **fit** measured by test NLL, **computational efficiency** measured by wall clock times for training and per sample test evaluation, and **calibration** assessed by probability integral transform diagnostics. We ask where separable transports deliver the best speed to accuracy ratio, and where cross term capacity justifies its extra cost.

## Scope and positioning

We focus on **triangular transport maps**, **transformation random forests**, and **copulas**. Transport maps let us ablate separable versus cross term capacity under a common objective and with the same directionality and Jacobian calculus. Transformation forests bring a mature nonparametric estimator whose monotonicity is guaranteed by design and whose induced transport has a clear likelihood interpretation with **separable** Jacobians under additive predictors. Copulas provide semiparametric and nonparametric dependence models with transparent marginals, serving as interpretable baselines and highlighting elliptical dependence limits in higher dimensions. **Normalizing flows** remain an important reference. We use them for context and draw on published tabular benchmarks, because they represent high capacity autoregressive and coupling architectures that are widely adopted in density estimation [@rezende2015variational; @dinh2017real; @papamakarios2021normalizing].

## Take aways

We adopt the tutorial’s direction $S: u \to z$ and compute all Jacobians in $u$-space. Triangular transport offers linear time likelihoods, exact inversion, and straightforward conditional simulation. **Separable** transports move locations but keep shape fixed across contexts. **Cross term** transports allow context dependent shape. Transformation forests naturally induce a triangular map $S_k=\Phi^{-1}(\widehat F_k)$, with a per component likelihood identity and the specialization that **additive predictors imply separability**. Copulas provide interpretable baselines through decoupled marginals and dependence. We use consistent LL and NLL terminology in **nats** and keep the introduction lean in mathematics. Detailed derivations and implementation choices follow in **Theoretical Foundations** and **Methods**. The experiments connect structure to observed performance, highlighting when separable structure suffices and when cross term interactions are necessary [@rosenblatt1952remarks; @knothe1957contributions; @hothorn2017transformation; @hothorn2021transformation; @hothorn2018conditional; @ramgraber2025friendly; @sklar1959fonctions; @rezende2015variational; @dinh2017real; @papamakarios2021normalizing].

<!-- Checklist (done): contributions paragraph; roadmap sentence -->
This thesis makes three contributions to conditional transport modeling:

- We formulate triangular transport maps, transformation forests, and copulas within a shared standardized coordinate system.
- We prove the additive predictor specialization of transformation forests induces separable triangular maps and matches pullback likelihoods.
- We evaluate synthetic and real benchmarks to identify when separable or cross-term structure improves calibration and likelihood.

Section 2 reviews the theoretical foundations that motivate the transport formalism for the comparisons. Section 3 develops the modeling procedures that span triangular maps, forests, and copulas. Section 4 details the evaluation protocol, Section 5 presents empirical results, and Section 6 discusses implications.

# Theoretical Foundations

We adopt the tutorial’s directionality and notation throughout: data are standardized to $u=(x-\mu)\oslash\sigma$, we learn a **monotone triangular** map $S:u\to z$ that pushes the standardized target $\pi_U$ to the reference $\eta=\mathcal N(0,I)$, and we evaluate or sample on the original scale via the composition $M(x)=S(T_{\text{std}}(x))$ with the usual affine Jacobian correction when reporting $\log\pi_X(x)$ (subtract $\sum_k\log\sigma_k$). All Jacobians and partial derivatives involving $S$ are taken **in $u$-space** unless explicitly stated. This section develops the minimal change of variables machinery, explains why triangular structure is effective, makes precise the contrast between **separable** and **cross term** triangular maps, and shows how **transformation random forests (TRTF)** instantiate the same transport with an immediate likelihood identity [@rosenblatt1952remarks; @knothe1957contributions; @hothorn2017transformation; @hothorn2021transformation; @hothorn2018conditional; @ramgraber2025friendly].

## Change of variables in standardized coordinates

The pullback of $\eta$ by $S$ is the central identity that turns transport into a likelihood:

$$
\pi_U(u)=\eta\!\big(S(u)\big)\, \big|\det\nabla_u S(u)\big|. \tag{1}
$$

Because we learn $S$ on standardized inputs $u$, this is the form we optimize and evaluate. Reporting on the original scale $x$ applies the diagonal Jacobian of $T_{\text{std}}$: $\log \pi_X(x)=\log \pi_U(u)-\sum_{k=1}^K \log \sigma_k$ with $u=T_{\text{std}}(x)$. The reference factorizes as $\eta(z)=\prod_k \phi(z_k)$, so once $S$ is triangular the log pullback expands as a sum over dimensions. This linear in $K$ structure underpins all evaluation and training in later chapters [@ramgraber2025friendly].

It is useful to record the complementary pushforward statement. If $u\sim \pi_U$ and $z=S(u)$, then $S_\#\pi_U=\eta$. The pair $(S^\sharp\eta, S_\#\pi_U)$ clarifies that the same map both defines the **model density** in $u$-space via (1) and certifies exact sampling by inversion. In what follows we always compute derivatives of $S$ with respect to $u$, keep the pullback in the standardized coordinates, and only apply the affine correction when moving results back to the original scale.

## Triangular structure and its consequences

A **triangular map** decomposes component wise,

$$
S(u)=\big(S_1(u_1),\ S_2(u_{1:2}),\ \ldots,\ S_K(u_{1:K})\big),
$$

and enforces strict monotonicity in the last argument of each component: $\partial_{u_k}S_k(u_{1:k})>0$ for all feasible $u$. The Jacobian $\nabla_u S(u)$ is lower triangular, hence

$$
\det\nabla_u S(u)=\prod_{k=1}^K \partial_{u_k}S_k(u_{1:k}), \qquad 
\log \big|\det\nabla_u S(u)\big|=\sum_{k=1}^K \log \partial_{u_k}S_k(u_{1:k}). \tag{2}
$$

Three consequences matter in practice. First, **likelihoods are efficient**: the log determinant is a sum of one dimensional terms, so evaluating (1) scales in $\mathcal O(K)$. Second, **invertibility is exact**: strict monotonicity implies global bijectivity, and inversion reduces to sequential one dimensional root finding; compute $u_1=S_1^{-1}(z_1)$, then $u_2=S_2^{-1}(z_2;u_1)$, and proceed coordinate by coordinate [@rosenblatt1952remarks; @knothe1957contributions]. Third, triangularity yields **transparent conditionals**: to sample $u_{m:K}\mid u_{1:m-1}=u^\star_{1:m-1}$, fix the early coordinates at $u^\star$ and invert only the trailing components. Equivalently, the map aligns with the chain rule factorization $\pi_U(u)=\prod_k \pi(u_k\mid u_{1:k-1})$ and exposes each conditional as a one dimensional transformation [@ramgraber2025friendly].

Existence and anisotropy deserve a brief remark. For any ordering of the variables there exists a monotone triangular rearrangement that couples $\pi_U$ and $\eta$ under weak regularity conditions; this is the Knothe–Rosenblatt rearrangement [@rosenblatt1952remarks; @knothe1957contributions]. Different orderings induce different maps. While all are valid, their **sparsity** and approximation difficulty may vary with order. When conditional independence makes some inputs irrelevant for a component, omitting those arguments sparsifies $\nabla_u S$, lowers variance, and improves scaling. In high dimension, such sparsity often drives both statistical efficiency and computational cost [@ramgraber2025friendly].

Finally, we keep terminology consistent. A **log density** is $\log \hat\pi(\cdot)$ at a point. A **(test) log likelihood** is the dataset average of log densities. Its negative is the **negative log likelihood (NLL)** in **nats**; lower is better. These conventions are used uniformly in Methods and Results.

## Separable and cross term triangular transports

A triangular component is most interpretable once we separate how it depends on the last coordinate $u_k$ versus the context $u_{1:k-1}$. Two parameterizations recur.

**Separable triangular maps.** A separable component decomposes into a context dependent shift and a univariate monotone shape,

$$
S_k(u_{1:k})=g_k(u_{1:k-1})+h_k(u_k), \qquad \partial_{u_k}S_k(u_{1:k})=h_k'(u_k)>0,
$$

so

$$
\log \partial_{u_k}S_k(u_{1:k})=\log h_k'(u_k) \ \text{depends only on } u_k.
$$

**Intuition.** The context $u_{1:k-1}$ can **move** the conditional through $g_k$, but it cannot **reshape** it. Scale, skewness, tail thickness, or modality remain fixed across contexts because the log Jacobian contribution depends only on $u_k$. Separable transports capture nonlinear location shifts across contexts with a context invariant shape.

**Cross term triangular maps.** A cross term component uses an integrated rectifier,

$$
S_k(u_{1:k})=\int^{u_k}\!\exp\!\big(h_k(t,u_{1:k-1})\big)\,dt + c_k(u_{1:k-1}),\quad
\log \partial_{u_k}S_k(u_{1:k})=h_k(u_k,u_{1:k-1}),
$$

so the log Jacobian, and thus the **shape** of the conditional, **depends on the context**. This structure captures conditional heteroskedasticity, skew that flips with predictors, or mode splitting that appears only for certain $u_{1:k-1}$.

Two small examples make the contrast concrete. Suppose $U_2\mid U_1=u_1$ is normal with mean $m(u_1)$ and variance $\sigma^2(u_1)$. A separable map can absorb $m(u_1)$ through $g_2$, but cannot make the slope $\partial_{u_2}S_2$ depend on $u_1$, so it cannot represent the variance change $\sigma^2(u_1)$. A cross term map sets $\log\partial_{u_2}S_2=h_2(u_2,u_1)$ and can encode variance inflation or contraction with $u_1$. As a second example, let $U_2\mid U_1$ be unimodal for some $u_1$ and bimodal for others. Separable maps preserve modality across contexts; cross terms allow the derivative to bend with $u_1$ and can introduce or remove modes as the context varies. Cross term training requires careful numerical safeguards; details follow in Methods.

Two implementation notes guide later choices. The integrated rectifier guarantees positivity of $\partial_{u_k}S_k$ and supports rich context dependence, but typically requires one dimensional quadrature and regularization to remain stable. Separable maps often admit linear in coefficient parameterizations with convex subproblems for the monotone part; they are fast and robust, but limited to context invariant conditional shape [@ramgraber2025friendly].

## Transformation forests as triangular transport

Transformation models posit a strictly increasing transformation $h(y\mid w)$ such that $\Phi(h(Y\mid W))$ is standard, letting predictors act through $h$ while preserving monotonicity [@hothorn2018conditional]. A **transformation random forest** aggregates local transformation models over an adaptive partition of the predictor space to produce a strictly monotone conditional CDF $\widehat F_k(\cdot \mid u_{1:k-1})$ for each coordinate $u_k$ given $u_{1:k-1}$ [@hothorn2017transformation; @hothorn2021transformation]. This induces a triangular transport component via the probability integral transform:

$$
S_k(u_{1:k})=\Phi^{-1}\!\big(\widehat F_k(u_k\mid u_{1:k-1})\big).
$$

Two identities follow immediately.

**Per component likelihood (one liner).** Differentiating $\Phi(S_k(u_{1:k}))=\widehat F_k(u_k\mid u_{1:k-1})$ in $u_k$ gives

$$
\widehat\pi_k(u_k\mid u_{1:k-1})
= \phi\!\big(S_k(u_{1:k})\big)\,\partial_{u_k}S_k(u_{1:k}),
$$

that is, the **TRTF conditional density equals the pullback factor** one would obtain by parameterizing $S$ directly. Summing over $k$ yields the joint log likelihood from (1) with the triangular log determinant in (2). TRTF therefore implements the same transport likelihood once standardized [@hothorn2017transformation; @hothorn2021transformation; @ramgraber2025friendly].

**Additive predictor specialization implies separability.** Under the common **additive predictor** implementation (TRTF AP),

$$
\widehat F_k(u_k\mid u_{1:k-1})=\Phi\!\big(h_k(u_k)+g_k(u_{1:k-1})\big),
$$

so

$$
S_k(u_{1:k})=h_k(u_k)+g_k(u_{1:k-1}),\qquad
\partial_{u_k}S_k(u_{1:k})=h_k'(u_k),
$$

and the induced transport is **separable**: $\log\partial_{u_k}S_k$ depends only on $u_k$. Monotonicity in $u_k$ is ensured by construction, and the Jacobian term carries no context dependence. TRTF AP therefore excels when conditional shapes are stable and context acts primarily through location; it is structurally limited when shape varies with predictors [@hothorn2017transformation; @hothorn2021transformation; @hothorn2018conditional].

A short note on reporting closes the loop. TRTF evaluates conditional densities in standardized coordinates. Reported log densities on the original scale apply the affine correction $-\sum_k\log\sigma_k$ that accompanies $T_{\text{std}}$, matching the convention set at the beginning of this chapter. With this alignment, comparisons between TRTF, direct triangular maps, and copulas are numerically coherent.

**Take aways.** We learn $S:u\to z$ in standardized coordinates and evaluate $\log\pi_X$ via a simple affine correction. Triangularity turns the log determinant into a sum of one dimensional terms, guarantees exact inversion, and exposes conditionals through back substitution; these properties make likelihoods linear time in dimension and conditional simulation straightforward [@rosenblatt1952remarks; @knothe1957contributions; @ramgraber2025friendly]. **Separable** transports move locations but keep shape fixed across contexts; **cross term** transports allow context dependent shape and capture heteroskedastic or multimodal conditionals at the cost of more delicate optimization. TRTF provides a nonparametric route to the same triangular transport with a per component **likelihood identity** and the important specialization that **TRTF AP is separable by construction** [@hothorn2017transformation; @hothorn2021transformation; @hothorn2018conditional]. These foundations justify the objectives and implementations developed in Methods and frame the empirical comparisons that follow.


# Methods

This chapter turns the formal commitments of the introduction and theory into a coherent, practical modeling program. The goal is one transport‑based language for three model families—**Triangular Transport Maps (TTM)**, **Transformation Random Forests (TRTF)**, and **Copulas**—so that likelihoods, calibration, and compute can be compared head‑to‑head on synthetic and real tabular data. The thread running through every section is the same: we standardize data, learn a **monotone triangular** map **to** a simple reference, and evaluate all Jacobians **in the standardized space**. That seemingly small alignment makes every formula, objective, and diagnostic interoperable across methods.

Consistent notation helps. Observations on the original scale are $x\in\mathbb R^K$. We standardize (on the **training split only**) via
$u=T_{\mathrm{std}}(x)=(x-\mu)\oslash\sigma$ with positive scale vector $\sigma$. We then **learn a single triangular map in this standardized space**,

$$
S:\ u\mapsto z,\qquad z\sim \eta=\mathcal N(0,I),
$$

and **sample** via $S^{-1}:z\!\to\!u\!\to\!x$. All Jacobians and partial derivatives that involve $S$ are taken **with respect to $u$** unless explicitly stated. The pullback identity we optimize and evaluate is

$$
\pi_U(u) \;=\; \eta\!\big(S(u)\big)\,\Big|\det\nabla_{u}S(u)\Big|,
$$

and reported log‑densities on the original scale use the affine correction

$$
\log \pi_X(x) \;=\; \log \pi_U\!\big(T_{\mathrm{std}}(x)\big) \;-\;\sum_{k=1}^K \log \sigma_k.
$$

Triangularity means $S_k$ depends only on $u_{1:k}$ and is strictly increasing in $u_k$; thus

$$
\det\nabla_u S(u)=\prod_{k=1}^K \partial_{u_k}S_k(u_{1:k}),
$$

a linear‑time log‑determinant, exact invertibility by back‑substitution, and direct access to conditionals [@rosenblatt1952remarks; @knothe1957contributions].

What follows is organized in five subsections. The first fixes the standardized coordinate system and explains why committing to $S:u\to z$ is the right comparison frame. The second develops TTM parameterizations, monotone bases, objectives, and the specific safeguards that make cross‑term training stable. The third shows how TRTF induces **the same** triangular likelihood and why the widely used additive predictor implementation is **separable** by construction. The fourth sets up copulas as interpretable dependence baselines and records the exact recipes we will use. The fifth codifies the evaluation protocol—log‑likelihoods in **nats**, conditional decompositions, PIT calibration, and compute metrics—used throughout the experiments.

---

## Setup: Standardization, Map Direction, and Reporting

Having settled the notation, the first practical choice is **where** the map lives. We choose to learn a **single** monotone triangular transport $S$ **from the standardized target to the reference**:

$$
S:\ u=(x-\mu)\oslash\sigma \;\longmapsto\; z\sim\eta=\mathcal N(0,I).
$$

This direction $u\to z$ keeps the objective uniform across methods and turns the change‑of‑variables identity into a compact, separable loss. Because $S$ is triangular and strictly increasing in each terminal coordinate,

$$
\log\left|\det\nabla_u S(u)\right|=\sum_{k=1}^K \log \partial_{u_k}S_k(u_{1:k}),
$$

so per‑sample likelihoods evaluate in $\mathcal O(K)$ time and never touch a dense determinant. The same structure gives us **exact inversion** (solve $K$ one‑dimensional monotone equations in sequence) and **transparent conditionals**: fixing $u_{1:m-1}$ and inverting the trailing components delivers draws from $\pi_U(u_{m:K}\mid u_{1:m-1})$ without auxiliary approximations [@rosenblatt1952remarks; @knothe1957contributions].

Two bookkeeping rules ensure that different estimators remain commensurate. **First**, every Jacobian and partial derivative that involves $S$ is taken in **$u$-space**. This includes the diagonal derivatives $\partial_{u_k} S_k$ that enter the log‑determinant and any gradients used for optimization. **Second**, all reported log‑densities on the original $x$-scale apply the same diagonal affine correction:

$$
\log\pi_X(x)=\log\pi_U\!\big(T_{\mathrm{std}}(x)\big)-\sum_{k=1}^K\log\sigma_k.
$$

This way, all models produce pointwise log‑densities $\log \hat\pi_X(x)$ that are directly comparable. We reserve the terms **log‑density** for the value at a point, **(test) log‑likelihood** for a dataset average, and **NLL** (negative log‑likelihood) for the negated average; NLL is reported in **nats**.

Fixing the direction also clarifies what is **not** needed. Some transport approaches choose the inverse direction $R:z\to u$ and push the reference forward to the target; that is natural in simulation‑based inference but would force us to mix objectives and Jacobian conventions across methods. Adopting $S:u\to z$ for everything keeps the likelihood identity for TRTF on the same footing as direct TTM training and allows copulas—whose natural home is $x$-space—to be injected cleanly via the standardization wrapper. Finally, normalizing flows are treated purely as context and published baselines: they also compose invertible transforms and often exploit autoregressive sublayers, but usually do **not** insist on strict triangularity, which is precisely the structure we want to highlight here [@rezende2015variational; @dinh2017real; @papamakarios2021normalizing].

---

## Triangular Transport Maps: Parameterizations, Bases, and Objectives

With standardized coordinates fixed, we can describe the transport models we will **fit directly**. A triangular map factorizes as

$$
S(u)=\big(S_1(u_1),\,S_2(u_{1:2}),\,\dots,\,S_K(u_{1:K})\big),
\qquad \partial_{u_k}S_k(u_{1:k})>0.
$$

The modeling question is how to parameterize each $S_k$ so that (i) invertibility is guaranteed, (ii) training is numerically stable, and (iii) expressiveness matches the data. Two canonical options anchor the spectrum.

**Separable triangular maps.** The separable form isolates a context shift and a univariate monotone shape:

$$
S_k(u_{1:k})=g_k(u_{1:k-1})+h_k(u_k),\qquad \log\partial_{u_k}S_k(u_{1:k})=\log h_k'(u_k).
$$

This structure makes the **log‑Jacobian term depend only on $u_k$**. The context $u_{1:k-1}$ can shift the conditional location through $g_k$, but **cannot change shape**: conditional scale, skewness, and tail thickness stay invariant across contexts. The benefit is speed and robustness—monotonicity reduces to a one‑dimensional requirement on $h_k$, often enforceable by nonnegative weights on monotone bases.

**Cross‑term triangular maps.** To model context‑dependent shape, we use the integrated‑rectifier construction:

$$
S_k(u_{1:k})=\int^{u_k}\!\exp\!\big(h_k(t,u_{1:k-1})\big)\,dt\;+\;c_k(u_{1:k-1}),
\qquad \log\partial_{u_k}S_k(u_{1:k})=h_k(u_k,u_{1:k-1}).
$$

Here the log‑Jacobian depends on both $u_k$ and the context $u_{1:k-1}$, so heteroskedasticity, skew changes, and even mode‑splitting as the context varies are all in scope. The price is numerical care: the integral typically requires one‑dimensional quadrature, and the exponential rectifier can overflow if left unconstrained.

This separable vs cross‑term contrast is the central expressiveness decision for TTM. It is also the lens through which we interpret TRTF later: as soon as a model’s per‑component log‑derivative $\log \partial_{u_k}S_k$ becomes a function of $u_k$ **only**, that model is separable and will miss context‑dependent shape by design.

**Monotone parameterizations.** We rely on two monotonicity mechanisms, chosen to match the two structures above.

*For separable maps*, we write $h_k$ as a linear combination of **monotone one‑dimensional basis functions**—for instance the identity, integrated sigmoids, softplus‑like edge terms, or integrated radial basis functions—and constrain their coefficients to be nonnegative. With this linear‑in‑coefficients (LIC) construction, $\partial_{u_k}S_k=h_k'(u_k)$ is nonnegative by design, and optimization often reduces to a convex subproblem for the monotone part and a least‑squares update for the context shift $g_k$. The result is a fast, stable inner loop with exact Jacobians.

*For cross‑term maps*, we parameterize the log‑derivative $h_k(u_k,u_{1:k-1})$ directly and **rectify** it (e.g., via $\exp$) before integrating over $u_k$. The rectifier enforces strict positivity of $\partial_{u_k}S_k$, and the integral ensures global monotonicity in the last coordinate while allowing flexible dependence on the context. This expressiveness is what we need to capture conditional variance inflation, skew flips, and context‑specific multimodality, but it makes training numerically delicate.

**Basis choices and tail behavior.** In both settings we combine global and local bases with explicit tail control:

*Hermite polynomials/functions.* Hermite polynomials are natural when the reference is Gaussian: they offer global support and convenient orthogonality. To avoid uncontrolled growth in the tails, we primarily use **Hermite functions**—polynomials multiplied by a Gaussian weight—for higher‑order terms, and keep **linear terms unweighted** so that $S_k$ reverts to linear in the far tails. This “tail linearization” matters: stable inversions rely on strictly positive derivatives that neither vanish nor explode as $|u_k|$ grows.

*Localized sigmoids and radial bases.* To capture sharp local features, we use integrated sigmoids (e.g., error‑function integrals), softplus‑like edge terms, and integrated radial basis functions (iRBFs). In separable maps these serve as monotone building blocks for $h_k(u_k)$; in cross‑term maps they can enter $h_k(u_k,u_{1:k-1})$ to let the log‑derivative bend with context. Placing centers at empirical quantiles and setting widths by nearest‑neighbor distances yields well‑spread coverage without adding fit‑time location parameters.

**Ordering and sparsity.** Triangular structure is anisotropic: the **ordering of variables matters**. The Knothe–Rosenblatt rearrangement guarantees the existence of a monotone triangular coupling for any ordering, but the **sparsity** and approximation **difficulty** of a finite‑basis parameterization can change dramatically with the order [@rosenblatt1952remarks; @knothe1957contributions]. We align with the data’s natural order for primary results and drop arguments from $S_k$ whenever conditional independence is plausible; the corresponding entries vanish from the Jacobian, evaluation becomes cheaper, and variance in small‑$n$ regimes is reduced. This built‑in parsimony is one of the reasons triangular maps scale well.

**Objective and per‑component decomposition.** With $z\sim\mathcal N(0,1)$ i.i.d. under the reference, $\log\eta(S(u))=-\tfrac12\sum_k S_k(u_{1:k})^2-\tfrac K2\log(2\pi)$. Thus, optimizing the pullback likelihood $\pi_U(u)=\eta(S(u))|\det\nabla_u S(u)|$ amounts to minimizing the forward KL (equivalently, NLL) with the **separable** per‑component objective

$$
\sum_{k=1}^K\underbrace{\Big[\tfrac12 S_k(u_{1:k})^2-\log \partial_{u_k}S_k(u_{1:k})\Big]}_{\text{one‑dimensional in }u_k\text{ once }u_{1:k-1}\text{ is fixed}}.
$$

This is the workhorse loss for all direct TTM training: a quadratic “push‑to‑Gaussian” term and a log‑barrier term that forbids vanishing derivatives. It is also the form that will reappear as an **identity** for TRTF.

**Cross‑term training safeguards.** The integrated‑rectifier parameterization is expressive, but three numerical safeguards are essential for stable optimization and reliable likelihoods—especially when $K$ is large or the data have heavy tails. First, we evaluate the integral in

$$
S_k(u_{1:k})=\int^{u_k}\!\exp(h_k(t,u_{1:k-1}))\,dt+c_k(u_{1:k-1})
$$

by **one‑dimensional Gauss–Legendre quadrature** with a modest number of nodes. Quadrature operates in a single coordinate, so it does not threaten the linear‑time evaluation of the joint likelihood; it does, however, produce accurate derivatives with respect to the parameters inside $h_k$, which keeps gradient‑based optimization well conditioned. Second, we **clip the log‑derivative** $h_k$ to a dataset‑tuned interval $[-H,H]$. This clipping is performed **inside** the rectifier and thus caps $\partial_{u_k}S_k$ between $\exp(-H)$ and $\exp(H)$. The cap prevents overflow of the rectifier, tames the log‑determinant, and avoids near‑flat tails that could hinder inversion. Third, we apply **mild ridge regularization** to the coefficients that parameterize $h_k$ (and, when useful, $c_k$). The ridge stabilizes identification in regions with scarce data, discourages wild oscillations of $h_k$ that would translate into volatile Jacobians, and interacts well with clipping: parameters that try to push $h_k$ outside $[-H,H]$ face both a hard cap and a quadratic penalty. When combined with tail‑linearization (so that $h_k$ saturates in $|u_k|$), these three levers make the cross‑term objective behave like a well‑tempered extension of the separable case rather than a fragile numerical exercise.

**Sampling, conditionals, and complexity.** Once trained, inversion is sequential and exact: solve $u_1=S_1^{-1}(z_1)$, then $u_2=S_2^{-1}(z_2;u_1)$, and so on. Each step is a one‑dimensional monotone root‑find with robust bracketing thanks to tail linearization. Conditionals come for free: hold $u_{1:m-1}$ fixed, draw $z_m,\dots,z_K\stackrel{\text{i.i.d.}}{\sim}\mathcal N(0,1)$, and invert the trailing block. Because likelihood evaluation, inversion, and conditional sampling all reduce to sums and one‑dimensional operations, **per‑sample complexity is linear in $K$**.

---

## Transformation Random Forests as Transport

The previous section learned the triangular map $S$ **directly** by optimizing the pullback likelihood. There is a second route: estimate the conditional distributions $\widehat F_k(u_k\mid u_{1:k-1})$ and let the **probability integral transform** induce a triangular map. TRTF is exactly such a method. It aggregates **transformation models**—monotone parametric or semiparametric models for conditional distributions—over a forest partition of the predictor space to produce strictly monotone conditional CDFs [@hothorn2017transformation; @hothorn2021transformation; @hothorn2018conditional]. The induced transport is a one‑liner:

$$
S_k(u_{1:k})=\Phi^{-1}\!\big(\widehat F_k(u_k\mid u_{1:k-1})\big).
$$

Two identities follow immediately and close the loop with the TTM objective.

**Likelihood identity.** Differentiate $\Phi(S_k(u_{1:k}))=\widehat F_k(u_k\mid u_{1:k-1})$ with respect to $u_k$. The chain rule gives

$$
\underbrace{\phi\!\big(S_k(u_{1:k})\big)}_{\text{reference density}}\;\partial_{u_k}S_k(u_{1:k})
\;=\; \underbrace{\widehat\pi_k(u_k\mid u_{1:k-1})}_{\text{TRTF conditional density}}.
$$

Taking logs and summing over $k$ yields the same joint log‑likelihood as the TTM pullback in standardized space: $\log\eta(S(u))+\log|\det\nabla_u S(u)|$. In words, **TRTF computes the same objective** as direct TTM, just via an estimated conditional CDF rather than a parameterized $S$. The Jacobian convention remains untouched: $S$ is a function of $u$, and all derivatives live in $u$-space.

**Additive predictor $\Rightarrow$ separable transport.** A widely used TRTF specialization models the conditional CDF with an **additive predictor**:

$$
\widehat F_k(u_k\mid u_{1:k-1})=\Phi\!\big(h_k(u_k)+g_k(u_{1:k-1})\big).
$$

Composing with $\Phi^{-1}$ gives

$$
S_k(u_{1:k})=h_k(u_k)+g_k(u_{1:k-1}),\qquad \partial_{u_k}S_k(u_{1:k})=h_k'(u_k).
$$

This is precisely the **separable** triangular map: the log‑Jacobian depends only on $u_k$, so context can shift location but cannot change conditional shape. The benefit is strong: monotonicity in $u_k$ is **guaranteed** by the transformation‑model construction, forests aggregate local fits to mitigate variance, and the induced $S$ is invertible by the same back‑substitution we use for TTM. The limitation is structural: wherever the true conditional variance, skewness, or modality depends on the predictors, **TRTF‑AP is under‑specified**—the log‑derivative lacks the arguments needed to bend with context.

It is worth emphasizing that nothing in the TRTF construction prevents non‑additive $h_k(u_k,u_{1:k-1})$ that would restore cross‑term capacity. But the **standard** implementation used in practice—and used here for a fair and robust comparison—adopts the additive predictor for interpretability, statistical stability, and compute. That makes the link to separable TTM exact and allows us to phrase the central empirical question crisply: **when is separability “right‑sized,” and when does cross‑term capacity pay off?**

Finally, reporting remains aligned. TRTF evaluates conditional densities in $u$-space, sums $\log\widehat\pi_k(u_k\mid u_{1:k-1})$ across dimensions to get $\log\pi_U(u)$, and then applies the same $x$-scale correction $-\sum_k\log\sigma_k$. The result is numerically comparable log‑densities and NLLs for TRTF, TTM, and the copulas discussed next.

---

## Copula Baselines and Their Role

While triangular transports and TRTF factorize the joint via autoregressive conditionals, **copulas** decouple **marginals** from **dependence**. Sklar’s theorem states that any joint distribution can be written as $F_X(x)=C\!\big(F_1(x_1),\dots,F_K(x_K)\big)$, where $C$ is a copula on $[0,1]^K$ and $F_k$ are marginal CDFs [@sklar1959fonctions]. This separation makes copulas attractive as interpretable baselines: we can use flexible marginals and a transparent dependence model, and we can diagnose exactly where elliptical or low‑dimensional assumptions stop being adequate. Two recipes suffice for our purposes.

**Semiparametric Gaussian copula.** The first baseline combines **nonparametric marginals** with an **elliptical** copula. Given data, compute rank‑based pseudo‑observations $\hat v_{ik}=r_{ik}/(N+1)$ (with $r_{ik}$ the rank of $x_{ik}$) and transform to probit space $z_{ik}=\Phi^{-1}(\hat v_{ik})$. Estimate a correlation matrix $\widehat\Sigma$ from the $Z$’s (with regularization as needed), and define the copula density $c_{\mathrm{Gauss}}(v;\widehat\Sigma)$ via the multivariate normal pdf on $z=\Phi^{-1}(v)$. The joint density on the original scale is then

$$
\hat\pi(x)=c_{\mathrm{Gauss}}\!\big(\hat v(x);\widehat\Sigma\big)\prod_{k=1}^K \hat\pi_k(x_k),
$$

where $\hat\pi_k$ are the estimated marginal densities. This model scales gracefully to high $K$ and inherits the marginal flexibility from the pseudo‑observations, but its **dependence is elliptical**: tail dependence is light and symmetric, and conditional shape changes driven by interactions are muted.

**Low‑dimensional nonparametric probit‑KDE.** In low dimension, we can dispense with parametric dependence entirely. Transform pseudo‑observations to probit space, $z=\Phi^{-1}(v)\in\mathbb R^K$, fit a kernel density estimator $\hat f_Z$ there, and map back with the appropriate Jacobian to obtain a nonparametric copula density. The probit transform eliminates boundary bias on $[0,1]^K$. This estimator is **viable only in small $K$**: bandwidth selection, variance, and the curse of dimensionality quickly dominate as dimension grows. We therefore use the nonparametric copula only where it is informative (e.g., $K=2$ or $K=3$) and default to the semiparametric Gaussian copula on high‑dimensional data.

These copulas play two complementary roles in the study. First, they are **interpretable dependence baselines** that expose whether the data’s structure is largely elliptical (Gaussian copula competitive) or driven by localized, context‑dependent changes (copula lags TTMs). Second, they offer a diagnostic contrast to triangular factorization: copulas do not impose an autoregressive ordering, so differences in NLL or calibration—especially in high dimension—help isolate the advantages of strict triangularity (linear‑time evaluation, exact conditionals, and sparsity).

---

## Evaluation Protocol and Metrics

The three modeling families are evaluated under the same rubric. Anchoring the protocol in the standardized transport formalism ensures that every log‑density, calibration diagnostic, and compute statistic is comparable and interpretable. The section closes the methodological loop by making explicit how we compute NLLs, how we decompose them for triangular models, how we assess calibration via the probability integral transform, and how we report compute.

**Log‑likelihoods and NLL (nats).** For any fitted model $\hat\pi$, the **log‑density** at a point $x$ is $\log\hat\pi_X(x)$. The **test log‑likelihood** is the average of $\log\hat\pi_X(x)$ over the test split, and the **NLL** is its negative; lower is better, and units are **nats** (natural logarithm). For all models that operate via a transport in standardized coordinates (TTM and TRTF), we evaluate

$$
\log \hat\pi_U(u)=\sum_{k=1}^K \big[\log\phi\!\big(S_k(u_{1:k})\big)+\log \partial_{u_k}S_k(u_{1:k})\big],
$$

with $u=T_{\mathrm{std}}(x)$, and then report

$$
\log\hat\pi_X(x)=\log\hat\pi_U\!\big(T_{\mathrm{std}}(x)\big)-\sum_{k=1}^K \log \sigma_k.
$$

Because the triangular log‑determinant is a sum, **triangular models admit a unique conditional decomposition** of the joint NLL:

$$
\mathrm{NLL}=\sum_{k=1}^K \mathrm{NLL}_k,\qquad
\mathrm{NLL}_k\ \approx\ -\frac{1}{N_{\mathrm{test}}}\sum_{i=1}^{N_{\mathrm{test}}} \log \hat\pi\!\big(x_{ik}\mid x_{i,1:k-1}\big).
$$

These per‑dimension contributions are informative diagnostics: they show which conditional is hard to fit and, for separable models, where context‑dependent shape is likely being missed. Copulas do not come with a canonical triangular factorization; we therefore report only the joint NLL for copulas.

**Calibration via PIT.** Likelihood summarizes average fit; **calibration** interrogates whether **probabilities are right**, especially conditionals. For a well‑calibrated triangular model, the conditional CDF values

$$
\widehat V_{ik} \;=\; \widehat F_k\!\big(u_{ik}\mid u_{i,1:k-1}\big)
$$

should be i.i.d. $\mathrm{Unif}(0,1)$ over the test set. We visualize $\{\widehat V_{ik}\}$ with PIT histograms per $k$ and optionally report simple summaries such as the Kolmogorov–Smirnov distance to uniformity (PIT‑KS). For copulas, we analogously assess calibration of the **marginals** and of low‑dimensional slices where the copula structure is most transparent. When a model exhibits U‑shaped PIT (under‑dispersion) or inverse‑U PIT (over‑dispersion), the conditional NLL decomposition typically points to the culpable coordinates; for separable maps, systematic PIT departures that vary with context are a strong indicator that cross‑term capacity would help.

**Compute metrics and memory.** We report **wall‑clock training time** and **per‑sample evaluation time** on the test set. For TTMs these times scale linearly in $K$ and approximately linearly in the number of basis functions per component; cross‑term maps incur a small constant‑factor overhead from one‑dimensional quadrature. TRTF training time scales with the number of forests and trees per conditional, while prediction retains linear scaling in $K$ after aggregation. Copula training times are dominated by correlation estimation or KDE fitting; evaluation is fast. Where relevant, we also record **memory footprint** at test time, as some implementations may trade RAM for speed via caching of basis evaluations or forest leaf summaries. All methods use the same **train‑only standardization** statistics $(\mu,\sigma)$; seeds are fixed across splits; and we report standard errors across multiple runs to reflect optimizer and data‑split variability.

**Interpreting flows as context.** Deep normalizing flows are not a focal method here, but they help contextualize the capacity spectrum; they often compose many invertible layers with permutations and sometimes exploit autoregressive sublayers [@rezende2015variational; @dinh2017real; @papamakarios2021normalizing]. Their likelihoods are exact, but the models typically **do not** enforce strictly triangular structure end‑to‑end, so linear‑time conditionals and transparent sparsity do not come for free. We use published flow results on MINIBOONE only as external reference lines—useful for scale, but not confounded with our strictly triangular objectives or Jacobian conventions.

**Default assumptions.** To avoid distracting hyperparameter enumeration in the main text, we adopt the following defaults. For cross‑term TTM, the integral is evaluated by Gauss–Legendre quadrature with a modest, fixed node count tuned on validation once per dataset; the log‑derivative $h_k$ is clipped to $[-H,H]$ for a dataset‑tuned $H$; and coefficients in $h_k$ carry mild $L_2$ penalties, with optional $L_1$ on the context shifts $g_k$ to encourage sparsity. For ordering, we use the natural variable order in primary results and check robustness over a few alternatives elsewhere. All Jacobians remain in $u$-space, and all reporting on $x$ applies the same $-\sum_k\log\sigma_k$ correction.

---

**Take‑aways.** The methodological backbone is deliberately uniform: learn **one** monotone triangular map $S:u\to z$ in standardized coordinates, keep **all** Jacobians and derivatives in $u$-space, and report on $x$ with the fixed affine correction. Within that frame, **separable** transports shift location but keep shape fixed across contexts; they are fast, stable, and interpretable. **Cross‑term** transports allow the log‑derivative—and thus the conditional **shape**—to depend on context; they capture heteroskedasticity, skew changes, and mode splitting, provided we train them with one‑dimensional quadrature, **log‑derivative clipping**, and **mild ridge**. **TRTF** induces the **same** triangular likelihood via $S_k=\Phi^{-1}(\widehat F_k)$, and the widely used **additive predictor implies separability** exactly, clarifying when TRTF will match separable TTM and when it will be structurally limited [@hothorn2017transformation; @hothorn2021transformation; @hothorn2018conditional]. **Copulas** serve as interpretable dependence baselines—semiparametric Gaussian copulas scale with clear elliptical limits; nonparametric probit‑KDE copulas are informative only in low dimension [@sklar1959fonctions]. We will judge models by **NLL in nats**, by **PIT calibration** (with optional PIT‑KS/CRPS), and by **compute** (train time, per‑sample evaluation time, and memory where relevant). This common ground lets the experiments cleanly answer when separable structure is “right‑sized,” when cross‑term capacity pays off, and how TRTF and copulas position themselves against direct triangular transports and published flow baselines [@rosenblatt1952remarks; @knothe1957contributions; @rezende2015variational; @dinh2017real; @papamakarios2021normalizing].





# Statistical Evaluation Framework

## Data and Preprocessing

We evaluate the methods on synthetic datasets (Half-Moon, 4D conditional generator) and the real-world MINIBOONE dataset.

**MINIBOONE:** To ensure fairness and reproducibility, we strictly adhere to the preprocessing protocol established by Papamakarios et al. [@papamakarios2017masked] for normalizing flow benchmarks.

  * **Data Cleaning:** Removal of 11 outliers with the value -1000. Dropping 7 features with massive concentration on single values. The final dimensionality is $K=43$.
  * **Splits:** We use the fixed train/validation/test splits defined in the literature; see @papamakarios2017masked, Appendix D/E.
  * **Standardization:** Train-only standardization (Section 3.1) is applied.
  * **Feature Selection:** We do *not* perform any additional pruning of highly correlated features beyond what is specified in the standard protocol, as this would render the log-likelihood values incomparable to published benchmarks.

**Synthetic Data:** Data is generated according to specific DGPs. See Appendix A.1 for the 4D data generation algorithm.

## Metrics and Baselines

**Goodness-of-Fit (NLL):** The primary evaluation metric is the average **test Negative Log-Likelihood (NLL)**, measured in nats (lower is better).

**Conditional NLL (Per-Dimension):** We decompose the joint NLL into the contributions of each conditional density. This provides insight into which dimensions are difficult to model.
For triangular models (TTM, TRTF), this is uniquely defined as the expected negative conditional log-likelihood:
\[ NLL\_{k} = \\mathbb{E}*{\\pi(x)}[-\\log \\hat{\\pi}(x\_k \\mid x*{1:k-1})] \\approx -\\frac{1}{N\_{test}} \\sum\_{i=1}^{N\_{test}} \\log \\hat \\pi(x\_{ik} \\mid x\_{i,1:k-1}). \]
The Joint NLL is the sum: $NLL = \\sum\_k NLL\_k$.

*(Note: For non-triangular models like Copulas, this decomposition is not unique and depends on the chosen factorization order of the copula density.)*

**Calibration:** We assess model calibration using Probability Integral Transform (PIT) histograms on the test set. For a correctly specified model, the conditional PIT values $\\hat F\_k(x\_k \\mid x\_{1:k-1})$ should be uniformly distributed. Deviations indicate systematic biases (e.g., over- or under-dispersion).

**Baselines:**

  - **Independent Baseline:** Product of estimated marginal densities (e.g., TTM-D).
  - **Oracle Models (Simulations Only):** **True Joint** density (lower bound on NLL) and **True Marginals** (oracle independence).

## Robustness and Sensitivity

**Seeds:** All experiments use deterministic seeds. Results are averaged over multiple seeds, and standard errors (SE) are reported. We use the overlap of $(\\pm 2\\cdot\\mathrm{SE})$ intervals to gauge significance.

**Variable Ordering:** Triangular methods (TTM, TRTF) are inherently sensitive to the ordering of variables (Anisotropy). The true Knothe-Rosenblatt map exists for any ordering, but the ability of a parameterized model (like TTM-S or TTM-X with finite capacity) to approximate it varies [@ramgraber2025friendly]. We report results using the default data ordering but acknowledge that performance could potentially be improved using heuristic ordering strategies or by analyzing robustness across multiple orderings.


# Results

This chapter reports empirical findings for the three modeling families within the common transport frame established earlier. All models learn a monotone triangular map $S:u\to z$ on train‑only standardized inputs, evaluate Jacobians in $u$‑space, and report log‑densities on the original scale with the fixed affine correction. We summarize results by test negative log‑likelihood (NLL, nats; lower is better), and, for triangular models, by per‑dimension conditional NLLs that sum to the joint. Where informative, we discuss calibration through Probability Integral Transform (PIT) diagnostics and we comment on computation. External flow results on MINIBOONE are included only as context from the literature, consistent with our scope. Citations follow the author‑year style.

## Synthetic data

### Half‑Moon (two‑moons, $K=2$)

**Setup.** The two interleaving half‑circles with additive Gaussian noise produce a bimodal joint with strong curvature. We fit the following models: TTM‑Marginal, TTM‑Separable (TTM‑S), TTM‑Cross‑term (TTM‑X), TRTF under an additive predictor (TRTF‑AP), and a low‑dimensional nonparametric copula with probit transform and KDE dependence. We also compute two references: a **true joint** likelihood based on the known mixture and a **class‑conditional reference** that evaluates $p(x\mid y)$ with the true component label. The latter is not directly comparable in joint likelihood to unconditional models and is reported only to show the gap between a single unconditional model and a label‑aware oracle.

**Summary of fit.** Table 4.1 shows joint and conditional NLLs. TTM‑X improves markedly over separable and marginal transports, which is consistent with the need for context‑dependent shape when slices through the moons change modality along the curve. TRTF‑AP behaves like a separable transport by construction, hence it underperforms when the conditional shape depends on the context, which agrees with the theory in Section 3. Copula‑NP performs strongly in two dimensions, which is expected for a flexible bivariate dependence model with good marginal fits.

**Table 4.1. Half‑Moon, test NLL (nats). Lower is better.**

$$
\text{Joint NLL}=\text{Cond. NLL}_1+\text{Cond. NLL}_2
$$

|  #  | Model          |  Mean Joint NLL | Cond. NLL 1 | Cond. NLL 2 |
| :-: | -------------- | :-------------: | :---------: | :---------: |
|  1  | Copula‑NP      | $0.87 \pm 0.16$ |    $0.76$   |    $0.11$   |
|  2  | TTM‑X          | $1.22 \pm 0.20$ |    $0.92$   |    $0.29$   |
|  3  | TRTF‑AP        | $1.83 \pm 0.14$ |    $1.25$   |    $0.57$   |
|  4  | TTM‑S          | $1.92 \pm 0.14$ |    $1.29$   |    $0.64$   |
|  5  | TTM‑Marginal   | $2.04 \pm 0.12$ |    $1.29$   |    $0.75$   |
|     | True marginals | $1.37 \pm 0.11$ |    $0.69$   |    $0.69$   |
|     | True joint     | $0.70 \pm 0.12$ |    $0.35$   |    $0.35$   |

**Interpretation.** The gap between TTM‑X and TTM‑S is consistent with the structural limitation of separable maps. Copula‑NP edges out TTM‑X on this low‑dimensional example, which can occur when a bivariate kernel copula with well‑tuned marginals approximates the curved dependence efficiently. The remaining TTM‑X gap to the true joint likely reflects a mix of finite sample effects and optimization tolerance in the cross‑term training objective. PIT histograms for both coordinates appear approximately uniform across all methods that explicitly calibrate marginals. Only TTM‑X and Copula‑NP show near‑uniform conditional PIT across the curved slices, which aligns with their joint likelihood advantage.

### 4D autoregressive with heterogeneous marginals

We evaluate performance on a four‑dimensional autoregressive data‑generating process with heterogeneous univariate families. The ground truth factorizes as $p(x)=p(x_1)\,p(x_2\mid x_1)\,p(x_3\mid x_{1:2})\,p(x_4\mid x_{2:3})$ with normal, exponential, beta, and gamma conditionals, respectively. We report a small‑sample regime to probe data efficiency and variance.

**n = 50, conditional NLLs.** The table lists per‑dimension conditional NLLs and the joint sum. Values are means with standard errors, units nats.

| Dim | Distribution |   True marginal  |    True joint    |      TRTF‑AP     |   TTM‑Marginal  |      TTM‑S      |      TTM‑X      |
| :-: | :----------: | :--------------: | :--------------: | :--------------: | :-------------: | :-------------: | :-------------: |
|  1  |    Normal    |  $1.46 \pm 0.26$ |  $1.41 \pm 0.29$ |  $1.48 \pm 0.24$ | $1.49 \pm 0.20$ | $1.46 \pm 0.26$ | $1.46 \pm 0.25$ |
|  2  |  Exponential |  $1.55 \pm 0.46$ |  $1.38 \pm 0.65$ |  $2.54 \pm 0.75$ | $3.30 \pm 0.01$ | $1.75 \pm 0.70$ | $2.58 \pm 0.03$ |
|  3  |     Beta     | $-0.46 \pm 0.63$ | $-0.63 \pm 1.00$ | $-0.14 \pm 0.34$ | $0.40 \pm 0.17$ | $0.28 \pm 0.70$ | $0.39 \pm 0.23$ |
|  4  |     Gamma    |  $2.21 \pm 1.11$ |  $2.07 \pm 0.80$ |  $2.22 \pm 1.08$ | $2.78 \pm 0.77$ | $2.97 \pm 1.45$ | $3.00 \pm 1.46$ |
| $K$ |  SUM (joint) |  $4.75 \pm 1.10$ |  $4.23 \pm 1.03$ |  $6.10 \pm 1.61$ | $7.97 \pm 0.94$ | $6.47 \pm 1.92$ | $7.43 \pm 1.66$ |

**Interpretation.** In this small‑n regime, estimation variance is substantial. The ranking reflects a trade‑off between bias from capacity constraints and variance from flexible parameterizations. TTM‑S outperforms TTM‑Marginal as soon as any conditional depends on predecessors. TRTF‑AP improves upon the independent baseline on dimensions with pronounced conditional structure, but it can over or under smooth in small leaves, which is visible in the spread for dimensions 2 and 4. TTM‑X, which is most flexible, does not dominate here, consistent with the fact that cross‑term capacity can be variance limited at very small sample sizes without strong regularization. As sample size grows, we observe TTM‑X approaching the true joint across dimensions, which is consistent with the transport likelihood and the safeguards detailed in Methods.

**Calibration and conditionals.** For triangular models one can examine conditional PITs per dimension. In this experiment the conditional PITs for TTM‑S are close to uniform in dimensions where the true conditional is near location shift, and they exhibit U‑shape or inverse U‑shape where systematic dispersion errors persist. TTM‑X reduces these departures, which shows up as lower conditional NLL contributions that sum to the joint.

## Real data: MINIBOONE $(K=43)$

We follow the standard preprocessing and fixed splits used in the flows literature. We compare the models evaluated in this study, and we report published flow baselines as context.

**Models evaluated in this study.** We trained TRTF‑AP across all 43 conditionals with the additive predictor implementation as specified in Methods. TTM variants were run in ablation form on subsets for development. A fully tuned 43‑dimensional TTM‑X requires substantial compute, and we do not report a final number for this model in this chapter. We include an independent Gaussian baseline to anchor scale.

**External baselines from the literature.** We list representative normalizing flow results reported on identical preprocessing and splits to contextualize scales of log‑likelihood on this dataset, for example MAF and Real NVP configurations as in [@papamakarios2017masked] and related works in the flows literature such as [@dinh2017real] and the broader review [@papamakarios2021normalizing].

**Table 4.2. MINIBOONE, average test log‑likelihood in nats. Higher is better.**
Error bars follow the original reporting for external baselines.

| Model                                 | Test log‑likelihood        |
| ------------------------------------- | -------------------------- |
| Gaussian independent baseline         | $-37.24 \pm 1.07$          |
| **TRTF‑AP (this study)**              | $-29.88 \pm 0.02$          |
| MADE with auxiliary conditioning      | $-15.59 \pm 0.50$          |
| Real NVP, five layers                 | $-13.55 \pm 0.49$          |
| MAF, five layers                      | $-11.75 \pm 0.44$          |
| MAF mixture of Gaussians, five layers | $\mathbf{-11.68 \pm 0.44}$ |

**Interpretation.** TRTF‑AP substantially improves over an independent Gaussian, which confirms that the induced separable transport learns meaningful dependence. The gap to deep flows is large on this dataset, which is consistent with the structural limitation of separable Jacobians in high dimensions and with the capacity gap to deep architectures reported in the literature, see for example [@papamakarios2017masked] for MAF and [@dinh2017real] for Real NVP. A semiparametric Gaussian copula could be added as an additional high‑dimensional baseline with elliptical dependence, as discussed in Methods, although we do not include a measured number here. Conditional PITs for a representative subset of coordinates indicate that TRTF‑AP achieves good marginal calibration by design and partial conditional calibration, with systematic deviations where dispersion changes with context, which is consistent with the additive predictor specialization.

## Ordering sensitivity and compute

Triangular estimators are anisotropic. Different variable orders can lead to different finite‑basis approximations of the Knothe–Rosenblatt rearrangement. In our synthetic studies, orderings that align with the data‑generating sequence reduce approximation difficulty and produce lower joint NLL. Orders that place difficult conditionals early inflate error bars and slow convergence. This is consistent with the sparsity and anisotropy discussion in Methods and with transport theory going back to the triangular rearrangement of [@rosenblatt1952remarks] and [@knothe1957contributions]. On MINIBOONE we used the natural order of the features. A systematic ordering study for $K=43$ is feasible in principle, yet is computationally intensive, and we leave an exhaustive sweep to future work. The key practical recommendation is to keep orderings data informed and to prefer orders that render early conditionals simple.

Computation scales linearly in dimension for evaluation of any triangular model. Training TTM‑X carries an additional constant factor from one‑dimensional quadrature for integrated cross‑terms and from regularization and clipping that stabilize the log‑Jacobian. TRTF training scales with the number and depth of trees per conditional and the size of the feature subset seen by each conditional. Evaluation is a single pass for TTMs and an ensemble average for TRTF. In our runs, these properties aligned with observed times and memory footprints. A full environment box and precise wall‑clock measurements are provided alongside scripts in the reproducibility materials.

## Take‑aways

1. On the Half‑Moon benchmark, TTM‑X and Copula‑NP capture curved dependence that separable transports and independent baselines miss. TRTF‑AP, which is separable by construction under the additive predictor, trails TTM‑X and Copula‑NP in joint NLL, consistent with theory.
2. On the 4D heterogeneous autoregressive task with $n=50$, flexible models can be variance limited. TTM‑S improves over independent fits, TRTF‑AP performs competitively given its regularization, and TTM‑X does not dominate at very small $n$. As $n$ grows, TTM‑X approaches the true joint, which matches the transport pullback objective.
3. On MINIBOONE, TRTF‑AP significantly improves over an independent baseline yet remains far from deep flows reported in the literature. This gap is consistent with the separable Jacobian and the capacity of modern flow architectures, see for example [@papamakarios2017masked] and [@dinh2017real].
4. Calibration diagnostics support the likelihood story. Models that can change conditional shape with context produce closer to uniform conditional PITs and better conditional NLL decompositions.
5. Ordering matters. Aligning the triangular factorization with the data’s causal or generative sequence improves sparsity and lowers NLL, which confirms the anisotropy rationale for triangular maps, see [@rosenblatt1952remarks] and [@knothe1957contributions].

**References cited in this section.**
Dinh, L., Krueger, D., and Bengio, Y. 2017. Real NVP.
Hothorn, T., Kneib, T., and Bühlmann, P. 2018. Conditional transformation models.
Hothorn, T., and Zeileis, A. 2017 and 2021. Transformation forests.
Knothe, H. 1957. Contributions to the theory of convex bodies.
Papamakarios, G., Pavlakou, T., and Murray, I. 2017. Masked Autoregressive Flow for density estimation.
Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., and Lakshminarayanan, B. 2021. Normalizing flows for probabilistic modeling and inference.
Rosenblatt, M. 1952. Remarks on a multivariate transformation.




# Discussion and Conclusion

This thesis compared Triangular Transport Maps (TTM), Transformation Random Forests (TRTF), and Copula models within a unified framework of measure transport, strictly adhering to the notation of Ramgraber et al. [@ramgraber2025friendly].

## Summary of Findings and Trade-offs (Q1, Q2)

*(TODO: Summarize the empirical performance. Highlight that TTM-X generally provides the best fit but requires complex implementation (e.g., NNs). TRTF-AP and TTM-S provide competitive performance when dependencies are primarily separable, but fail when complex interactions dominate.)*

The methods present distinct trade-offs:

  - **TTM:** Offers exact likelihoods and structured flexibility. TTM-S is highly interpretable, but TTM-X (especially NN-based) acts more like a black box and can be challenging to optimize and train.
  - **TRTF:** Provides a robust, nonparametric approach. The common TRTF-AP implementation is limited by the separable structure (Q3), and training $K$ separate forests is computationally intensive.
  - **Copulas:** Highly interpretable by separating marginals and dependence. Gaussian copulas are efficient but often misspecified (elliptical dependence); nonparametric copulas do not scale to high dimensions.

## Methodological Insights (Q3)

A key theoretical contribution is the clarification of the link between TRTF and TTM. We formally showed (Lemma 1) that TRTF generally induces a triangular transport with exact likelihood equivalence. Furthermore, we demonstrated (Proposition 1) that the TRTF implementation utilizing additive predictors (TRTF-AP) realizes a **separable** triangular transport (TTM-S).

This provides a rigorous interpretation and clarifies its limitations. Separable maps are effective when dependencies manifest as conditional location shifts. They fundamentally cannot capture interactions where the *shape* or *scale* of a conditional distribution changes based on the predictors (non-separable dependencies/cross-term interactions).

The empirical results (e.g., Half-Moon, the gap in MINIBOONE performance) confirm this theoretical limitation. When interactions are complex, the flexibility of cross-term transports (TTM-X or deep flows) is necessary.

## Conclusion

The choice of density estimation method depends on the data characteristics and modeling goals. Separable TTMs and TRTF-AP provide strong, interpretable baselines for structured tabular data. When complex interactions or tail behaviors dominate, more flexible approaches like cross-term TTMs are required, albeit at the cost of increased complexity and reduced interpretability. Future work could explore more flexible TRTF implementations that induce TTM-X structures while retaining the robustness of the ensemble approach.

\\newpage

# References

\\newpage

# Appendix A — Algorithms

This appendix details the mathematical pseudo-algorithms for the data generation process and the core procedures.

## A.1 Triangular Data-Generating Process (DGP)

\\begin{algorithm}[H]
\\caption{\\textbf{Triangular Data-Generating Process (DGP)}}
\\label{alg:dgp-config}
\\small
\\begin{algorithmic}[1]
\\Require Sample size $N$; dimension $K$; configuration $\\mathrm{cfg}={c\_k}*{k=1}^K$ where $c\_k.\\mathrm{distr}$ is a 1D distribution family and $c\_k.\\mathrm{parm}:\\mathbb{R}^{k-1}\!\\to\!\\mathbb{R}^{m\_k}$ is a parameter mapping function.
\\Ensure Matrix $X\\in\\mathbb{R}^{N\\times K}$ drawn via the autoregressive scheme $x\_k \\sim F\_k(,\\cdot,;\\theta\_k(x*{1:k-1}))$.
\\Statex
\\State Initialize $X\\in\\mathbb{R}^{N\\times K}$
\\For{$i=1{:}N$} \\Comment{\\emph{i.i.d. rows}}
\\For{$k=1{:}K$} \\Comment{\\emph{Sequential across dimensions}}
\\State $c \\leftarrow \\mathrm{cfg}[k]$; \\hspace{0.6em} $x\_{1:k-1}\\leftarrow X\_{i,1:(k-1)}$
\\State $\\theta\_k \\leftarrow c.\\mathrm{parm}(x\_{1:k-1})$ \\Comment{Calculate parameters based on previous dimensions}
\\State Ensure $\\theta\_k$ satisfies constraints of $c.\\mathrm{distr}$ (e.g., positivity for scale parameters).
\\State Draw $X\_{i,k} \\sim \\text{SampleFrom}(c.\\mathrm{distr};\\theta\_k)$
\\EndFor
\\EndFor
\\State \\Return $X$
\\end{algorithmic}
\\end{algorithm}

## A.2 Transformation Random Forest (TRTF-AP) Procedure

\\begin{algorithm}[H]
\\caption{\\textbf{TRTF-AP Training and Prediction (Additive Predictor Implementation)}}
\\label{alg:trtf-script}
\\small
\\begin{algorithmic}[1]
\\Require Training data $X\_{\\mathrm{tr}}$; Hyperparameters.
\\Ensure Fitted model $\\mathcal{M} = {(\\mu, \\sigma), {\\mathcal{T}*k}*{k=1}^K}$.
\\Function{TrainTRTF}{$X\_{\\mathrm{tr}}$}
\\State Compute standardization parameters $(\\mu, \\sigma)$ from $X\_{\\mathrm{tr}}$.
\\State $U\_{\\mathrm{tr}} \\leftarrow T\_{\\text{std}}(X\_{\\mathrm{tr}})$.
\\For{$k=1{:}K$}
\\State Training tuples $D\_k = {(U\_{i,1:k-1}, U\_{ik})}*{i\\in\\mathrm{tr}}$.
\\State Grow transformation forest $\\mathcal{T}*k$ on $D\_k$.
\\Statex \\emph{Details:} Each tree partitions the predictor space $\\mathbb{R}^{k-1}$. In each leaf $\\ell$, estimate parameters $\\theta*{k,\\ell}$ of an additive TM (e.g., Bernstein coefficients for $h\_k$ and local shift $g\_k$) by maximizing localized log-likelihood:
\[ \\hat\\theta*{k,\\ell}\\in\\arg\\max\_\\theta\\ \\sum\_{i\\in\\ell}\\log \\pi\_k(U\_{ik};\\theta). \]
\\State Splits maximize the increase in this objective.
\\EndFor
\\State \\Return $\\mathcal{M}$.
\\EndFunction
\\Function{PredictLogDensity}{$\\mathcal{M}, X\_{\\mathrm{new}}$}
\\State $U\_{\\mathrm{new}} \\leftarrow T\_{\\text{std}}(X\_{\\mathrm{new}})$.
\\State Initialize $L \\in \\mathbb{R}^{N}$.
\\For{$i=1{:}N$}
\\State $L\_i \\leftarrow 0$.
\\For{$k=1{:}K$}
\\State Evaluate conditional density $\\hat \\pi\_k(U\_{ik}\\mid U\_{i,1:k-1})$ using forest $\\mathcal{T}*k$.
\\Statex \\emph{Aggregation:} $\\hat \\pi\_k$ is derived from the weighted average of leaf estimates across trees.
\\State $L\_i \\leftarrow L\_i + \\log \\hat \\pi\_k(U*{ik}\\mid U\_{i,1:k-1}) - \\log\\sigma\_k$. \\Comment{Include Standardization Jacobian}
\\EndFor
\\EndFor
\\State \\Return $L$.
\\EndFunction
\\end{algorithmic}
\\end{algorithm}

## A.3 Triangular Transport Maps (TTM) Procedure

\\begin{algorithm}[H]
\\caption{\\textbf{TTM Training and Prediction (General)}}
\\label{alg:ttm-general}
\\small
\\begin{algorithmic}[1]
\\Require Training data $X\_{\\mathrm{tr}}$; TTM Parameterization (Defines structure of $S\_k$ and parameters $\\Theta$).
\\Ensure Fitted model $\\mathcal{M} = {(\\mu, \\sigma), \\hat\\Theta}$.
\\Function{TrainTTM}{$X\_{\\mathrm{tr}}$}
\\State Compute $(\\mu, \\sigma)$. $U\_{\\mathrm{tr}} \\leftarrow T\_{\\text{std}}(X\_{\\mathrm{tr}})$.
\\State Optimize $\\Theta$ by minimizing the Negative Log-Likelihood (Forward KL divergence), assuming Gaussian reference $\\eta(z)$:
\[ \\hat\\Theta \\in \\arg\\min\_{\\Theta} \\sum\_{i\\in\\mathrm{tr}} \\sum\_{k=1}^K \\Big(\\tfrac12 S\_k(U\_{i,1:k}; \\Theta)^2 - \\log \\partial\_{U\_k} S\_k(U\_{i,1:k}; \\Theta)\\Big). \]
\\State Constraints on $\\Theta$ must ensure strict monotonicity of $S\_k$.
\\State \\Return $\\mathcal{M}$.
\\EndFunction
\\Function{PredictLogDensity}{$\\mathcal{M}, X\_{\\mathrm{new}}$}
\\State $U\_{\\mathrm{new}} \\leftarrow T\_{\\text{std}}(X\_{\\mathrm{new}})$.
\\State Initialize $L \\in \\mathbb{R}^{N}$.
\\For{$i=1{:}N$}
\\State $L\_i \\leftarrow 0$.
\\For{$k=1{:}K$}
\\State $z\_k \\leftarrow S\_k(U\_{i,1:k}; \\hat\\Theta)$.
\\State $\\log J\_k \\leftarrow \\log \\partial\_{U\_k} S\_k(U\_{i,1:k}; \\hat\\Theta)$.
\\State $LD\_k \\leftarrow -\\tfrac12 z\_k^2 - \\tfrac12\\log(2\\pi) + \\log J\_k - \\log\\sigma\_k$.
\\State $L\_i \\leftarrow L\_i + LD\_k$.
\\EndFor
\\EndFor
\\State \\Return $L$.
\\end{algorithmic}
\\end{algorithm}

```
```
