**Meeting 2: Transkript**

**Hothorn:**  Es gibt die Literatur in Bezug auf diese ganzen Neuronalen Netze. Das müssen wir uns einmal anschauen, diese Flows für die Dichteabschätzung. Ich würde mich aber zuerst komplett auf numerische Daten konzentrieren, also kontinuierliche multivariate Daten.

Okay. Also, die grundsätzliche Idee ist ja, dass wir sagen, okay, ich kann jede multivariate Dichte faktorisieren in ein Produkt über bedingte Dichten, also univariate bedingte Dichten. Wenn ich jetzt die gemeinsame Dichte von zwei Variablen habe, ist das die univariate Dichte der ersten multipliziert mit der bedingten Dichte der zweiten gegeben der ersten. Bei einer dritten Variable zieht sich das so weiter. Das ist trivialerweise einfach so.

Die Idee ist jetzt, dass wir mehrere univariate bedingte Dichten schätzen, und das kann man entweder mit diesen Transformation Forests machen oder mit Transformation Boosting. Man kann auch noch andere Methoden anwenden, das werden wir im Laufe der Zeit herausfinden. Diese bedingten Dichten multipliziert man einfach miteinander. Man beginnt also mit der univariaten Dichte der ersten Variable, dann modelliert man die zweite gegeben die erste, die dritte gegeben die ersten beiden und so weiter. Wenn ich J solche Variablen habe, benötige ich also J Modelle.

Normalizing Flows funktionieren im Prinzip genauso, aber sie schätzen die Dichte gemeinsam. Die Frage ist, ob man das auch getrennt schätzen kann. Diese Forests und Boosting-Methoden sind nicht schlecht, weil sie relativ unempfindlich gegenüber Hyperparametern sind. Man muss nicht ewig herumprobieren, bevor man ein vernünftiges Ergebnis erhält, und man kann damit experimentieren.

Das Beispiel, das ich geschickt habe, verwendet einen Forest und zeigt Spiralen – zwei ineinandergehende Halbmonde, was eine ziemlich komplizierte Dichte ist. Mit klassischen Methoden wäre das schwer zu modellieren, aber mit diesen flexiblen Modellen gelingt das relativ gut. Ich würde vorschlagen, dass wir uns zuerst die Literatur zu diesen Flows in der Deep-Learning-Gemeinschaft anschauen und Benchmark-Beispiele heraussuchen. Diese sollten sowohl einfach, also zwei- oder dreidimensional, als auch höherdimensional sein, wo es dann interessanter wird.

**Faro:**  
Als Neugier, wenn Sie sagen, das mit dem Random Forest funktioniert ganz gut, oder was Sie bisher kennen, warum besteht dann das Interesse, das auf neuronale Netze auszuweiten?

**Hothorn:**  
Nein, das machen wir gar nicht. Die leben nebeneinander. Neurale Netze sind eine Möglichkeit, solche Dichten zu schätzen, von der man weiß, dass sie gut funktioniert. Es gibt auch klassische Ansätze wie Kerndichte-Schätzer, zu denen es eine umfangreiche Literatur gibt. Diese würde ich im Moment jedoch nicht vertiefen, weil sie relativ schwer zu lesen sind, sehr theoretisch und es nur wenige Implementierungen gibt, die tatsächlich etwas leisten. Und wenn es etwas höherdimensional wird, wird alles sehr kompliziert. Diese Flows sind besser, weil sie Dichten effizienter schätzen können und vor allem vernünftige Benchmark-Beispiele haben, sowohl synthetische als auch reale Daten. 

**Faro:**
Sie haben gesagt, dass wir das auch mit empirischen Daten testen müssen, um die Effektivität zu überprüfen. Welche Daten haben Sie da im Sinn?

**Hothorn:**  
Es gibt tausende von Datensätzen. Ich würde erstmal versuchen herauszufinden, welche Ansätze es gibt. Ich möchte einfach eine multivariate Dichte schätzen. Die klassischen Methoden sind die Kerndichte-Schätzer, die man sich auch anschauen kann. Flows sind eine andere Möglichkeit. Ich würde nochmal eine Literaturrecherche durchführen – einfach nach "Density Estimation" und "Random Forest". Es gibt auch andere Methoden, die ich gesehen habe, die ich nicht aufgeschrieben habe, aber es gibt durchaus verschiedene Ansätze. Ich möchte herausfinden, wie diese grundsätzlich funktionieren und ob wir dieselbe Idee verfolgen, dass man univariate Regressionsmodelle schätzt und diese dann in der Dichte multipliziert. Die zwei Papers, die ich Ihnen geschickt habe, sind Methoden, um solche univariaten Modelle zu schätzen, von denen wir alles haben, was wir brauchen. Sie sind also praktisch gut geeignet für diese Komponenten. Wir haben ja ein bisschen Zeit, bevor wir tatsächlich starten müssen. Deswegen werde ich jetzt mal ein bisschen querlesen. Ich werde jetzt nicht anfangen, R-Code zu schreiben, weil das Zeit kostet und man sich dann in eine Höhle bohrt, aus der man nicht mehr herauskommt. Ich werde Literatur suchen und dabei nicht ins Detail gehen, sondern die Grundidee erfassen: Welche Techniken werden angewendet? Wie wird das evaluiert? Theoretisch. Und welche Beispieldatensätze oder Simulationsmodelle gibt es? Das brauchen wir sowieso. Es wäre gut, wenn Sie mal ein Git-Repository eröffnen und es mit mir teilen. Dort können wir die Materialien einpflegen, damit wir nicht Sachen per E-Mail hin und her schicken müssen. Dazu gehört auch, dass ich interessante Papers finde. Ich brauche nur die DOI, kein PDF. Vielleicht ein Dokument, wo man kommentiert, welches Paper es ist und ein paar Zeilen dazu, damit man sich grob erinnert, wie die Sachen zusammenhängen. Wenn es Simulationsdaten oder Simulationscode gibt, würde ich versuchen, diese gleich zu integrieren. Denn das brauchen wir sowieso. Wenn es Implementierungen gibt, die einfach anzuwenden sind, wie R-Pakete, würde ich die ebenfalls sammeln. Ich würde auf CRAN suchen, welche R-Implementierungen es gibt, und diese ebenfalls zusammenstellen. Das brauchen wir als Vergleichsmaßstab.

**Faro:**  
Klingt gut. Angenommen, wir bringen etwas sehr Cooles bis Ende April heraus – wie hoch sind die Chancen, dass das funktioniert, insbesondere hinsichtlich einer Publikation? Einfach aus Neugier, wie groß die Chancen sind.

**Hothorn:**  
Das ist schwer zu sagen. Grundsätzlich könnte es gut funktionieren, aber wahrscheinlich nicht sehr gut. Das ist meine a priori Einschätzung. Ich lasse mich überraschen, aber das wäre die Idee. Wenn man das aufschreibt, muss man relativ viel Arbeit reinstecken. Das kostet viel Zeit. Im Moment würde ich das als Spielwiese sehen – interessant, aber ohne große Erwartungen. Wir schauen dann, wo das hinführt. Gerade in diesem Feld gibt es sehr viel zu tun. Wenn man die Literatur angeschaut hat, sieht man, auf welchem Niveau die Arbeiten sind. Dann kann man einschätzen, ob wir das Niveau auch erreichen können. Ich vermute, das Niveau wird sehr hoch sein, das müssen wir allerdings prüfen.