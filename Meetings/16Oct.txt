Das Thema der Variante 2 dreht sich um das Modellieren von Dichten für multivariate Variablen, bei denen wir eine ganze Reihe von Variablen (Y1, Y2, …, YJ) beobachtet haben. Der Ansatz basiert auf einem Transformationsmodell, um die Abhängigkeiten zwischen diesen Variablen zu beschreiben und gleichzeitig die Modellierung flexibel zu halten.

Der Ausgangspunkt ist die Dekomposition der gemeinsamen Dichte in bedingte Dichten. Die Gesamtwahrscheinlichkeit für die Variablen kann geschrieben werden als das Produkt von bedingten Wahrscheinlichkeiten: Die marginale Dichte von Y1 multipliziert mit der bedingten Dichte von Y2 gegeben Y1, multipliziert mit der bedingten Dichte von Y3 gegeben Y1 und Y2, und so weiter bis zur bedingten Dichte von Yj gegeben Y1 bis Yj-1. 

Formal ausgedrückt lautet dies:
log(f(y_1, …, y_J)) = log(f(y_1)) + log(f(y_2 | y_1)) + … + log(f(y_J | y_1, …, y_{J-1})).

Der übergeordnete Gedanke ist es, für jede bedingte Dichte ein Regressionsmodell anzuwenden. Die bedingte Dichte f(y_j | y_1, …, y_{j-1}) kann als eine Art Regressionsmodell betrachtet werden, bei dem die Zielvariable y_j von den vorherigen Variablen abhängt. Um dies modellieren zu können, wird vorgeschlagen, dieses Regressionsmodell mit flexiblen Methoden zu lösen, anstatt nur mit linearen Modellen zu arbeiten.

Flexiblere Ansätze umfassen hier den Einsatz von Random Forests, Boosting-Methoden oder auch neuronalen Netzen. Die Motivation dahinter ist, dass solche Methoden in der Lage sind, komplexe Abhängigkeiten besser zu modellieren als einfache lineare Modelle, die oft nicht ausreichen, wenn die Daten kompliziertere Muster aufweisen. Mit diesen Methoden kann man auch nichttriviale Verteilungen approximieren, die mit herkömmlichen Modellen wie Copulas nur schwer zu erfassen wären.

Ein anschauliches Beispiel war eine Dichte mit einer komplexen Form, die eine "Half-Moon"-artige Struktur hat, also eine unregelmäßige, nicht zusammenhängende Dichteverteilung. Solche Dichten lassen sich mit klassischen Methoden nur schwer abbilden, während flexible Modelle wie die oben genannten dies gut bewältigen können.

Der Vorteil des vorgestellten Ansatzes liegt darin, dass jedes dieser Regressionsmodelle separat geschätzt werden kann, ohne dass die einzelnen Schätzungen miteinander kommunizieren müssen. Das heißt, die Modelle können auch individuell regularisiert und penalisert werden, was die Implementierung vereinfacht.

Die nächsten Schritte wären, eine Implementierung dieser Methode zu erstellen und sie gegenüber bestehenden Methoden zu evaluieren, die für solche Arten von Verteilungen entwickelt wurden. Ein solcher empirischer Vergleich wäre wertvoll, um die Effektivität dieses flexiblen Ansatzes zu validieren. Insgesamt zielt das Vorgehen darauf ab, einen Machine Learning-Ansatz zu verfolgen, bei dem besonders die Verwendung von Random Forests, Boosting oder auch neuronalen Netzen im Vordergrund steht.