---
title: "Multivariate Density Estimation: Comparing Transformation Random Forest, Normalizing Flows and Copulas"
author: "Léon Kia Faro"
date: "`r format(Sys.Date())`"
output:
  pdf_document:
    number_sections: true
    toc: true
    latex_engine: xelatex
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    fig_caption: true
df_print: paged
fontsize: 11pt
geometry: margin=1in
linkcolor: blue
header-includes:
  - \usepackage{amsmath,amssymb,mathtools}
  - \usepackage{algorithm}
  - \usepackage{algpseudocode}   % provides \begin{algorithmic}, \State, \Function, \Comment, \Statex
  - \floatname{algorithm}{Algorithm}
  - \renewcommand{\algorithmicrequire}{\textbf{Input:}}
  - \renewcommand{\algorithmicensure}{\textbf{Output:}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Introduction

Estimating the joint density \(p(x_1,\dots,x_K)\) of a multivariate random vector \(x=(x_1,\ldots,x_K)\) is a core problem in statistical modeling and unsupervised learning. By learning \(p(x)\) from data, we enable a range of tasks including probabilistic inference, anomaly detection, data imputation, and generative modeling. However, high-dimensional density estimation is challenging due to the curse of dimensionality and complex dependency structures.

This thesis investigates three modern approaches to multivariate density estimation – **Triangular Transport Maps (TTM)**, **Transformation Random Forests (TRTF)**, and **Copula Models** – comparing their methodologies and performance. We focus on how each method transforms or models a complex joint distribution by leveraging simpler components, and we evaluate them on benchmark data.

A unifying perspective for density estimation is **measure transport**, where one transforms a complicated **target distribution** \(\pi\) on \(\mathbb{R}^K\) into a simpler **reference distribution** \(\eta\) (often a standard Gaussian or uniform). If \(S:\mathbb{R}^K \to \mathbb{R}^K\) is an invertible transport map such that \(z = S(x)\) has distribution \(\eta\) when \(x \sim \pi\), then we can estimate \(\pi\) via learning \(S\) and use \(S^{-1}\) for sampling from \(\pi\). **Normalizing flows** implement this idea using deep neural networks: they construct \(S\) as a composition of many invertible transformations with tractable Jacobians. *Triangular transport maps*, in contrast, impose a structured, *monotonic* form on \(S\) that offers theoretical and practical advantages. Meanwhile, **transformation forests** (TRTF) take a nonparametric ensemble approach, using tree-based models to estimate conditional distributions. Finally, **copula models** provide a statistically interpretable way to construct multivariate densities by separating marginal distributions from the dependency structure (the copula). In this work, we formulate each approach in a common framework, strictly adopting the notation and terminology of a recent tutorial on triangular transport maps. We then empirically compare their performance on a real dataset and discuss theoretical connections between the methods.

## Contributions

- **Unified formulation:** We place TTM, TRTF, and copulas under a single change-of-variables and conditional-factorization framework with **train-only standardization**, **per-dimension log-density accounting in nats**, and a common **Predict-API** (matrix `logdensity_by_dim`; vector `logdensity` as row sums).
- **Theory bridge:** We provide a concise derivation showing that **TRTF induces a separable triangular map**; the TRTF joint likelihood equals the sum of conditional log-pdfs (plus the single standardization Jacobian).
- **Evaluation protocol:** Deterministic seeds, identical splits and preprocessing, acceptance checks (shapes, constants, stability), and reporting of **joint NLL** and **per-dimension NLL** with standard errors.
- **Comparative study:** Experiments on synthetic settings (Half-Moon; 4D conditional generator) and real data (MINIBOONE), highlighting strengths, weaknesses, and trade-offs of each class.

# Research Questions

- **Q1: Performance** — How do TTM, TRTF, and copula models compare in terms of goodness-of-fit for multivariate density estimation? We evaluate models on held-out test data via log-likelihood and per-dimension contributions.
- **Q2: Trade-offs** — What are the trade-offs among complexity, interpretability, and computational efficiency? Triangular maps yield exact likelihoods and interpretable Jacobians; forests and flows may be more flexible but less transparent.
- **Q3: Theory** — In what precise sense does TRTF realize a triangular transport, and when might cross-term transports exceed separable maps?

# Methods

We outline the methodology for each approach. We let \(\pi(x)\) denote the unknown target density (on \(\mathbb{R}^K\)) and \(\eta(z)\) a reference (typically \(\mathcal N(0,I)\) or product uniforms). All methods estimate \(\pi\) either by constructing \(S\) with \(S(X)\sim\eta\) for \(X\sim\pi\) or by factorizing \(\pi\) into lower-dimensional components.

## Common setup (summary; see Appendix A for full pseudoalgorithms)

- **Train-only standardization.** For \(k=1,\dots,K\),
  \[
  u_k \,=\, \frac{x_k-\mu_k}{\sigma_k},\qquad \sigma_k>0,
  \]
  where \((\mu_k,\sigma_k)\) are computed on train and persisted to val/test. This preprocessing contributes **exactly once per dimension** the Jacobian term \(-\log\sigma_k\) to the log-density.
- **Predict-API invariants.** `predict(., "logdensity_by_dim") \in \mathbb{R}^{N\times K}`; `predict(., "logdensity") \in \mathbb{R}^N` with row sums. Units are **nats**; no NA/Inf.
- **Determinism.** A global seed fixes splits, training, and evaluation; repeated runs match up to machine precision.

## Triangular Transport Maps

We adopt the transport view: learn an invertible map \(S:\mathbb{R}^K\to\mathbb{R}^K\) that pushes \(\pi\) to \(\eta\) (typically \(\mathcal N(0,I)\)). A map \(S\) is **triangular** if
\[
S(x)=(S_1(x_1),\ S_2(x_{1:2}),\ \dots,\ S_K(x_{1:K})),
\]
and each \(S_k\) is **strictly increasing in its last argument** \(x_k\). This structure implies a factorized Jacobian
\[
\det \nabla S(x) \;=\; \prod_{k=1}^K \partial_{x_k} S_k(x_{1:k}),
\]
which makes change‑of‑variables numerically stable.

### Train‑only standardization
We standardize features on the **training split only**:
\[
u_k \,=\, \frac{x_k-\mu_k}{\sigma_k},\qquad \sigma_k>0,
\]
and apply the same \((\mu_k,\sigma_k)\) to validation/test. The Jacobian of this preprocessing contributes **exactly once per dimension** \(-\log\sigma_k\) to the log‑density.

### Pullback density
Let \(z=S(u)\) with \(\eta=\mathcal N(0,I)\). The log‑density on original \(x\) (units: nats) is
\begin{equation}
\label{eq:ttm-pullback}
\log \pi(x)
\,=\, \sum_{k=1}^K\Big[ -\tfrac12 z_k(u)^2 - \tfrac12 \log(2\pi) + \log \partial_{u_k} S_k(u_{1:k}) \Big] \;-\; \sum_{k=1}^K \log \sigma_k,
\end{equation}
where the \(-\tfrac12\log(2\pi)\) constant appears **exactly once per dimension**.

### Parameterizations used in this thesis
We study three monotone triangular families (evaluated on standardized inputs \(u\)):

- **Diagonal / marginal (linear monotone):**
  \[
  S_k(u_k) = a_k + b_k\,u_k,\quad b_k>0,\qquad 
  \log \partial_{u_k} S_k = \log b_k.
  \]

- **Separable:**
  \[
  S_k(u_{1:k}) = g_k(u_{1:k-1}) + f_k(u_k),\quad f_k'(u_k)>0,\qquad
  \log \partial_{u_k} S_k = \log f_k'(u_k).
  \]

- **Cross‑term:**
  \[
  S_k(u_{1:k}) = g_k(u_{1:k-1}) + \int_0^{u_k} \exp\!\big(h_k(t, u_{1:k-1})\big)\,dt,\quad
  \log \partial_{u_k} S_k = h_k(u_k,u_{1:k-1}).
  \]

All three are **triangular** and guarantee invertibility by construction (monotonicity in \(u_k\)).

## Transformation Random Forests (TRTF)

**Goal.** Estimate the joint via an autoregressive chain of conditional models \(\pi(x_k\mid x_{1:k-1})\), \(k=1,\dots,K\), on **standardized** inputs \(u=(x-\mu)\oslash\sigma\).

### Transformation models
A transformation model specifies conditional CDF/PDF as
\[
F_k(y\mid u_{1:k-1})=\Phi\!\big(h_k(y)-\eta_k(u_{1:k-1})\big),\qquad
f_k(y\mid u_{1:k-1})=\phi\!\big(h_k(y)-\eta_k(u_{1:k-1})\big)\,h_k'(y),
\]
where \(h_k\) is **strictly increasing** in \(y\) and \(\eta_k\) is a smooth effect of the predictors. 

### Forest construction
For each \(k\ge 2\), a **transformation forest** partitions the \(u_{1:k-1}\)-space and fits local transformation models in the leaves. Splits maximize increase in leaf log‑likelihood; bagging and \(mtry\) reduce variance. Aggregating leaf models across trees yields a smooth conditional \(\hat F_k(\cdot\mid u_{1:k-1})\) and \(\hat f_k(\cdot\mid u_{1:k-1})\).

### Prediction and joint log‑likelihood
For a query point \(x\) (standardized to \(u\)), the **per‑dimension** log‑density contributions are
\[
\mathrm{LD}_k(x) \;=\; \log \hat f_k\!\big(u_k \,\big|\, u_{1:k-1}\big)\;-\;\log \sigma_k,\quad k=1,\dots,K,
\]
and the joint log‑density is \(L(x)=\sum_k \mathrm{LD}_k(x)\). This is **exact** for the fitted TRTF model—i.e., it is the model’s normalized density—because each \(\hat f_k\) is a proper conditional pdf and the chain rule factorization is valid.

### API invariants (used throughout the thesis)
- `predict(., "logdensity_by_dim")` returns an \(N\times K\) numeric matrix \(\mathrm{LD}\).
- `predict(., "logdensity")` returns a length‑\(N\) numeric vector \(L\) with \(L_i=\sum_k \mathrm{LD}_{ik}\) (tolerance \(\le 10^{-12}\)).
- No NA/Inf; units are **nats**.

## Theoretical Links between TRTF and TTM

We make the connection precise: a TRTF is a **separable triangular transport**.

### Proposition 1 (TRTF induces a separable triangular map).
Let \(\hat F_k\) be the TRTF estimate of \(F_{X_k\mid X_{1:k-1}}\) on standardized inputs \(u=(x-\mu)\oslash\sigma\). Define
\[
\hat S_k(u_{1:k}) \;=\; \Phi^{-1}\!\big(\hat F_k(u_k\mid u_{1:k-1})\big)
\;=\; h_k(u_k)-\eta_k(u_{1:k-1}).
\]
Then \(\hat S=(\hat S_1,\dots,\hat S_K)\) is triangular, and each component is strictly increasing in \(u_k\) with
\[
\partial_{u_k}\hat S_k(u) \;=\; h_k'(u_k) \;>\;0.
\]
Hence \(\hat S\) is a valid **separable** triangular transport.

*Sketch.* Monotonicity follows from the transformation model: \(\Phi^{-1}\) is monotone, and \(h_k\) is strictly increasing in \(u_k\). Triangularity follows from the autoregressive conditioning on \(u_{1:k-1}\). \(\square\)

### Corollary 1 (Exact model density via pullback).
With \(z=\hat S(u)\) and Gaussian reference \(\eta=\mathcal N(0,I)\), the model log‑density satisfies
\[
\log \hat\pi(x)
= \sum_{k=1}^K \Big[ -\tfrac12 z_k(u)^2 - \tfrac12\log(2\pi) + \log h_k'(u_k) \Big] - \sum_{k=1}^K \log \sigma_k
= \sum_{k=1}^K \log \hat f_k(u_k\mid u_{1:k-1}) - \sum_{k=1}^K \log \sigma_k.
\]
Thus the TRTF joint log‑likelihood equals the sum of conditional log‑pdfs (plus the **single** standardization Jacobian), matching the triangular pullback formula.

### Remark (Reference choice).
Using \(\mathrm{Unif}(0,1)\) as reference via \(\tilde S_k(u)=\hat F_k(u_k\mid u_{1:k-1})\) is equally valid. Composing with \(\Phi^{-1}\) recovers the Gaussian reference above.

### Sampling
- **Joint sampling**: draw \(z\sim\mathcal N(0,I)\), then recursively
  \[
  u_k \,=\, h_k^{-1}\!\big(z_k+\eta_k(u_{1:k-1})\big),\qquad 
  x_k \,=\, \mu_k+\sigma_k\,u_k.
  \]
- **Conditional sampling** \(X_k\mid X_{1:k-1}=x\): draw \(z_k\sim\mathcal N(0,1)\) and invert \(h_k\) as above (or invert \(\hat F_k(\cdot\mid u_{1:k-1})\) numerically for a given \(u_{1:k-1}\)).

### Scope of the link
The TRTF‑induced map is **separable**. Cross‑term triangular maps (with \(\log \partial_{u_k}S_k\) depending jointly on \(u_k\) and \(u_{1:k-1}\)) are strictly more expressive, but also more delicate numerically; we include them as a separate class in our experiments.








## Exact likelihoods under triangular maps and TRTF

Both triangular maps and TRTF admit **exact model likelihoods**:
- **Triangular maps.** Use \eqref{eq:ttm-pullback} with \(z=S(u)\) and \(\log\partial_{u_k}S_k\) from the chosen parameterization (diagonal/separable/cross‑term). Include \(-\tfrac12\log(2\pi)\) once per dimension and subtract \(\sum_k\log\sigma_k\) **exactly once**.
- **TRTF.** Sum the conditional log‑pdfs \(\log \hat f_k(u_k\mid u_{1:k-1})\) and subtract \(\sum_k\log\sigma_k\) **exactly once**. This equals the triangular pullback for the separable map induced by the forest.

## Copula Models (Parametric and Nonparametric)

Copulas decouple marginals from dependence: with \(u_i = F_{X_i}(x_i)\) and copula density \(c(u)\), the joint density factorizes as \(\pi(x) = c(u)\prod_i f_{X_i}(x_i)\). We consider a Gaussian copula with empirical marginals (semiparametric) and a smoothed empirical copula (nonparametric). These highlight the cost of misspecified dependence (parametric) versus the data demands of flexible nonparametric dependence estimation.

## Baselines: True Joint and True Marginals

For simulations, **True Joint** provides an oracle upper bound on test log-likelihood, while **True Marginals** (independent product of true marginals) isolates the contribution of dependence modeling. For real data (MiniBooNE), only the independent baseline is feasible.

# Statistical Evaluation Framework

## Data and Preprocessing
For MiniBooNE we follow standard preprocessing: train-only standardization, removal of near-constant attributes, and pruning highly correlated features (to avoid trivial ridges). Established train/val/test splits are used to ensure comparability with prior work.

## Log-Likelihood Estimation
Primary metric is *test* log-likelihood in nats. TTM and copulas allow exact evaluation via change-of-variables and copula factorization, respectively. TRTF evaluates the sum of conditional pdf logs, using the local parametric family from each forest leaf. We also compute calibration checks (marginal QQ) and dependence diagnostics (rank correlations) as secondary assessments.

## Fairness and Robustness
All methods share the same splits, seeds, and comparable complexity; each result is averaged over multiple seeds with standard errors reported. We apply paired tests or overlap of \(\pm 2\cdot\mathrm{SE}\) to gauge practical significance differences.

# Results

## Half-Moon

|  # | model               | mean_joint_nll | per_dim_nll_1 | per_dim_nll_2 |
| -: | ------------------- | -------------- | ------------- | ------------- |
|  1 | True_uncond.        | 1.37 ± 0.11    | 0.69          | 0.69          |
|  2 | True_cond.          | 0.70 ± 0.12    | 0.35          | 0.35          |
|  3 | TRTF                | 1.83 ± 0.14    | 1.25          | 0.57          |
|  4 | TTM_marginal        | 2.04 ± 0.12    | 1.29          | 0.75          |
|  5 | TTM_sep             | 1.92 ± 0.14    | 1.29          | 0.64          |
|  6 | TTM_cross           | 1.22 ± 0.20    | 0.92          | 0.29          |
|  7 | Copula_np           | 0.87 ± 0.16    | 0.76          | 0.11          |

## 4D Conditional Data Generation

**n = 50**

|Dim | Distribution | True (marginal) | True (joint) | Random Forest | Marginal Map | Separable Map | Cross-Term Map |
|--: | :----------- | :-------------: | :----------: | :-----------: | :----------: | :-----------: | :------------: |
| 1  | norm         |   1.46 ± 0.26   |  1.41 ± 0.29 |  1.48 ± 0.24  |  1.49 ± 0.20 |  1.46 ± 0.26  |   1.46 ± 0.25  |
| 2  | exp          |   1.55 ± 0.46   |  1.38 ± 0.65 |  2.54 ± 0.75  |  3.30 ± 0.01 |  1.75 ± 0.70  |   2.58 ± 0.03  |
| 3  | beta         |   −0.46 ± 0.63  | −0.63 ± 1.00 |  −0.14 ± 0.34 |  0.40 ± 0.17 |  0.28 ± 0.70  |   0.39 ± 0.23  |
| 4  | gamma        |   2.21 ± 1.11   |  2.07 ± 0.80 |  2.22 ± 1.08  |  2.78 ± 0.77 |  2.97 ± 1.45  |   3.00 ± 1.46  |
| k  | SUM          |   4.75 ± 1.10   |  4.23 ± 1.03 |  6.10 ± 1.61  |  7.97 ± 0.94 |  6.47 ± 1.92  |   7.43 ± 1.66  |

## MINIBOONE Dataset

We trained TTM (monotone NN maps), TRTF (500 trees per conditional, depth≈10), Gaussian and nonparametric copulas, and the independent baseline. Hyperparameters were selected by validation likelihood where applicable.

## Benchmark Comparison on MINIBOONE

Average test log likelihood (in nats) for conditional density estimation. Error bars correspond to 2 standard deviations.

| **Model**                     | **Miniboone**           |
|------------------------------|-------------------------|
| Gaussian (indep. baseline)   | \(-37.24 \pm 1.07\)     |
| MADE (ACN)                   | \(-15.59 \pm 0.50\)     |
| MADE MoG                     | \(-12.27 \pm 0.47\)     |
| Real NVP (5-layer)           | \(-13.55 \pm 0.49\)     |
| Real NVP (10-layer)          | \(-13.84 \pm 0.52\)     |
| MAF (5-layer)                | \(-11.75 \pm 0.44\)     |
| MAF (10-layer)               | \(-12.24 \pm 0.45\)     |
| MAF MoG (5-layer)            | **\(-11.68 \pm 0.44\)** |
| **TRTF (Transformation Forest)** | \(-29.88 \pm 0.02\) *(ours)* |

## Discussion of TRTF Result

TRTF substantially improves over the independent Gaussian baseline, indicating it learns nontrivial dependencies, but it underperforms modern flow models by a wide margin. Likely causes include bias from local parametric families, high-dimensional conditioning (43D), and the need for many partitions to capture complex interactions. Qualitatively, TRTF samples preserve first-order moments and some pairwise structures but miss higher-order structure, consistent with the likelihood gap.

## Triangular Transport Map Results

(*Add figures/tables with per-dimension NLL and calibration plots; the exact model likelihood allows comprehensive tail and PIT diagnostics.*)

## Copula Model Results

(*Report both parametric Gaussian and nonparametric copulas; include marginal calibration and tail dependence measures.*)

## True Joint/Marginal Baseline Results

(*For simulations, include oracle baselines to contextualize achievable performance.*)

# Discussion and Conclusion

**Methodological takeaway.** TRTF realizes a **separable** triangular transport and thus provides exact, normalized likelihoods via the sum of conditional log-pdfs. Separable TTM and TRTF excel on problems dominated by conditional shifts and monotone shape changes. Cross-term triangular transports can capture complex, localized interactions beyond separable structure but require careful numerical stabilization.

**Comparative view.** Parametric copulas are strong under elliptical dependence and weak otherwise; nonparametric copulas can adapt to local/tail asymmetries at the cost of higher data and smoothing choices. Neural flows (deep normalizing flows) remain the most flexible among the classes studied, but pay in interpretability and hyperparameter complexity. 

**Practical guidance.** Start with separable TTM/TRTF for structured tabular problems with moderate \(K\) and a premium on interpretability and exact likelihoods. Escalate to cross-term TTM or neural flows when interactions/tails overwhelm separable structure, and to nonparametric copulas when marginal calibration is critical and data are abundant.

---

# Appendix A — Access confirmation & Mathematical pseudoalgorithms

**Access confirmation.** I have carefully read `/mnt/data/a_friendly_introduction_to_triangular_transport.md` and will strictly adhere to its notation (triangular maps \(S=(S_1,\dots,S_K)\), change of variables, monotone \(S_k\) in the last argument, Jacobian product, forward-KL training, etc.) in what follows.

## Common conventions (all models)

- Data: \(X\in\mathbb{R}^{N\times K}\) split into train/val/test with a **global seed** \(s\).
- Train-only standardization: for \(k=1,\dots,K\),
  \[
  \mu_k=\tfrac1{N_{\mathrm{tr}}}\!\sum_{i\in\mathrm{tr}} x_{ik},\qquad
  \sigma_k=\sqrt{\tfrac1{N_{\mathrm{tr}}-1}\!\sum_{i\in\mathrm{tr}}(x_{ik}-\mu_k)^2},\quad \sigma_k>0.
  \]
  Write \(u_{ik}=(x_{ik}-\mu_k)/\sigma_k\) and \(u=(x-\mu)\oslash\sigma\).
- API invariants:
  \(\texttt{predict}(\cdot,\text{“logdensity\_by\_dim”})\to \mathbb{R}^{N\times K}\),
  \(\texttt{predict}(\cdot,\text{“logdensity”})\to \mathbb{R}^{N}\) with row sums:
  \[
  L_i=\sum_{k=1}^K \mathrm{LD}_{ik}\quad (\forall i),\qquad\text{no NA/Inf.}
  \]
- If a Gaussian reference \(\eta=\mathcal{N}(0,I)\) is used via a triangular map \(z=S(u)\), then for any \(x\),
  \[
  \boxed{\mathrm{LD}_k(x)= -\tfrac12 z_k(u)^2-\tfrac12\log(2\pi)+\log\partial_{u_k}S_k(u)-\log\sigma_k,}
  \]
  and \(L(x)=\sum_k\mathrm{LD}_k(x)\).
  (Triangular structure: \(S_k=S_k(u_{1:k})\) with \(\partial_{u_k}S_k>0\); Jacobian factorizes as \(\det\nabla S=\prod_k\partial_{u_k}S_k\).)
- Determinism: identical seeds \(\Rightarrow\) identical outputs up to machine precision.

## A) True Marginals (“oracle–independence” model)

**Inputs.** Oracle marginal pdfs \(\{\pi_k\}_{k=1}^K\) (on original scale).

**Fit.** 1. Compute \((\mu,\sigma)\) on train; persist.

**Predict.** For each \(x\) (row-wise): 1. \(u=(x-\mu)\oslash\sigma\). 2. For each \(k\):
\[
\pi_{k,\mathrm{std}}(u_k)=\sigma_k\,\pi_k(\mu_k+\sigma_k u_k),\qquad
\mathrm{LD}_k=\log\pi_{k,\mathrm{std}}(u_k)-\log\sigma_k=\log\pi_k(x_k).
\]
3. Output \(\mathrm{LD}=(\mathrm{LD}_k)_{k=1}^K\), \(L=\sum_k \mathrm{LD}_k\).
*(Unit test: if \(\pi_k=\mathcal{N}(0,1)\) and \(\mu=0,\sigma=1\), then \(\mathrm{LD}_k=-\tfrac12x_k^2-\tfrac12\log(2\pi)\).)*

## B) True Joint (“oracle–autoreg.” model)

**Inputs.** Either
- (B1) oracle conditional pdfs \(\{\pi(x_k\mid x_{1:k-1})\}_{k=1}^K\), or
- (B2) only oracle joint pdf \(\pi(x)\).

**Fit.** Compute \((\mu,\sigma)\) on train; persist.

**Predict.** For each \(x\):

- **Case B1 (preferred; triangular factorization).**
  For \(k=1,\dots,K\), set
  \[
  \mathrm{LD}_1=\log\pi(x_1),\qquad
  \mathrm{LD}_k=\log\pi(x_k\mid x_{1:k-1})\quad(k\ge2),\qquad
  L=\sum_k\mathrm{LD}_k=\log\pi(x).
  \]
  *(Standardization is a no-op algebraically: \(\log\pi(x)=\log\pi_{\mathrm{std}}(u)-\sum_k\log\sigma_k\) with \(\pi_{\mathrm{std}}(u)=\pi(\mu+\sigma\odot u)\prod_k\sigma_k\).)*

- **Case B2 (joint only).**
  \[
  \mathrm{LD}_k=
  \begin{cases}
  0,& k=1,\dots,K-1,\\
  \log\pi(x),& k=K,
  \end{cases}
  \qquad L=\log\pi(x).
  \]
  (Maintains shape and row-sum invariants.)

## C) TRTF (Transformation Random Forest; autoregressive triangular CDF map)

**Model class.** For \(k=1,\dots,K\), estimate conditional CDFs \(F_k(y\mid u_{1:k-1})\) and pdfs \(f_k(y\mid u_{1:k-1})\) on standardized inputs \(u=(x-\mu)\oslash\sigma\).

**Fit.** For each \(k\):
1. Training tuples \(\{(u_{i,1:k-1},u_{ik})\}_{i\in\mathrm{tr}}\).
2. Grow a transformation forest \(\mathcal{T}_k=\{T_{k,t}\}_{t=1}^T\).
   Each tree partitions \(\mathbb{R}^{k-1}\) into leaves \(\ell\). In leaf \(\ell\), estimate param \(\theta_{k,\ell}\) of a **monotone transformation model** for \(u_k\) by maximizing localized log-likelihood
   \[
   \hat\theta_{k,\ell}\in\arg\max_\theta\ \sum_{i\in\ell}\log f_k(u_{ik};\theta),
   \]
   where \(f_k(\cdot;\theta)\) is a parametric pdf (e.g. Gaussian/Laplace) and \(F_k(\cdot;\theta)\) its CDF.
   Splits maximize increase in this objective (with standard complexity controls).
3. Aggregation. For query \(u_{1:k-1}\), define weights \(w_{k,t,\ell}(u_{1:k-1})\) indicating membership/proximity to leaf \(\ell\) of tree \(t\) and set
   \[
   \hat f_k(\,\cdot\mid u_{1:k-1})=\sum_{t,\ell}w_{k,t,\ell}(u_{1:k-1})\,f_k(\,\cdot\,;\hat\theta_{k,\ell}),
   \]
   \(\hat F_k\) analogously. (Enforces monotone CDFs \(\Rightarrow\) invertible in \(u_k\).)

**Predict.** For each \(x\):
1. \(u=(x-\mu)\oslash\sigma\).
2. For \(k=1,\dots,K\): evaluate \(\hat f_k(u_k\mid u_{1:k-1})\) and set
   \[
   \boxed{\mathrm{LD}_k=\log \hat f_k(u_k\mid u_{1:k-1})-\log\sigma_k,}
   \qquad L=\sum_k\mathrm{LD}_k.
   \]
*(Equivalently, define a triangular map \(\hat S_k(u_{1:k})=\hat F_k(u_k\mid u_{1:k-1})\) to the reference \(\mathrm{Unif}(0,1)\); monotonicity in \(u_k\) is automatic.)*

## D) TTM‑D (Diagonal / Marginal triangular transport)

**Parameterization (on standardized \(u\)).**
\( S_k(u_k)=a_k+b_k\,u_k,\qquad b_k>0. \)

**Training objective (maps from samples; Gaussian reference).**
\[
\min_{\{a_k,b_k>0\}} \ \sum_{i\in\mathrm{tr}}\sum_{k=1}^K \Big(\tfrac12 S_k(u_{ik})^2-\log b_k\Big).
\]

**Predict.** For each \(x\):
\( u=(x-\mu)\oslash\sigma,\quad z_k=S_k(u_k),\quad
 \mathrm{LD}_k=-\tfrac12 z_k^2-\tfrac12\log(2\pi)+\log b_k-\log\sigma_k, \quad L=\sum_k\mathrm{LD}_k. \)

## E) TTM‑S (Separable triangular transport)

**Parameterization (on standardized \(u\)).**
\( S_k(u_{1:k})=g_k(u_{1:k-1})+f_k(u_k),\qquad f_k'\!(u_k)>0. \)
Typical basis: \(g_k(u_{1:k-1})=\sum_j c^{\text{non}}_{k,j}\,\psi^{\text{non}}_{k,j}(u_{1:k-1})\),
\( f_k(u_k)=\sum_j c^{\text{mon}}_{k,j}\,\psi^{\text{mon}}_{k,j}(u_k)\) with \(\psi^{\text{mon}}\) monotone (e.g. iRBF/edge terms).

**Training objective (decouples by \(k\)).**
\[
\min_{\{c^{\text{non}}_k,c^{\text{mon}}_k\}} \ J_k(c^{\text{non}}_k,c^{\text{mon}}_k)
= \sum_{i\in\mathrm{tr}} \Big(\tfrac12 S_k(u_{i,1:k})^2 - \log f_k'(u_{ik})\Big).
\]
(Option: eliminate \(c^{\text{non}}_k\) in closed form given \(c^{\text{mon}}_k\) via normal equations; then solve a convex box-constrained problem in \(c^{\text{mon}}_k\ge 0\).)

**Predict.** For each \(x\):
\( u=(x-\mu)\oslash\sigma,\quad z_k=S_k(u_{1:k}),\quad
 \mathrm{LD}_k=-\tfrac12 z_k^2-\tfrac12\log(2\pi)+\log f_k'(u_k)-\log\sigma_k,\quad L=\sum_k\mathrm{LD}_k. \)

## F) TTM‑X (Cross‑term triangular transport)

**Parameterization (on standardized \(u\)).**
\( S_k(u_{1:k})=g_k(u_{1:k-1})+\int_{0}^{u_k}\! r\big(h_k(t,u_{1:k-1})\big)\,dt,\qquad r:\mathbb{R}\to\mathbb{R}_+\ (\text{e.g. }\exp,\ \text{softplus}). \)
Here \(g_k=\sum_j c^{\text{non}}_{k,j}\psi^{\text{non}}_{k,j}\), and \(h_k=\sum_j c^{\text{cr}}_{k,j}\psi^{\text{cr}}_{k,j}\) may include cross-terms \(\psi^{\text{cr}}_{k,j}(t,u_{1:k-1})\).

**Training objective (per \(k\)).**
\[
\min_{\{c^{\text{non}}_k,c^{\text{cr}}_k\}} \ J_k
= \sum_{i\in\mathrm{tr}} \Big(\tfrac12 S_k(u_{i,1:k})^2 - \log \partial_{u_k}S_k(u_{i,1:k})\Big), \quad
\partial_{u_k}S_k = r\big(h_k(u_{ik},u_{i,1:k-1})\big).
\]
(Compute the integral by 1D quadrature; enforce stability by clipping \(h_k\) to \([{-}H,H]\) during training/inference.)

**Predict.** For each \(x\):
\( u=(x-\mu)\oslash\sigma,\quad z_k=S_k(u_{1:k}),\quad
 \mathrm{LD}_k=-\tfrac12 z_k^2-\tfrac12\log(2\pi)+\log r\!\big(h_k(u_k,u_{1:k-1})\big)-\log\sigma_k,\quad L=\sum_k\mathrm{LD}_k. \)
# Appendix A — Peudoalgorithms

## 4D Data Gernation


\begin{algorithm}[H]
\caption{\textbf{Triangular Data-Generating Process (DGP) from Configuration}}
\label{alg:dgp-config}
\small
\begin{algorithmic}[1]
\Require Sample size $N\in\mathbb{N}$; dimension $K\in\mathbb{N}$; configuration $\mathrm{cfg}=\{c_k\}_{k=1}^K$ with fields $c_k.\mathrm{distr}\in\{$named 1D family$\}$ and parameter functor $c_k.\mathrm{parm}:\mathbb{R}^{k-1}\!\to\!\mathbb{R}^{m_k}$ (possibly empty); flag $\mathrm{return\_params}\in\{\texttt{TRUE},\texttt{FALSE}\}$.
\Ensure Matrix $X\in\mathbb{R}^{N\times K}$ drawn via the autoregressive/triangular scheme $x_k \sim F_k(\,\cdot\,;\theta_k(x_{1:k-1}))$; optionally a per-dimension parameter history.
\Statex

\State \textbf{Global constants:} numerical floor $\varepsilon\leftarrow 10^{-3}$ for strictly positive parameters.

\Function{Sanitize}{$\theta$} \Comment{\emph{ensure positivity/finite}}
  \ForAll{scalar components $p$ of $\theta$}
     \State \textbf{if} $p\notin(0,\infty)$ \textbf{then} $p\leftarrow \varepsilon$
  \EndFor
  \State \Return $\theta$
\EndFunction

\Function{NormalizeGammaKeys}{$\theta,\,\mathrm{distr}$} \Comment{\emph{compatibility mapping}}
  \If{$\mathrm{distr}=\text{Gamma}$ \textbf{and} $\theta$ has keys \texttt{shape1}, \texttt{shape2}}
     \State $\theta \leftarrow \{\texttt{shape}=\theta[\texttt{shape1}],\ \texttt{scale}=\theta[\texttt{shape2}]\}$
  \EndIf
  \State \Return $\theta$
\EndFunction

\Function{GenerateIIDFromConfig}{$N,\mathrm{cfg},\mathrm{return\_params}$}
  \State Initialize $X\in\mathbb{R}^{N\times K}$ as \texttt{NA}; set column names $(X1,\dots,XK)$
  \If{$\mathrm{return\_params}$}
     \State Initialize $\mathsf{ParamHist}[k]$ as list of length $N$ for each $k=1{:}K$
  \EndIf
  \For{$i=1{:}N$} \Comment{\emph{i.i.d. rows; sequential across dimensions}}
     \For{$k=1{:}K$}
        \State $c \leftarrow \mathrm{cfg}[k]$; \hspace{0.6em} $x_{1:k-1}\leftarrow X_{i,1:(k-1)}$
        \State $\theta_k \leftarrow \begin{cases}
           \varnothing, & \text{if } c.\mathrm{parm}=\varnothing\\
           c.\mathrm{parm}(x_{1:k-1}), & \text{otherwise}
        \end{cases}$
        \State $\theta_k \leftarrow$ \Call{NormalizeGammaKeys}{$\theta_k,\,c.\mathrm{distr}$}
        \State $\theta_k \leftarrow$ \Call{Sanitize}{$\theta_k$}
        \State Draw $X_{i,k} \sim r_{c.\mathrm{distr}}(1;\theta_k)$ \Comment{\emph{one-sample RNG of the named family}}
        \If{$\mathrm{return\_params}$ \textbf{and} $\theta_k\neq\varnothing$}
           \State $\mathsf{ParamHist}[k][i] \leftarrow \theta_k$ \Comment{\emph{store named parameter list for this $(i,k)$}}
        \EndIf
     \EndFor
  \EndFor
  \If{$\mathrm{return\_params}$}
     \State For each $k$: collate non-empty $\mathsf{ParamHist}[k][i]$ across $i$ into a tabular record $\mathsf{Params}[k]$ (row-binded by matching names)
     \State \Return $\{X,\ \mathsf{Params}[1{:}K]\}$
  \Else
     \State \Return $X$
  \EndIf
\EndFunction

\Function{GenSamples}{$G,\mathrm{return\_params}$} \Comment{\emph{compatibility wrapper}}
  \State \Return \Call{GenerateIIDFromConfig}{$G.n,\ G.config,\ \mathrm{return\_params}$}
\EndFunction

\Statex
\State \textbf{Notes.} (i) The triangular structure is enforced by iterating $k=1{:}K$ and letting $\theta_k$ depend only on $x_{1:k-1}$.
(ii) Determinism is controlled by the \emph{caller}'s RNG seed; this routine draws from the current RNG state.
(iii) No standardization is applied at generation time (raw $X$); any train-only standardization and its Jacobian correction are handled downstream during density evaluation.
\end{algorithmic}
\end{algorithm}

## TRTF 

\begin{algorithm}[H] % use [H] to place the float exactly here
\caption{\textbf{TRTF (Script-level) — Training, Prediction, and NLL Wrappers}}
\label{alg:trtf-script}
\small
\begin{algorithmic}[1]
\Require Data $X\in\mathbb{R}^{n\times K}$; hyperparameters $(\texttt{ntree},\texttt{minsplit},\texttt{minbucket},\texttt{maxdepth},\texttt{seed})$; $\texttt{cores}$.
\Ensure Model $\mathsf{M}_{\mathrm{core}}$ of class \texttt{mytrtf} with fields $\{\mathsf{ymod}[1{:}K],\ \mathsf{forest}[1{:}(K-1)],\ \mathsf{varimp},\ \texttt{seed}\}$; predictors returning per-dimension and joint log-densities (nats); invariants: finiteness and row-sum equality.

\Function{TrainTRTFCore}{$X,\texttt{ntree},\texttt{minsplit},\texttt{minbucket},\texttt{maxdepth},\texttt{seed},\texttt{cores}$}
  \State \textbf{assert} $X$ numeric $n\times K$; set RNG seed $\texttt{seed}$
  \State $D\leftarrow$ data frame of $X$ with columns $(X1,\dots,XK)$
  \State $\mathsf{ymod}[k]\leftarrow \textsc{BoxCox}(X_k\sim 1;\ D)$ for $k=1{:}K$ \Comment{Normal reference; includes $-\tfrac12\log(2\pi)$ \emph{once}}
  \State $\mathsf{ctrl}\leftarrow \textsc{ctree\_control}(\texttt{minsplit},\texttt{minbucket},\texttt{maxdepth})$
  \For{$k=2{:}K$}
     \State $\mathrm{fm}: X_k \sim X_1+\cdots+X_{k-1}$; $mtry\leftarrow \max\{1,\lfloor (k-1)/2 \rfloor\}$
     \State $\mathsf{forest}[k-1]\leftarrow \textsc{traforest}(\mathsf{ymod}[k],\ \mathrm{fm},\ D;\ \texttt{trace}=\texttt{TRUE},\ \texttt{ntree},\ \texttt{mtry},\ \texttt{mltargs}=\emptyset,\ \texttt{cores},\ \mathsf{ctrl})$
  \EndFor
  \State $\mathsf{varimp}[k-1]\leftarrow \textsc{varimp}(\mathsf{forest}[k-1])$ for $k=2{:}K$
  \State \Return $\mathsf{M}_{\mathrm{core}}=\{\mathsf{ymod}[1{:}K],\ \mathsf{forest}[1{:}(K-1)],\ \mathsf{varimp},\ \texttt{seed}\}$ with class \texttt{mytrtf}
\EndFunction

\Function{PredictMyTRTF}{$\mathsf{M}_{\mathrm{core}},\ X_{\mathrm{new}},\ \mathrm{type}\in\{\text{"logdensity"},\text{"logdensity\_by\_dim"}\}$}
  \State \textbf{assert} class(\,$\mathsf{M}_{\mathrm{core}}$)=\texttt{mytrtf}, $X_{\mathrm{new}}\in\mathbb{R}^{N\times K}$
  \State $D_{\mathrm{new}}\leftarrow$ data frame of $X_{\mathrm{new}}$ with columns $(X1,\dots,XK)$; $N\leftarrow$ rows$(D_{\mathrm{new}})$
  \State $LD_{:,1}\leftarrow \textsc{predict}(\mathsf{ymod}[1], D_{\mathrm{new}}, \texttt{type}=\text{"logdensity"})$; \textbf{assert} $LD_{:,1}\in\mathbb{R}^N$
  \For{$j=1{:}(K-1)$} \Comment{$k=j+1$}
     \State $q\leftarrow D_{\mathrm{new}}[\text{"X"}\!+\!(j{+}1)]$
     \State $r\leftarrow \textsc{predict}(\mathsf{forest}[j], D_{\mathrm{new}}, \texttt{type}=\text{"logdensity"}, q,\ \texttt{cores},\ \texttt{trace}=\texttt{TRUE})$
     \State \textbf{if} $r\in\mathbb{R}^N$ \textbf{then} $LD_{:,j+1}\leftarrow r$ \textbf{else}
           $M\leftarrow \textsc{as.matrix}(r)$; \textbf{if} $\mathrm{ncol}(M)>N$ then trim to first $N$;
           \textbf{assert} $M\in\mathbb{R}^{N\times N}$; $LD_{:,j+1}\leftarrow \mathrm{diag}(M)$
  \EndFor
  \State $LL\leftarrow [LD_{:,1}\ \cdots\ LD_{:,K}]$; \textbf{assert} $LL\in\mathbb{R}^{N\times K}$ and all finite
  \State \textbf{if} $\mathrm{type}=\text{"logdensity\_by\_dim"}$ \textbf{then} \Return $LL$
  \State $L\leftarrow LL\,\mathbf{1}_K$; \textbf{assert} all finite and $\max_i|L_i-\sum_{k=1}^K LL_{i,k}|\le 10^{-10}$; \Return $L$
\EndFunction

\Function{LogL\_TRTF}{$\mathsf{M}_{\mathrm{core}}, X$}
  \State \Return $-\mathrm{mean}(\Call{PredictMyTRTF}{\mathsf{M}_{\mathrm{core}}, X, \text{"logdensity"}})$; \textbf{assert} finite
\EndFunction

\Function{LogL\_TRTF\_dim}{$\mathsf{M}_{\mathrm{core}}, X$}
  \State $LL\leftarrow \Call{PredictMyTRTF}{\mathsf{M}_{\mathrm{core}}, X, \text{"logdensity\_by\_dim"}}$
  \State \Return $-\mathrm{colMeans}(LL)$; \textbf{assert} all finite
\EndFunction

\Function{Fit\_TRTF}{$S=\{X_{\mathrm{tr}},X_{\mathrm{te}}\},\ \mathrm{cfg},\ \texttt{seed},\ \texttt{cores}$}
  \State \textbf{assert} $X_{\mathrm{tr}},X_{\mathrm{te}}$ matrices with same $K$; set RNG seed if non-null
  \State $\mathsf{M}_{\mathrm{core}}\leftarrow \Call{TrainTRTFCore}{X_{\mathrm{tr}},\ \texttt{ntree}=n_{\mathrm{tr}},\ \texttt{minsplit},\ \texttt{minbucket},\ \texttt{maxdepth},\ \texttt{seed},\ \texttt{cores}}$
  \State attach $\mathrm{cfg}$ to model (book-keeping)
  \State $nll_{\mathrm{tr}}\leftarrow \Call{LogL\_TRTF}{\mathsf{M}_{\mathrm{core}}, X_{\mathrm{tr}}}$
  \State $\mathbf{nll}_{\mathrm{te}}^{(k)}\leftarrow \Call{LogL\_TRTF\_dim}{\mathsf{M}_{\mathrm{core}}, X_{\mathrm{te}}}$; \quad $nll_{\mathrm{te}}\leftarrow \sum_{k=1}^K \mathbf{nll}_{\mathrm{te}}^{(k)}$
  \State $v\leftarrow$ rowSums$\bigl(-\Call{PredictMyTRTF}{\mathsf{M}_{\mathrm{core}}, X_{\mathrm{te}}, \text{"logdensity\_by\_dim"}}\bigr)$; \quad $\mathrm{se}_{\mathrm{te}}\leftarrow \mathrm{sd}(v)/\sqrt{|v|}$
  \State \Return model augmented with $\{nll_{\mathrm{tr}},\ nll_{\mathrm{te}},\ \mathrm{se}_{\mathrm{te}},\ \mathbf{nll}_{\mathrm{te}}^{(k)}\}$
\EndFunction
\end{algorithmic}
\end{algorithm}

# Appendix B — Acceptance checks (all models; required)

**Shapes.** If \(\mathrm{LD}\in\mathbb{R}^{N\times K}\) is `predict(., "logdensity_by_dim")` and \(L\in\mathbb{R}^N\) is `predict(., "logdensity")`, then \(\sum_k \mathrm{LD}_{ik}=L_i\) for all \(i\), tolerance \(\le 10^{-12}\).

**Constants.** With Gaussian reference, include \(-\tfrac12\log(2\pi)\) **once per dimension**. No double‑counting across layers.

**Standardization Jacobian.** Train‑only \((\mu,\sigma)\) must be persisted and applied to val/test. Subtract \(\sum_k\log\sigma_k\) **exactly once overall** (implemented as \(-\log\sigma_k\) inside each \(\mathrm{LD}_k\)).

**Determinism.** Identical global seeds yield identical outputs up to \(10^{-15}\).

**Stability.** No NA/Inf in \(\mathrm{LD}\) or \(L\); for cross‑term maps clip logits \(h_k\in[-H,H]\); for TRTF enforce leaf pdfs bounded away from zero on the working support (PIT clipping \(u\in[1/(N_{\mathrm{tr}}+1),\,N_{\mathrm{tr}}/(N_{\mathrm{tr}}+1)]\)).
