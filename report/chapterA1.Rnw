% LaTeX file for Chapter 01
<<'preambleA1',include=FALSE>>=
library(knitr)
opts_chunk$set(
    fig.path='figure/cha1_fig', 
    self.contained=FALSE,
    cache=TRUE
)

@


\chapter{Appendix}\label{ch:appendix}

\section{Unified Transport Schematic}\label{app:transport-schematic-figure}
Figure~\ref{fig:transport-schematic} provides the full schematic of the unified transport pipeline referenced throughout the thesis. The landscape layout preserves readability for the granular annotations on each modeling branch.

\begin{landscape}
\begin{figure}[p]
\centering
\begin{tikzpicture}[
  scale=0.92,
  transform shape,
  >=Latex,
  node distance=8mm,
  process/.style={draw, rounded corners, align=center, font=\small, text width=56mm, minimum height=12mm, inner sep=4mm, fill=white},
  header/.style={draw, rounded corners, align=center, font=\small\bfseries, text width=56mm, minimum height=12mm, inner sep=4mm, fill=gray!15},
  arrow/.style={->, thick}
]

\node[process, font=\small\bfseries, text width=64mm] (data) {Input data: original features $x$};
\node[process, font=\small\bfseries, text width=72mm, below=10mm of data] (std) {Train-only standardization; re-use $\mu$, $\sigma$ from Appendix~\ref{app:ttm-core}};
\node[process, font=\small\bfseries, text width=80mm, below=8mm of std] (jac) {Common pullback evaluation; accumulate $\log\eta(S(u)) + \log|\det \nabla_u S(u)|$ (Equations~\eqref{eq:ch1-pullback}--\eqref{eq:ch1-affine-correction})};

\node[header, below=18mm of jac, xshift=-70mm] (ttmTitle) {Triangular transport maps};
\node[header, below=18mm of jac] (trtfTitle) {Transformation Random Forests};
\node[header, below=18mm of jac, xshift=70mm] (copTitle) {Copula models};

\matrix (ttmMatrix) [below=6mm of ttmTitle, column sep=6mm, row sep=4mm] {
  \node[header, text width=52mm] (ttmMargTitle) {TTM-Marginal}; & \node[header, text width=52mm] (ttmSepTitle) {TTM-Separable}; \\
  \node[process, text width=52mm] (ttmMargStepOne) {Closed-form affine coefficients $(a_k, b_k)$ in $u$-space}; & \node[process, text width=52mm] (ttmSepStepOne) {Context basis $g_k$ on predecessors, monotone basis $h_k$ on $u_k$}; \\
  \node[process, text width=52mm] (ttmMargStepTwo) {Forward pass via \texttt{ttm\_forward}; sum log Jacobian diagonals}; & \node[process, text width=52mm] (ttmSepStepTwo) {Solve constrained optimisation, enforce positivity, store Jacobian terms}; \\
};

\node[process, below=6mm of trtfTitle, text width=62mm] (trtfStepOne) {Estimate monotone marginals (Appendix~\ref{app:trtf})};
\node[process, below=6mm of trtfStepOne, text width=62mm] (trtfStepTwo) {Fit transformation forests for $X_k \mid X_{1:(k-1)}$ with additive predictor};
\node[process, below=6mm of trtfStepTwo, text width=62mm] (trtfStepThree) {Recover triangular likelihood via probability integral transform};

\node[process, below=6mm of copTitle, text width=60mm] (copStepOne) {Estimate train marginals $\widehat F_k$, densities $\widehat f_k$};
\node[process, below=6mm of copStepOne, text width=60mm] (copStepTwo) {Fit dependence on probit scores (KDE copula)};
\node[process, below=6mm of copStepTwo, text width=60mm] (copStepThree) {Combine marginals with copula-based dependence};

\node[process, font=\small\bfseries, text width=150mm, below=26mm of trtfStepThree] (outputs) {Reported outputs: log density, conditional diagnostics, samples, calibration, compute summaries};

\draw[arrow] (data) -- (std);
\draw[arrow] (std) -- (jac);
\draw[arrow] (jac.south west) to[out=215,in=90] (ttmTitle.north);
\draw[arrow] (jac) -- (trtfTitle);
\draw[arrow] (jac.south east) to[out=325,in=90] (copTitle.north);

\draw[arrow] (ttmTitle) -- (ttmMargTitle);
\draw[arrow] (ttmTitle) -- (ttmSepTitle);
\draw[arrow] (ttmMargTitle) -- (ttmMargStepOne);
\draw[arrow] (ttmMargStepOne) -- (ttmMargStepTwo);
\draw[arrow] (ttmSepTitle) -- (ttmSepStepOne);
\draw[arrow] (ttmSepStepOne) -- (ttmSepStepTwo);

\draw[arrow] (trtfTitle) -- (trtfStepOne);
\draw[arrow] (trtfStepOne) -- (trtfStepTwo);
\draw[arrow] (trtfStepTwo) -- (trtfStepThree);

\draw[arrow] (copTitle) -- (copStepOne);
\draw[arrow] (copStepOne) -- (copStepTwo);
\draw[arrow] (copStepTwo) -- (copStepThree);

\draw[arrow] (ttmMargStepTwo) to[out=-90,in=195] (outputs.west);
\draw[arrow] (ttmSepStepTwo) to[out=-90,in=165] (outputs.west);
\draw[arrow] (trtfStepThree) -- (outputs);
\draw[arrow] (copStepThree) to[out=-90,in=0] (outputs.east);

\end{tikzpicture}%
\caption{Unified evaluation pipeline shared by triangular transport maps, Transformation Random Forests, and copulas. The diagram shows how standardized features flow through the triangular pullback or copula dependence block before reporting log densities, conditional diagnostics, samples, calibration metrics, and compute summaries. The shared preprocessing, Jacobian accumulation, and reporting path define the evaluation protocol used across the thesis.}
\label{fig:transport-schematic}
\end{figure}
\end{landscape}

\section{Pseudo-code Summaries for Model Routines}

This appendix records consolidated pseudo-code for the core R implementations used in the experiments. Each summary captures inputs, main processing stages, and outputs so the execution flow is transparent without consulting the source code files.

\subsection{Transformation Random Forest (TRTF)}\label{app:trtf}

\textbf{Routine:}\quad\texttt{fit\_TRTF(S, config, seed, cores)} (calls \texttt{mytrtf}).

\begin{enumerate}
  \item Validate that the training matrix is numeric, set the RNG seed, and label columns as $X_1,\ldots,X_K$.
  \item Fit an intercept-only transformation model \texttt{BoxCox} for each $X_k$ to provide baseline monotone transformations.
  \item For $k = 2,\ldots,K$:
    \begin{enumerate}
      \item Build the formula $X_k \sim X_1 + \cdots + X_{k-1}$.
      \item Choose \texttt{mtry = max(1, floor((k-1)/2))} and standard \texttt{ctree} controls (\texttt{minsplit}, \texttt{minbucket}, \texttt{maxdepth}).
      \item Fit a transformation forest with \texttt{traforest} and store the conditional model (one forest per $k$).
    \end{enumerate}
  \item Return a \texttt{mytrtf} object containing baseline transformations, conditional forests, variable-importance scores, and the seed.

  \item \textbf{Prediction (\texttt{predict.mytrtf}):}
    \begin{enumerate}
      \item Convert new data to the same column naming scheme and evaluate $X_1$ through its baseline transformation model to obtain marginal log densities.
      \item For each conditional forest ($k\geq 2$) evaluate the log density of $X_k$ given $X_{1:(k-1)}$, extracting the diagonal when the forest returns a log-density matrix.
      \item Stack the per-dimension log densities (\texttt{logdensity\_by\_dim}) or sum them to obtain the joint log likelihood (\texttt{logdensity}).
    \end{enumerate}
\end{enumerate}

\subsection{Nonparametric Copula Baseline}\label{app:copula}

\textbf{Routine:}\quad\texttt{fit\_copula\_np(S, seed)}.

\begin{enumerate}
  \item Inspect the training matrix and optional class labels; detect whether the dedicated copula packages are available.
  \item If prerequisites fail (dimension $K \neq 2$ or labels missing), fall back to independent univariate kernel density estimates per dimension and store them for later interpolation.
  \item Otherwise, for each class label:
    \begin{enumerate}
      \item Fit one-dimensional \texttt{kde1d} models to each marginal $X_1$ and $X_2$.
      \item Convert training samples to pseudo-observations using mid-ranks scaled by $(n+1)^{-1}$ and clamp to $(\varepsilon, 1-\varepsilon)$.
      \item Fit a two-dimensional kernel copula with \texttt{kdecopula::kdecop} (method \texttt{TLL2}).
      \item Store marginals, copula fit, and effective sample size for the class.
    \end{enumerate}
  \item Record class priors and return a \texttt{copula\_np} object.

  \item \textbf{Prediction (\texttt{predict.copula\_np}):}
    \begin{enumerate}
      \item In fallback mode evaluate each univariate KDE at the requested points and sum log densities.
      \item In copula mode compute marginal log densities and CDF values, evaluate the copula density, and either:
        \begin{enumerate}
          \item Average over class-specific log densities weighted by priors (mixture prediction), or
          \item Use the class labels supplied at prediction time.
        \end{enumerate}
      \item Return per-dimension log densities or their sum depending on the requested type.
    \end{enumerate}
\end{enumerate}

\subsection{Triangular Transport Core Utilities}\label{app:ttm-core}

\textbf{Module:}\quad\texttt{ttm\_core.R} (shared by marginal and separable TTM fits).

\begin{enumerate}
  \item Provide train-only standardisation helpers that cache feature means and standard deviations and reapply them to new data.
  \item Define basis builders: polynomial features for predecessor coordinates $g_k$, monotone basis functions $f_k$ for the current coordinate, and their derivatives.
  \item Implement optional ordering heuristics (identity or Cholesky pivoting with optional Gaussianisation) and persist selected permutations.
  \item Expose a dispatcher \texttt{ttm\_forward(model, X)} that:
    \begin{enumerate}
      \item Standardises inputs using stored parameters.
      \item For marginal maps applies affine transformations $a_k + b_k x_k$ with precomputed coefficients.
      \item For separable maps constructs $g_k$ and $f_k$, computes $S_k = g_k + f_k$, and records the Jacobian diagonal $\partial_{x_k} S_k$.
    \end{enumerate}
  \item Provide \texttt{ttm\_ld\_by\_dim} to combine the forward map with the Gaussian reference, yielding per-dimension log densities used by all TTM variants.
\end{enumerate}

\subsection{Marginal Triangular Transport Map}\label{app:ttm-marg}

\textbf{Routine:}\quad\texttt{fit\_ttm\_marginal(data, seed)}.

\begin{enumerate}
  \item Split data into train/test subsets if only a matrix is provided; otherwise accept a prepared list.
  \item Standardise training features and, for each dimension $k$, compute closed-form coefficients $(a_k, b_k)$ that minimise the Gaussian pullback objective subject to $b_k > 0$.
  \item Store model parameters (standardisation, per-dimension coefficients, ordering) and time measurements.
  \item During prediction call \texttt{ttm\_forward} with the marginal coefficients and convert Jacobian diagonals to log densities via \texttt{ttm\_ld\_by\_dim}; aggregate per-dimension contributions when the joint log density is requested.
\end{enumerate}

\subsection{Separable Triangular Transport Map}\label{app:ttm-sep}

\textbf{Routine:}\quad\texttt{fit\_ttm\_separable(data, degree\_g, lambda, seed)}.

\begin{enumerate}
  \item Prepare train/test splits and standardise training features as in the marginal case.
  \item For each coordinate $k$:
    \begin{enumerate}
      \item Build polynomial features $g_k$ on previous coordinates (degree set by \texttt{degree\_g}).
      \item Build monotone basis functions $f_k$ on the current coordinate and their derivatives.
      \item If \texttt{degree\_g = 0}, use the marginal closed-form solution to recover affine parameters.
      \item Otherwise solve the regularised optimisation problem
            $\min_c \frac{1}{2}\lVert (I - \Phi_{\text{non}} M)c \rVert^2 - \sum \log (B c) + \lambda\,\text{penalty}(c)$
            using \texttt{optim} with L-BFGS-B while enforcing positivity of the derivative.
      \item Store coefficients $c_{\text{non}}$ and $c_{\text{mon}}$ for the coordinate.
    \end{enumerate}
  \item Assemble the model list with standardisation parameters, coefficients, and metadata; record training/prediction timings.
  \item At prediction time re-use \texttt{ttm\_forward} and \texttt{ttm\_ld\_by\_dim} to obtain per-dimension and joint log densities.
\end{enumerate}

\subsection{Evaluation Utilities}\label{app:evaluation}

\textbf{Module:}\quad\texttt{evaluation.R} (experiment orchestration).

\begin{enumerate}
  \item Define convenience helpers such as \texttt{stderr(x)} and \texttt{add\_sum\_row} for table post-processing.
  \item \texttt{prepare\_data(n, config, seed)} samples from the configured data-generating process, splits the sample into train/validation/test sets, and returns both the matrix of draws and the split structure.
  \item \texttt{fit\_models(S, config)} fits the oracle TRUE density and the TRTF baseline on a split, times their evaluations, and returns the fitted objects together with per-dimension log-likelihood arrays.
  \item \texttt{calc\_loglik\_tables(models, config, X\_te, ...)} aggregates negative log-likelihoods (nats) for TRUE (marginal and joint), TRTF, TTM, and separable TTM, formats the results with standard-error bands, appends a summary row, and renames columns for presentation.
  \item \texttt{eval\_halfmoon(mods, S, out\_csv)} ensures all requisite models are available (TRTF, TTM variants, copula baseline), evaluates them on the half-moon test split, computes joint and per-dimension negative log-likelihoods, and optionally persists the metrics as CSV artefacts.
\end{enumerate}

These structured summaries allow reproducing the algorithmic flow of each model without navigating the full R implementation.
