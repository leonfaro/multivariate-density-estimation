
% LaTeX file for Chapter 02
<<'preamble02',include=FALSE>>=
library(knitr)
opts_chunk$set(
    fig.path = 'figure/ch02_fig',
    self.contained = FALSE,
    cache = TRUE
)
@

\chapter{Methodological Background}\label{ch:background}

\section{Transport Framework and Notation}\label{sec:transport-frame}

This section establishes the coordinate system, notation, and algebraic identities used throughout the thesis. The schematic in Appendix~\ref{ch:appendix} remains valid; here we focus only on the formulas that are needed later. We set up the standardized pullback likelihood, the triangularity assumption, and the Jacobian factorization.

We write $u_{1:k}=(u_1,\ldots,u_k)$ and use $\partial_{u_k}$ for partial derivatives with respect to $u_k$.

We observe data on the original scale $x \in \mathbb{R}^K$. Training-split statistics define a fixed standardization map
\begin{equation}
  u \;=\; T_{\mathrm{std}}(x) \;=\; (x-\mu)\oslash\sigma,\qquad \sigma_k>0,
  \label{eq:transport-standardise}
\end{equation}
where $\mu$ and $\sigma$ are the empirical mean and standard deviation estimated on the training split, and $\oslash$ denotes elementwise division. Each feature is shifted and rescaled once using training data only. All derivatives and Jacobians are then taken in $u$-space, which avoids leakage and keeps results comparable across estimators.

We model $\pi_U$ as the pullback of a simple reference $\eta$ through a triangular map $S:u\mapsto z$. The reference is the $K$-variate standard normal $\eta(z)$. Applying the change-of-variables formula gives
\begin{equation}
  \pi_U(u) \;=\; \eta\!\left(S(u)\right)\,\left|\det\nabla_u S(u)\right|.
  \label{eq:transport-pullback}
\end{equation}

To report log densities on the original scale, we apply only the diagonal affine correction implied by standardization:
\begin{equation}
  \log \pi_X(x) \;=\; \log \pi_U\!\left(T_{\mathrm{std}}(x)\right) - \sum_{k=1}^{K}\log\sigma_k.
  \label{eq:transport-affine}
\end{equation}

The transport is assumed to be lower triangular and strictly increasing in each coordinate,
\begin{equation}
  S(u) \;=\; \big(S_1(u_1), S_2(u_{1:2}), \ldots, S_K(u_{1:K})\big), 
  \qquad \partial_{u_k}S_k(u_{1:k})>0,
  \label{eq:transport-triangular}
\end{equation}
so the Jacobian $\nabla_u S(u)$ is lower triangular. Its determinant factorizes as
\begin{equation}
  \log \big|\det \nabla_u S(u)\big| \;=\; \sum_{k=1}^{K}\log \partial_{u_k}S_k(u_{1:k}).
  \label{eq:transport-det}
\end{equation}

This factorization makes the \emph{log-determinant} cost $\mathcal{O}(K)$ once the diagonal terms are available. Evaluating all $S_k(u_{1:k})$ can be $\mathcal{O}(K^2)$ for a general triangular map, but reduces to $\mathcal{O}(K)$ in the separable parameterization of Section~\ref{sec:transport-separable}.

Strict monotonicity on each coordinate ($\partial_{u_k} S_k>0$) together with triangularity implies that $S$ is a bijection with a triangular inverse, computable by back-substitution.

\begin{table}[t]
  \centering
  \caption{Notation for the transport framework used in Chapters~\ref{ch:background} and~\ref{ch:dataanalysis}. All derivatives and Jacobians are with respect to $u$; log densities on $x$ apply the affine correction in Equation~\eqref{eq:transport-affine}.}
  \label{tab:transport-notation}
  \begin{tabular}{ll}
    \hline
    Symbol & Meaning \\ 
    \hline
    $x \in \mathbb{R}^K$ & Original features on the data scale \\ 
    $T_{\mathrm{std}}$ & Standardization map using training $(\mu,\sigma)$ \\ 
    $u = T_{\mathrm{std}}(x)$ & Standardized evaluation coordinates \\ 
    $z \in \mathbb{R}^K$ & Reference coordinates after transport \\ 
    $S:u\mapsto z$ & Monotone lower-triangular transport map \\ 
    $\nabla_u S(u)$ & Jacobian of $S$ with respect to $u$ \\ 
    $\eta(z)$ & $K$-variate standard normal density \\ 
    $\varphi(t)$, $\Phi(t)$ & Univariate standard normal density and CDF \\ 
    $\pi_U$, $\pi_X$ & Densities on $u$- and $x$-space \\ 
    $\mu$, $\sigma$ & Training mean vector and positive scales \\ 
    $K$ & Dimension of the feature vector \\ 
    \hline
  \end{tabular}
\end{table}

Taken together, the triangular and strictly monotone structure turns joint density evaluation
and sampling into sequential one-dimensional stages, and the standardized coordinates allow us
to report on the $x$-scale with only the diagonal correction from Equation~\eqref{eq:transport-affine}.

\section{Separable Triangular Maps and Transformation Random Forests}\label{sec:transport-separable}

We now move from the general triangular setup to the first concrete parameterization: the \textbf{separable triangular map}. We then show how \textbf{Transformation Random Forests (TRTF)} fit into the same framework. The aim is a shared likelihood that we can train, compare, and diagnose across estimators without changing conventions.

\subsection{Separable triangular component}

A separable component has the form
\begin{equation}
  S_k(u_{1:k}) \;=\; g_k(u_{1:k-1}) + h_k(u_k), \qquad h_k'(u_k) > 0,
  \label{eq:transport-separable}
\end{equation}
where earlier coordinates shift location through $g_k$, while $h_k$ shapes the marginal and ensures monotonicity. To fix identifiability, we enforce a convention such as centering $g_k$ on the training split, $\mathbb E[g_k(U_{1:k-1})]=0$.

With the standard normal reference, the pullback identity becomes
\begin{equation}
  \log \pi_U(u) \;=\; \sum_{k=1}^{K}\Big[\log \varphi\!\big(S_k(u_{1:k})\big) + \log h_k'(u_k)\Big].
  \label{eq:transport-likelihood}
\end{equation}
Up to a constant, the per-sample negative log-likelihood is
\begin{equation}
  \mathcal{L}(u) \;=\; \sum_{k=1}^{K}\Big[\tfrac{1}{2}\,S_k(u_{1:k})^2 - \log h_k'(u_k)\Big].
  \label{eq:transport-loss}
\end{equation}

For a dataset $\{u^{(i)}\}_{i=1}^n$, the empirical objective reads
\[
\mathcal L_n = \frac{1}{n}\sum_{i=1}^n \sum_{k=1}^{K}
\Big[\tfrac{1}{2}\,S_k(u^{(i)}_{1:k})^2 - \log h_k'(u^{(i)}_k)\Big] + \text{const}.
\]

These formulas evaluate in $\mathcal{O}(K)$ time per sample, since the Jacobian term depends only on $u_k$. Sampling inverts $S$ by back-substitution, and log densities on $x$-space apply the affine correction in Equation~\eqref{eq:transport-affine}. The speed comes with a trade-off: separability fixes the conditional shape along $u_k$ once $g_k$ shifts the location, so variance, skewness, and modality no longer adapt to the context $u_{1:k-1}$.

\subsection{Transformation Random Forests in the framework}\label{sec:transport-trtf}

Transformation Random Forests (TRTF) fit conditional CDFs and insert them into the triangular pipeline via the probability integral transform. Let $\widehat F_k(\cdot \mid u_{1:k-1})$ denote the smoothed conditional CDF. Define
\begin{equation}
  S_k(u_{1:k}) \;=\; \Phi^{-1}\!\Big(\widehat F_k(u_k \mid u_{1:k-1})\Big).
  \label{eq:transport-trtf-map}
\end{equation}

If the forest is parameterized additively as
\begin{equation}
  \widehat F_k(u_k \mid u_{1:k-1}) \;=\; \Phi\!\big(h_k(u_k) + g_k(u_{1:k-1})\big),
  \label{eq:transport-trtf-additive}
\end{equation}
then we recover the separable structure,
\begin{equation}
  S_k(u_{1:k}) \;=\; h_k(u_k) + g_k(u_{1:k-1}), \qquad h_k'(u_k) > 0.
  \label{eq:transport-trtf-separable}
\end{equation}

\paragraph{Derivative check.}
\[
\partial_{u_k}S_k
= \frac{d}{du_k}\Phi^{-1}(\Phi(h_k(u_k)+g_k))
= h_k'(u_k) > 0,
\]
so the Jacobian diagonal is indeed $h_k'(u_k)$ in the additive TRTF case.

\paragraph{Numerical note.}
In practice, forests may produce $\widehat F_k$ values outside $(0,1)$ by tiny amounts or with imperfect monotonicity. We therefore clip to $(\varepsilon,1-\varepsilon)$ with $\varepsilon=10^{-6}$ and, if needed, apply isotonic post-processing to enforce strict increase. This keeps $S_k=\Phi^{-1}(\widehat F_k)$ well defined.

With this forest aggregation and strictly monotone conditionals, the resulting TRTF map coincides with the separable parameterization, so it delivers the same likelihood and back-substitution sampler.

\section{Copula Baselines}\label{sec:transport-copula}

We now turn to copulas, used here as diagnostic baselines rather than high-dimensional competitors. The focus is on a nonparametric bivariate copula when $K=2$, with an independence fallback otherwise. Standardization remains global (based on the full training split), whereas marginal fits for copulas are per-class. This ensures comparability on the $x$-scale when reporting.

\subsection{Estimation and reporting}

For each class $y$, we proceed as follows:

\begin{itemize}
  \item \textbf{Marginals:} Fit univariate kernel densities (\texttt{kde1d}) to obtain $\widehat f_k(\cdot \mid y)$ and CDFs $\widehat F_k(\cdot \mid y)$.
  \item \textbf{Pseudo-observations:} Transform samples to $(0,1)$ using mid-ranks,
  \begin{equation}
    U_{ik}^{(y)} \;=\; \frac{\operatorname{rank}(X_{ik}^{(y)})}{n_y + 1}.
    \label{eq:copula-midranks}
  \end{equation}
  Mid-ranks handle ties; if the data contain many, a tiny jitter is added before ranking.
  \item \textbf{Dependence:} Fit a bivariate kernel copula density $c_y(u_1,u_2)$ using \texttt{kdecopula::kdecop} with local quadratic smoothing, which uses a transformation approach to avoid boundary bias \citep{nagler2017kdecopula}.
\end{itemize}

For prediction at $x=(x_1,x_2)$:
\begin{enumerate}
  \item Evaluate $\widehat f_k(x_k \mid y)$ and $\widehat F_k(x_k \mid y)$.
  \item Evaluate $c_y(\widehat F_1(x_1\mid y), \widehat F_2(x_2\mid y))$.
  \item Combine them through
  \begin{equation}
    \log \widehat \pi_X(x \mid Y=y) 
      \;=\; \log \widehat f_1(x_1 \mid y) + \log \widehat f_2(x_2 \mid y)
      + \log c_y\!\Big(\widehat F_1(x_1 \mid y), \widehat F_2(x_2 \mid y)\Big).
    \label{eq:copula-logdensity}
  \end{equation}
\end{enumerate}
When labels are unknown, we mix across classes using priors $\pi(y)$ and a stable log-sum-exp.

\subsection{Fallback and scope}

If $K\neq 2$, labels are absent, or packages unavailable, we fall back to an independence model:
\begin{equation}
  \log \widehat \pi_X^{\mathrm{ind}}(x) \;=\; \sum_{k=1}^{K} \log \widehat f_k(x_k).
  \label{eq:copula-independence}
\end{equation}
Hence the NP-Copula is restricted to $K=2$ and is used only as a diagnostic baseline.

\subsection{Consistency with evaluation frame}

Copulas operate directly on the original scale $x$ and therefore bypass the affine correction of Section~\ref{sec:transport-frame}. Still, the logic is parallel to the triangular branch: start from a simple reference (independent uniforms), add a dependence correction, and return a single joint log density. Using mid-ranks for fitting and $\widehat F_k$ for reporting is consistent with Sklar's theorem \citep{sklar1959fonctions,nelsen2006introduction,joe2014dependence}.

\subsection{Terminology and references}

We use \textit{NP-Copula} for the nonparametric kernel estimator and \textit{Independence} for the fallback KDE product. References include Sklar’s theorem and standard copula texts \citep{sklar1959fonctions,nelsen2006introduction,joe2014dependence}, kernel copula methodology \citep{nagler2017kdecopula}, and the transformation-model literature for context \citep{hothorn2021transformation}. In practice, the NP-copula baseline for $K=2$ combines per-class marginals with a dependence correction, providing a transparent diagnostic comparator to the triangular transport models.
